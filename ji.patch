diff --git a/.gitignore b/.gitignore
index 6567dbfe..16569d83 100644
--- a/.gitignore
+++ b/.gitignore
@@ -6,3 +6,4 @@ logs/
 *.pyc
 *.tar.gz
 *.pem
+.vimspector.session
diff --git a/CMakeLists.txt b/CMakeLists.txt
index abad4c32..62021727 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -18,7 +18,7 @@ if( NOT EXISTS "${PROJECT_SOURCE_DIR}/utils/.git" )
 	message(" ")
 	message("          git submodule update --init")
 	message(" ")
-	message(FATAL_ERROR "missing required git submodules, see instructions above")
+	message(WARNING "missing required git submodules, see instructions above, ok if you copied all files (e.g. remote developmen)")
 endif()
 
 
diff --git a/c/detectNet.cpp b/c/detectNet.cpp
index 3c160ad2..ba3920bd 100644
--- a/c/detectNet.cpp
+++ b/c/detectNet.cpp
@@ -1,1147 +1,1375 @@
-/*
- * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
- * DEALINGS IN THE SOFTWARE.
- */
- 
-#include "detectNet.h"
-#include "objectTracker.h"
-#include "tensorConvert.h"
-#include "modelDownloader.h"
-
-#include "cudaMappedMemory.h"
-#include "cudaFont.h"
-#include "cudaDraw.h"
-
-#include "commandLine.h"
-#include "filesystem.h"
-#include "logging.h"
-
-
-#define OUTPUT_CVG  0	// Caffe has output coverage (confidence) heat map
-#define OUTPUT_BBOX 1	// Caffe has separate output layer for bounding box
-
-#define OUTPUT_UFF  0	// UFF has primary output containing detection results
-#define OUTPUT_NUM	1	// UFF has secondary output containing one detection count
-
-#define OUTPUT_CONF 0	// ONNX SSD-Mobilenet has confidence as first, bbox second
-
-#define CHECK_NULL_STR(x)	(x != NULL) ? x : "NULL"
-//#define DEBUG_CLUSTERING
-
-
-// constructor
-detectNet::detectNet( float meanPixel ) : tensorNet()
-{
-	mTracker   = NULL;
-	mMeanPixel = meanPixel;
-	mLineWidth = 2.0f;
-
-	mNumClasses  = 0;
-	mClassColors = NULL;
-	
-	mDetectionSets = NULL;
-	mDetectionSet  = 0;
-	mMaxDetections = 0;
-	mOverlayAlpha  = DETECTNET_DEFAULT_ALPHA;
-	
-	mConfidenceThreshold = DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD;
-	mClusteringThreshold = DETECTNET_DEFAULT_CLUSTERING_THRESHOLD;
-}
-
-
-// destructor
-detectNet::~detectNet()
-{
-	SAFE_DELETE(mTracker);
-	
-	CUDA_FREE_HOST(mDetectionSets);
-	CUDA_FREE_HOST(mClassColors);
-}
-
-
-// init
-bool detectNet::init( const char* prototxt, const char* model, const char* class_labels, const char* class_colors,
-			 	  float threshold, const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
-				  uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
-{
-	LogInfo("\n");
-	LogInfo("detectNet -- loading detection network model from:\n");
-	LogInfo("          -- prototxt     %s\n", CHECK_NULL_STR(prototxt));
-	LogInfo("          -- model        %s\n", CHECK_NULL_STR(model));
-	LogInfo("          -- input_blob   '%s'\n", CHECK_NULL_STR(input_blob));
-	LogInfo("          -- output_cvg   '%s'\n", CHECK_NULL_STR(coverage_blob));
-	LogInfo("          -- output_bbox  '%s'\n", CHECK_NULL_STR(bbox_blob));
-	LogInfo("          -- mean_pixel   %f\n", mMeanPixel);
-	LogInfo("          -- class_labels %s\n", CHECK_NULL_STR(class_labels));
-	LogInfo("          -- class_colors %s\n", CHECK_NULL_STR(class_colors));
-	LogInfo("          -- threshold    %f\n", threshold);
-	LogInfo("          -- batch_size   %u\n\n", maxBatchSize);
-
-	// create list of output names	
-	std::vector<std::string> output_blobs;
-
-	if( coverage_blob != NULL )
-		output_blobs.push_back(coverage_blob);
-
-	if( bbox_blob != NULL )
-		output_blobs.push_back(bbox_blob);
-	
-	// ONNX SSD models require larger workspace size
-	if( modelTypeFromPath(model) == MODEL_ONNX )
-	{
-		size_t gpuMemFree = 0;
-		size_t gpuMemTotal = 0;
-		
-		CUDA(cudaMemGetInfo(&gpuMemFree, &gpuMemTotal));
-
-		if( gpuMemTotal <= (2048 << 20) )
-			mWorkspaceSize = 512 << 20;
-		else
-			mWorkspaceSize = 2048 << 20;
-	}
-
-	// load the model
-	if( !LoadNetwork(prototxt, model, NULL, input_blob, output_blobs, 
-				  maxBatchSize, precision, device, allowGPUFallback) )
-	{
-		LogError(LOG_TRT "detectNet -- failed to initialize.\n");
-		return false;
-	}
-	
-	// allocate detection sets
-	if( !allocDetections() )
-		return false;
-
-	// load class descriptions
-	loadClassInfo(class_labels);
-	loadClassColors(class_colors);
-
-	// set the specified threshold
-	SetConfidenceThreshold(threshold);
-
-	return true;
-}
-
-
-// Create
-detectNet* detectNet::Create( const char* prototxt, const char* model, float mean_pixel, 
-						const char* class_labels, float threshold,
-						const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
-						uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
-{
-	return Create(prototxt, model, mean_pixel, class_labels, NULL, threshold, input_blob,
-			    coverage_blob, bbox_blob, maxBatchSize, precision, device, allowGPUFallback);
-}
-
-
-// Create
-detectNet* detectNet::Create( const char* prototxt, const char* model, float mean_pixel, 
-						const char* class_labels, const char* class_colors, float threshold,
-						const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
-						uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
-{
-	// check for built-in model string
-	if( FindModel(DETECTNET_MODEL_TYPE, model) )
-	{
-		return Create(model, threshold, maxBatchSize, precision, device, allowGPUFallback);
-	}
-	else if( fileExtension(model).length() == 0 )
-	{
-		LogError(LOG_TRT "couldn't find built-in detection model '%s'\n", model);
-		return NULL;
-	}
-
-	// load custom model
-	detectNet* net = new detectNet(mean_pixel);
-	
-	if( !net )
-		return NULL;
-
-	if( !net->init(prototxt, model, class_labels, class_colors, threshold, input_blob, coverage_blob, bbox_blob,
-				maxBatchSize, precision, device, allowGPUFallback) )
-		return NULL;
-
-	return net;
-}
-
-
-// Create (UFF)
-detectNet* detectNet::Create( const char* model, const char* class_labels, float threshold, 
-						const char* input, const Dims3& inputDims, 
-						const char* output, const char* numDetections,
-						uint32_t maxBatchSize, precisionType precision,
-				   		deviceType device, bool allowGPUFallback )
-{
-	detectNet* net = new detectNet();
-	
-	if( !net )
-		return NULL;
-
-	LogInfo("\n");
-	LogInfo("detectNet -- loading detection network model from:\n");
-	LogInfo("          -- model        %s\n", CHECK_NULL_STR(model));
-	LogInfo("          -- input_blob   '%s'\n", CHECK_NULL_STR(input));
-	LogInfo("          -- output_blob  '%s'\n", CHECK_NULL_STR(output));
-	LogInfo("          -- output_count '%s'\n", CHECK_NULL_STR(numDetections));
-	LogInfo("          -- class_labels %s\n", CHECK_NULL_STR(class_labels));
-	LogInfo("          -- threshold    %f\n", threshold);
-	LogInfo("          -- batch_size   %u\n\n", maxBatchSize);
-	
-	// create list of output names	
-	std::vector<std::string> output_blobs;
-
-	if( output != NULL )
-		output_blobs.push_back(output);
-
-	if( numDetections != NULL )
-		output_blobs.push_back(numDetections);
-	
-	// load the model
-	if( !net->LoadNetwork(NULL, model, NULL, input, inputDims, output_blobs, 
-					  maxBatchSize, precision, device, allowGPUFallback) )
-	{
-		LogError(LOG_TRT "detectNet -- failed to initialize.\n");
-		return NULL;
-	}
-	
-	// allocate detection sets
-	if( !net->allocDetections() )
-		return NULL;
-
-	// load class descriptions
-	net->loadClassInfo(class_labels);
-	net->loadClassColors(NULL);
-
-	// set the specified threshold
-	net->SetConfidenceThreshold(threshold);
-
-	return net;
-}
-
-
-// Create
-detectNet* detectNet::Create( const char* network, float threshold, uint32_t maxBatchSize, 
-						precisionType precision, deviceType device, bool allowGPUFallback )
-{
-	nlohmann::json model;
-	
-	if( !DownloadModel(DETECTNET_MODEL_TYPE, network, model) )
-		return NULL;
-	
-	std::string model_dir = "networks/" + model["dir"].get<std::string>() + "/";
-	std::string model_path = model_dir + model["model"].get<std::string>();
-	std::string prototxt = JSON_STR(model["prototxt"]);
-	std::string labels = JSON_STR(model["labels"]);
-	std::string colors = JSON_STR(model["colors"]);
-	
-	if( prototxt.length() > 0 )
-		prototxt = model_dir + prototxt;
-	
-	if( locateFile(labels).length() == 0 )
-		labels = model_dir + labels;
-	
-	if( locateFile(colors).length() == 0 )
-		colors = model_dir + colors;
-	
-	// get model input/output layers
-	std::string input = JSON_STR_DEFAULT(model["input"], DETECTNET_DEFAULT_INPUT);
-	std::string output_cvg = DETECTNET_DEFAULT_COVERAGE;
-	std::string output_bbox = DETECTNET_DEFAULT_BBOX;
-	std::string output_count = "";  // uff
-	
-	nlohmann::json output = model["output"];
-	
-	if( output.is_object() )
-	{
-		if( output["cvg"].is_string() )
-			output_cvg = output["cvg"].get<std::string>();
-		else if( output["scores"].is_string() )
-			output_cvg = output["scores"].get<std::string>();
-		
-		if( output["bbox"].is_string() )
-			output_bbox = output["bbox"].get<std::string>();
-		
-		if( output["count"].is_string() )
-			output_count = output["count"].get<std::string>();
-	}
-	
-	// some older model use the mean_pixel setting
-	float mean_pixel = 0.0f;
-	
-	if( model["mean_pixel"].is_number() )
-		mean_pixel = model["mean_pixel"].get<float>();
-		
-	// UFF models need the input dims parsed
-	Dims3 input_dims;
-	nlohmann::json dims = model["input_dims"];
-	
-	if( dims.is_array() && dims.size() == 3 )
-	{
-		for( uint32_t n=0; n < 3; n++ )
-			input_dims.d[n] = dims[n].get<int>();
-		
-		return Create(model_path.c_str(), labels.c_str(), threshold, input.c_str(), input_dims, 
-				    output_bbox.c_str(), output_count.c_str(), maxBatchSize, precision, device, allowGPUFallback);
-	}
-	
-	return Create(prototxt.c_str(), model_path.c_str(), mean_pixel, labels.c_str(), colors.c_str(), threshold,
-			    input.c_str(), output_cvg.c_str(), output_bbox.c_str(), maxBatchSize, precision, device, allowGPUFallback);
-}
-
-
-// Create
-detectNet* detectNet::Create( int argc, char** argv )
-{
-	return Create(commandLine(argc, argv));
-}
-
-
-// Create
-detectNet* detectNet::Create( const commandLine& cmdLine )
-{
-	detectNet* net = NULL;
-
-	// parse command line parameters
-	const char* modelName = cmdLine.GetString("network");
-	
-	if( !modelName )
-		modelName = cmdLine.GetString("model", "ssd-mobilenet-v2");
-
-	int maxBatchSize = cmdLine.GetInt("batch_size");
-	
-	if( maxBatchSize < 1 )
-		maxBatchSize = DEFAULT_MAX_BATCH_SIZE;
-	
-	// confidence used to be called threshold (support both)
-	float threshold = cmdLine.GetFloat("threshold");
-	
-	if( threshold == 0.0f )
-	{
-		threshold = cmdLine.GetFloat("confidence"); 
-		
-		if( threshold == 0.0f )
-			threshold = DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD;
-	}
-
-	// parse the model type
-	if( !FindModel(DETECTNET_MODEL_TYPE, modelName) )
-	{
-		const char* prototxt     = cmdLine.GetString("prototxt");
-		const char* input        = cmdLine.GetString("input_blob");
-		const char* out_blob     = cmdLine.GetString("output_blob");
-		const char* out_cvg      = cmdLine.GetString("output_cvg");
-		const char* out_bbox     = cmdLine.GetString("output_bbox");
-		const char* class_labels = cmdLine.GetString("class_labels");
-		const char* class_colors = cmdLine.GetString("class_colors");
-		
-		if( !input ) 	
-			input = DETECTNET_DEFAULT_INPUT;
-
-		if( !out_blob )
-		{
-			if( !out_cvg )  out_cvg  = DETECTNET_DEFAULT_COVERAGE;
-			if( !out_bbox ) out_bbox = DETECTNET_DEFAULT_BBOX;
-		}
-
-		if( !class_labels )
-			class_labels = cmdLine.GetString("labels");
-
-		if( !class_colors )
-			class_colors = cmdLine.GetString("colors");
-		
-		float meanPixel = cmdLine.GetFloat("mean_pixel");
-
-		net = detectNet::Create(prototxt, modelName, meanPixel, class_labels, class_colors, threshold, input, 
-							out_blob ? NULL : out_cvg, out_blob ? out_blob : out_bbox, maxBatchSize);
-	}
-	else
-	{
-		// create detectNet from pretrained model
-		net = detectNet::Create(modelName, threshold, maxBatchSize);
-	}
-
-	if( !net )
-		return NULL;
-
-	// enable layer profiling if desired
-	if( cmdLine.GetFlag("profile") )
-		net->EnableLayerProfiler();
-
-	// set some additional options
-	net->SetOverlayAlpha(cmdLine.GetFloat("alpha", DETECTNET_DEFAULT_ALPHA));
-	net->SetClusteringThreshold(cmdLine.GetFloat("clustering", DETECTNET_DEFAULT_CLUSTERING_THRESHOLD));
-	
-	// enable tracking if requested
-	net->SetTracker(objectTracker::Create(cmdLine));
-	
-	return net;
-}
-	
-
-// allocDetections
-bool detectNet::allocDetections()
-{
-	// determine max detections
-	if( IsModelType(MODEL_UFF) )	// TODO:  fixme
-	{
-		LogInfo(LOG_TRT "W = %u  H = %u  C = %u\n", DIMS_W(mOutputs[OUTPUT_UFF].dims), DIMS_H(mOutputs[OUTPUT_UFF].dims), DIMS_C(mOutputs[OUTPUT_UFF].dims));
-		mMaxDetections = DIMS_H(mOutputs[OUTPUT_UFF].dims) * DIMS_C(mOutputs[OUTPUT_UFF].dims);
-	}
-	else if( IsModelType(MODEL_ONNX) )
-	{
-		mNumClasses = DIMS_H(mOutputs[OUTPUT_CONF].dims);
-		mMaxDetections = DIMS_C(mOutputs[OUTPUT_CONF].dims) /** mNumClasses*/;
-		LogInfo(LOG_TRT "detectNet -- number of object classes: %u\n", mNumClasses);
-	}	
-	else
-	{
-		mNumClasses = DIMS_C(mOutputs[OUTPUT_CVG].dims);
-		mMaxDetections = DIMS_W(mOutputs[OUTPUT_CVG].dims) * DIMS_H(mOutputs[OUTPUT_CVG].dims) * mNumClasses;
-		LogInfo(LOG_TRT "detectNet -- number of object classes: %u\n", mNumClasses);
-	}
-
-	LogVerbose(LOG_TRT "detectNet -- maximum bounding boxes:   %u\n", mMaxDetections);
-
-	// allocate array to store detection results
-	const size_t det_size = sizeof(Detection) * mNumDetectionSets * mMaxDetections;
-	
-	if( !cudaAllocMapped((void**)&mDetectionSets, det_size) )
-		return false;
-	
-	memset(mDetectionSets, 0, det_size);
-	return true;
-}
-
-
-// loadClassInfo
-bool detectNet::loadClassInfo( const char* filename )
-{
-	if( !LoadClassLabels(filename, mClassDesc, mClassSynset, mNumClasses) )
-		return false;
-
-	if( IsModelType(MODEL_UFF) )
-		mNumClasses = mClassDesc.size();
-
-	LogInfo(LOG_TRT "detectNet -- number of object classes:  %u\n", mNumClasses);
-	
-	if( filename != NULL )
-		mClassPath = locateFile(filename);	
-	
-	return true;
-}
-
-
-// loadClassColors
-bool detectNet::loadClassColors( const char* filename )
-{
-	return LoadClassColors(filename, &mClassColors, mNumClasses, DETECTNET_DEFAULT_ALPHA);
-}
-
-
-// Detect
-int detectNet::Detect( float* input, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay )
-{
-	return Detect((void*)input, width, height, IMAGE_RGBA32F, detections, overlay);
-}
-
-
-// Detect
-int detectNet::Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection** detections, uint32_t overlay )
-{
-	Detection* det = mDetectionSets + mDetectionSet * GetMaxDetections();
-
-	if( detections != NULL )
-		*detections = det;
-
-	mDetectionSet++;
-
-	if( mDetectionSet >= mNumDetectionSets )
-		mDetectionSet = 0;
-	
-	return Detect(input, width, height, format, det, overlay);
-}
-
-
-// Detect
-int detectNet::Detect( float* input, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay )
-{
-	return Detect((void*)input, width, height, IMAGE_RGBA32F, detections, overlay);
-}
-
-
-// Detect
-int detectNet::Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t overlay )
-{
-	// verify parameters
-	if( !input || width == 0 || height == 0 || !detections )
-	{
-		LogError(LOG_TRT "detectNet::Detect( 0x%p, %u, %u ) -> invalid parameters\n", input, width, height);
-		return -1;
-	}
-	
-	if( !imageFormatIsRGB(format) )
-	{
-		LogError(LOG_TRT "detectNet::Detect() -- unsupported image format (%s)\n", imageFormatToStr(format));
-		LogError(LOG_TRT "                       supported formats are:\n");
-		LogError(LOG_TRT "                          * rgb8\n");		
-		LogError(LOG_TRT "                          * rgba8\n");		
-		LogError(LOG_TRT "                          * rgb32f\n");		
-		LogError(LOG_TRT "                          * rgba32f\n");
-
-		return false;
-	}
-	
-	// apply input pre-processing
-	if( !preProcess(input, width, height, format) )
-		return -1;
-	
-	// process model with TensorRT 
-	PROFILER_BEGIN(PROFILER_NETWORK);
-
-	if( !ProcessNetwork() )
-		return -1;
-	
-	PROFILER_END(PROFILER_NETWORK);
-	
-	// post-processing / clustering
-	const int numDetections = postProcess(input, width, height, format, detections);
-
-	// render the overlay
-	if( overlay != 0 && numDetections > 0 )
-	{
-		if( !Overlay(input, input, width, height, format, detections, numDetections, overlay) )
-			LogError(LOG_TRT "detectNet::Detect() -- failed to render overlay\n");
-	}
-	
-	// wait for GPU to complete work			
-	//CUDA(cudaDeviceSynchronize());	// BUG is this needed here?
-
-	// return the number of detections
-	return numDetections;
-}
-
-
-// preProcess
-bool detectNet::preProcess( void* input, uint32_t width, uint32_t height, imageFormat format )
-{
-	PROFILER_BEGIN(PROFILER_PREPROCESS);
-
-	if( IsModelType(MODEL_UFF) )
-	{
-		// SSD (TensorFlow / UFF)
-		if( CUDA_FAILED(cudaTensorNormBGR(input, format, width, height, 
-								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
-								    make_float2(-1.0f, 1.0f), GetStream())) )
-		{
-			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorNormBGR() failed\n");
-			return false;
-		}
-	}
-	else if( IsModelType(MODEL_ONNX) )
-	{
-		// SSD (PyTorch / ONNX)
-		if( CUDA_FAILED(cudaTensorNormMeanRGB(input, format, width, height,
-									   mInputs[0].CUDA, GetInputWidth(), GetInputHeight(), 
-									   make_float2(0.0f, 1.0f), 
-									   make_float3(0.5f, 0.5f, 0.5f),
-									   make_float3(0.5f, 0.5f, 0.5f), 
-									   GetStream())) )
-		{
-			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorNormMeanRGB() failed\n");
-			return false;
-		}
-	}
-	else if( IsModelType(MODEL_CAFFE) )
-	{
-		// DetectNet (Caffe)
-		if( CUDA_FAILED(cudaTensorMeanBGR(input, format, width, height,
-								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
-								    make_float3(mMeanPixel, mMeanPixel, mMeanPixel), 
-								    GetStream())) )
-		{
-			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorMeanBGR() failed\n");
-			return false;
-		}
-	}
-	else if( IsModelType(MODEL_ENGINE) )
-	{
-		// https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet
-		if( CUDA_FAILED(cudaTensorNormRGB(input, format, width, height,
-								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
-								    make_float2(0.0f, 1.0f), 
-								    GetStream())) )
-		{
-			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorMeanRGB() failed\n");
-			return false;
-		}
-	}
-	
-	PROFILER_END(PROFILER_PREPROCESS);
-	return true;
-}
-
-
-// postProcess
-int detectNet::postProcess( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections )
-{
-	PROFILER_BEGIN(PROFILER_POSTPROCESS);
-	
-	// parse the bounding boxes
-	int numDetections = 0;
-
-	if( IsModelType(MODEL_UFF) )	
-		numDetections = postProcessSSD_UFF(detections, width, height);
-	else if( IsModelType(MODEL_ONNX) )
-		numDetections = postProcessSSD_ONNX(detections, width, height);
-	else if( IsModelType(MODEL_CAFFE) )
-		numDetections = postProcessDetectNet(detections, width, height);
-	else if( IsModelType(MODEL_ENGINE) )
-		numDetections = postProcessDetectNet_v2(detections, width, height);
-	else
-		return -1;
-
-	// sort the detections by area
-	sortDetections(detections, numDetections);
-	
-	// verify the bounding boxes are within the bounds of the image
-	for( int n=0; n < numDetections; n++ )
-	{
-		if( detections[n].Top < 0 )
-			detections[n].Top = 0;
-		
-		if( detections[n].Left < 0 )
-			detections[n].Left = 0;
-		
-		if( detections[n].Right >= width )
-			detections[n].Right = width - 1;
-		
-		if( detections[n].Bottom >= height )
-			detections[n].Bottom = height - 1;
-	}
-	
-	// update tracking
-	if( mTracker != NULL && mTracker->IsEnabled() )
-		numDetections = mTracker->Process(input, width, height, format, detections, numDetections);
-	
-	PROFILER_END(PROFILER_POSTPROCESS);	
-	return numDetections;
-}
-
-
-// postProcessSSD_UFF
-int detectNet::postProcessSSD_UFF( Detection* detections, uint32_t width, uint32_t height )
-{
-	int numDetections = 0;
-	
-	const int rawDetections = *(int*)mOutputs[OUTPUT_NUM].CPU;
-	const int rawParameters = DIMS_W(mOutputs[OUTPUT_UFF].dims);
-
-	for( int n=0; n < rawDetections; n++ )
-	{
-		float* object_data = mOutputs[OUTPUT_UFF].CPU + n * rawParameters;
-
-		if( object_data[2] < mConfidenceThreshold )
-			continue;
-
-		detections[numDetections].TrackID   = -1; //numDetections; //(uint32_t)object_data[0];
-		detections[numDetections].ClassID    = (uint32_t)object_data[1];
-		detections[numDetections].Confidence = object_data[2];
-		detections[numDetections].Left       = object_data[3] * width;
-		detections[numDetections].Top        = object_data[4] * height;
-		detections[numDetections].Right      = object_data[5] * width;
-		detections[numDetections].Bottom	  = object_data[6] * height;
-
-		if( detections[numDetections].ClassID >= mNumClasses )
-		{
-			LogError(LOG_TRT "detectNet::Detect() -- detected object has invalid classID (%u)\n", detections[numDetections].ClassID);
-			detections[numDetections].ClassID = 0;
-		}
-
-		if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
-			continue;
-
-		numDetections += clusterDetections(detections, numDetections);
-	}
-
-	return numDetections;
-}
-
-
-// postProcessSSD_ONNX
-int detectNet::postProcessSSD_ONNX( Detection* detections, uint32_t width, uint32_t height )
-{
-	int numDetections = 0;
-	
-	float* conf = mOutputs[OUTPUT_CONF].CPU;
-	float* bbox = mOutputs[OUTPUT_BBOX].CPU;
-
-	const uint32_t numBoxes = DIMS_C(mOutputs[OUTPUT_BBOX].dims);
-	const uint32_t numCoord = DIMS_H(mOutputs[OUTPUT_BBOX].dims);
-
-	for( uint32_t n=0; n < numBoxes; n++ )
-	{
-		uint32_t maxClass = 0;
-		float    maxScore = -1000.0f;
-
-		// class #0 in ONNX-SSD is BACKGROUND (ignored)
-		for( uint32_t m=1; m < mNumClasses; m++ )	
-		{
-			const float score = conf[n * mNumClasses + m];
-
-			if( score < mConfidenceThreshold )
-				continue;
-
-			if( score > maxScore )
-			{
-				maxScore = score;
-				maxClass = m;
-			}
-		}
-
-		// check if there was a detection
-		if( maxClass <= 0 )
-			continue; 
-
-		// populate a new detection entry
-		const float* coord = bbox + n * numCoord;
-
-		detections[numDetections].TrackID   = -1; //numDetections;
-		detections[numDetections].ClassID    = maxClass;
-		detections[numDetections].Confidence = maxScore;
-		detections[numDetections].Left       = coord[0] * width;
-		detections[numDetections].Top        = coord[1] * height;
-		detections[numDetections].Right      = coord[2] * width;
-		detections[numDetections].Bottom	  = coord[3] * height;
-
-		if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
-			continue;
-		
-		numDetections += clusterDetections(detections, numDetections);
-	}
-
-	return numDetections;
-}
-
-
-// postProcessDetectNet
-int detectNet::postProcessDetectNet( Detection* detections, uint32_t width, uint32_t height )
-{
-	float* net_cvg   = mOutputs[OUTPUT_CVG].CPU;
-	float* net_rects = mOutputs[OUTPUT_BBOX].CPU;
-	
-	const int ow  = DIMS_W(mOutputs[OUTPUT_BBOX].dims);	// number of columns in bbox grid in X dimension
-	const int oh  = DIMS_H(mOutputs[OUTPUT_BBOX].dims);	// number of rows in bbox grid in Y dimension
-	const int owh = ow * oh;							// total number of bbox in grid
-	const int cls = GetNumClasses();					// number of object classes in coverage map
-	
-	const float cell_width  = /*width*/ GetInputWidth() / ow;
-	const float cell_height = /*height*/ GetInputHeight() / oh;
-	
-	const float scale_x = float(width) / float(GetInputWidth());
-	const float scale_y = float(height) / float(GetInputHeight());
-
-#ifdef DEBUG_CLUSTERING	
-	LogDebug(LOG_TRT "input width %u height %u\n", GetInputWidth(), GetInputHeight());
-	LogDebug(LOG_TRT "cells x %i  y %i\n", ow, oh);
-	LogDebug(LOG_TRT "cell width %f  height %f\n", cell_width, cell_height);
-	LogDebug(LOG_TRT "scale x %f  y %f\n", scale_x, scale_y);
-#endif
-
-	// extract and cluster the raw bounding boxes that meet the coverage threshold
-	int numDetections = 0;
-
-	for( uint32_t z=0; z < cls; z++ )	// z = current object class
-	{
-		for( uint32_t y=0; y < oh; y++ )
-		{
-			for( uint32_t x=0; x < ow; x++)
-			{
-				const float coverage = net_cvg[z * owh + y * ow + x];
-				
-				if( coverage < mConfidenceThreshold )
-					continue;
-
-				const float mx = x * cell_width;
-				const float my = y * cell_height;
-				
-				const float x1 = (net_rects[0 * owh + y * ow + x] + mx) * scale_x;	// left
-				const float y1 = (net_rects[1 * owh + y * ow + x] + my) * scale_y;	// top
-				const float x2 = (net_rects[2 * owh + y * ow + x] + mx) * scale_x;	// right
-				const float y2 = (net_rects[3 * owh + y * ow + x] + my) * scale_y;	// bottom 
-				
-			#ifdef DEBUG_CLUSTERING
-				LogDebug(LOG_TRT "rect x=%u y=%u  conf=%f  (%f, %f)  (%f, %f) \n", x, y, coverage, x1, y1, x2, y2);
-			#endif		
-
-				// merge with list, checking for overlaps
-				bool detectionMerged = false;
-
-				for( uint32_t n=0; n < numDetections; n++ )
-				{
-					if( detections[n].ClassID == z && detections[n].Expand(x1, y1, x2, y2) )
-					{
-						detectionMerged = true;
-						break;
-					}
-				}
-
-				// create new entry if the detection wasn't merged with another detection
-				if( !detectionMerged )
-				{
-					detections[numDetections].TrackID   = -1; //numDetections;
-					detections[numDetections].ClassID    = z;
-					detections[numDetections].Confidence = coverage;
-				
-					detections[numDetections].Left   = x1;
-					detections[numDetections].Top    = y1;
-					detections[numDetections].Right  = x2;
-					detections[numDetections].Bottom = y2;
-				
-					numDetections++;
-				}
-			}
-		}
-	}
-	
-	return numDetections;
-}
-
-
-// postProcessDetectNet_v2
-int detectNet::postProcessDetectNet_v2( Detection* detections, uint32_t width, uint32_t height )
-{
-	int numDetections = 0;
-	
-	float* conf = mOutputs[OUTPUT_CONF].CPU;
-	float* bbox = mOutputs[OUTPUT_BBOX].CPU;
-
-	const int cells_x  = DIMS_W(mOutputs[OUTPUT_BBOX].dims);	// number of columns in bbox grid in X dimension
-	const int cells_y  = DIMS_H(mOutputs[OUTPUT_BBOX].dims);	// number of rows in bbox grid in Y dimension
-	const int numCells = cells_x * cells_y;					// total number of bbox in grid
-
-	const float cell_width  = GetInputWidth() / cells_x;
-	const float cell_height = GetInputHeight() / cells_y;
-	
-	const float scale_x = float(width) / float(GetInputWidth());
-	const float scale_y = float(height) / float(GetInputHeight());
-
-	const float bbox_norm = 35.0f;  // https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps/blob/edd383cb2e4c7d18ee95ddbf9fdcf4db7803bb6e/tao_triton/python/postprocessing/detectnet_processor.py#L78
-	const float offset = 0.5f;      // https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps/blob/edd383cb2e4c7d18ee95ddbf9fdcf4db7803bb6e/tao_triton/python/postprocessing/detectnet_processor.py#L79
-	
-#ifdef DEBUG_CLUSTERING	
-	LogDebug(LOG_TRT "input width %u height %u\n", GetInputWidth(), GetInputHeight());
-	LogDebug(LOG_TRT "cells x %i  y %i\n", cells_x, cells_y);
-	LogDebug(LOG_TRT "cell width %f  height %f\n", cell_width, cell_height);
-	LogDebug(LOG_TRT "scale x %f  y %f\n", scale_x, scale_y);
-#endif
-
-	for( uint32_t c=0; c < mNumClasses; c++ )   // c = current object class
-	{
-		for( uint32_t y=0; y < cells_y; y++ )
-		{
-			for( uint32_t x=0; x < cells_x; x++)
-			{
-				const float confidence = conf[c * numCells + y * cells_x + x];
-				
-				if( confidence < mConfidenceThreshold )
-					continue;
-
-				const float cx = float(x * cell_width + offset) / bbox_norm;
-				const float cy = float(y * cell_height + offset) / bbox_norm;
-				
-				const float x1 = (bbox[(c * 4 + 0) * numCells + y * cells_x + x] - cx) * -bbox_norm * scale_x;
-				const float y1 = (bbox[(c * 4 + 1) * numCells + y * cells_x + x] - cy) * -bbox_norm * scale_y;
-				const float x2 = (bbox[(c * 4 + 2) * numCells + y * cells_x + x] + cx) *  bbox_norm * scale_x;
-				const float y2 = (bbox[(c * 4 + 3) * numCells + y * cells_x + x] + cy) *  bbox_norm * scale_y;
-								
-			#ifdef DEBUG_CLUSTERING
-				LogDebug(LOG_TRT "rect x=%u y=%u  conf=%f  (%f, %f)  (%f, %f) \n", x, y, confidence, x1, y1, x2, y2);
-			#endif
-				
-				detections[numDetections].TrackID   = -1; //numDetections;
-				detections[numDetections].ClassID    = c;
-				detections[numDetections].Confidence = confidence;
-				detections[numDetections].Left       = x1;
-				detections[numDetections].Top        = y1;
-				detections[numDetections].Right      = x2;
-				detections[numDetections].Bottom	  = y2;
-
-				if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
-					continue;
-		
-				numDetections += clusterDetections(detections, numDetections);
-			}
-		}
-	}
-	
-	return numDetections;
-}
-	
-	
-// clusterDetections
-int detectNet::clusterDetections( Detection* detections, int n )
-{
-	if( n == 0 )
-		return 1;
-
-	// test each detection to see if it intersects
-	for( int m=0; m < n; m++ )
-	{
-		if( detections[n].Intersects(detections[m], mClusteringThreshold) )	// TODO NMS or different threshold for same classes?
-		{
-			// if the intersecting detections have different classes, pick the one with highest confidence
-			// otherwise if they have the same object class, expand the detection bounding box
-		#ifdef CLUSTER_INTERCLASS
-			if( detections[n].ClassID != detections[m].ClassID )
-			{
-				if( detections[n].Confidence > detections[m].Confidence )
-				{
-					detections[m] = detections[n];
-
-					detections[m].TrackID = -1; //m;
-					detections[m].ClassID = detections[n].ClassID;
-					detections[m].Confidence = detections[n].Confidence;	
-				}
-				
-				return 0; // merged detection
-			}
-			else
-		#else
-			if( detections[n].ClassID == detections[m].ClassID )
-		#endif
-			{
-				detections[m].Expand(detections[n]);
-				detections[m].Confidence = fmaxf(detections[n].Confidence, detections[m].Confidence);
-
-				return 0; // merged detection
-			}
-		}
-	}
-
-	return 1;	// new detection
-}
-
-
-// sortDetections (by area)
-void detectNet::sortDetections( Detection* detections, int numDetections )
-{
-	if( numDetections < 2 )
-		return;
-
-	// order by area (descending) or confidence (ascending)
-	for( int i=0; i < numDetections-1; i++ )
-	{
-		for( int j=0; j < numDetections-i-1; j++ )
-		{
-			if( detections[j].Area() < detections[j+1].Area() ) //if( detections[j].Confidence > detections[j+1].Confidence )
-			{
-				const Detection det = detections[j];
-				detections[j] = detections[j+1];
-				detections[j+1] = det;
-			}
-		}
-	}
-
-	// renumber the instance ID's
-	//for( int i=0; i < numDetections; i++ )
-	//	detections[i].TrackID = i;	
-}
-
-
-// from detectNet.cu
-cudaError_t cudaDetectionOverlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, detectNet::Detection* detections, int numDetections, float4* colors );
-
-// Overlay
-bool detectNet::Overlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t numDetections, uint32_t flags )
-{
-	PROFILER_BEGIN(PROFILER_VISUALIZE);
-
-	if( flags == 0 )
-	{
-		LogError(LOG_TRT "detectNet -- Overlay() was called with OVERLAY_NONE, returning false\n");
-		return false;
-	}
-
-	// if input and output are different images, copy the input to the output first
-	// then overlay the bounding boxes, ect. on top of the output image
-	if( input != output )
-	{
-		if( CUDA_FAILED(cudaMemcpy(output, input, imageFormatSize(format, width, height), cudaMemcpyDeviceToDevice)) )
-		{
-			LogError(LOG_TRT "detectNet -- Overlay() failed to copy input image to output image\n");
-			return false;
-		}
-	}
-
-	// make sure there are actually detections
-	if( numDetections <= 0 )
-	{
-		PROFILER_END(PROFILER_VISUALIZE);
-		return true;
-	}
-
-	// bounding box overlay
-	if( flags & OVERLAY_BOX )
-	{
-		if( CUDA_FAILED(cudaDetectionOverlay(input, output, width, height, format, detections, numDetections, mClassColors)) )
-			return false;
-	}
-	
-	// bounding box lines
-	if( flags & OVERLAY_LINES )
-	{
-		for( uint32_t n=0; n < numDetections; n++ )
-		{
-			const Detection* d = detections + n;
-			const float4& color = mClassColors[d->ClassID];
-
-			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Top, d->Right, d->Top, color, mLineWidth));
-			CUDA(cudaDrawLine(input, output, width, height, format, d->Right, d->Top, d->Right, d->Bottom, color, mLineWidth));
-			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Bottom, d->Right, d->Bottom, color, mLineWidth));
-			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Top, d->Left, d->Bottom, color, mLineWidth));
-		}
-	}
-			
-	// class label overlay
-	if( (flags & OVERLAY_LABEL) || (flags & OVERLAY_CONFIDENCE) || (flags & OVERLAY_TRACKING) )
-	{
-		static cudaFont* font = NULL;
-
-		// make sure the font object is created
-		if( !font )
-		{
-			font = cudaFont::Create(adaptFontSize(width));  // 20.0f
-	
-			if( !font )
-			{
-				LogError(LOG_TRT "detectNet -- Overlay() was called with OVERLAY_FONT, but failed to create cudaFont()\n");
-				return false;
-			}
-		}
-
-		// draw each object's description
-	#ifdef BATCH_TEXT
-		std::vector<std::pair<std::string, int2>> labels;
-	#endif 
-		for( uint32_t n=0; n < numDetections; n++ )
-		{
-			const char* className  = GetClassDesc(detections[n].ClassID);
-			const float confidence = detections[n].Confidence * 100.0f;
-			const int2  position   = make_int2(detections[n].Left+5, detections[n].Top+3);
-			
-			char buffer[256];
-			char* str = buffer;
-			
-			if( flags & OVERLAY_LABEL )
-				str += sprintf(str, "%s ", className);
-			
-			if( flags & OVERLAY_TRACKING && detections[n].TrackID >= 0 )
-				str += sprintf(str, "%i ", detections[n].TrackID);
-			
-			if( flags & OVERLAY_CONFIDENCE )
-				str += sprintf(str, "%.1f%%", confidence);
-
-		#ifdef BATCH_TEXT
-			labels.push_back(std::pair<std::string, int2>(buffer, position));
-		#else
-			float4 color = make_float4(255,255,255,255);
-		
-			if( detections[n].TrackID >= 0 )
-				color.w *= 1.0f - (fminf(detections[n].TrackLost, 15.0f) / 15.0f);
-			
-			font->OverlayText(output, format, width, height, buffer, position.x, position.y, color);
-		#endif
-		}
-
-	#ifdef BATCH_TEXT
-		font->OverlayText(output, format, width, height, labels, make_float4(255,255,255,255));
-	#endif
-	}
-	
-	PROFILER_END(PROFILER_VISUALIZE);
-	return true;
-}
-
-
-// OverlayFlagsFromStr
-uint32_t detectNet::OverlayFlagsFromStr( const char* str_user )
-{
-	if( !str_user )
-		return OVERLAY_DEFAULT;
-
-	// copy the input string into a temporary array,
-	// because strok modifies the string
-	const size_t str_length = strlen(str_user);
-	const size_t max_length = 256;
-	
-	if( str_length == 0 )
-		return OVERLAY_DEFAULT;
-
-	if( str_length >= max_length )
-	{
-		LogError(LOG_TRT "detectNet::OverlayFlagsFromStr() overlay string exceeded max length of %zu characters ('%s')", max_length, str_user);
-		return OVERLAY_DEFAULT;
-	}
-	
-	char str[max_length];
-	strcpy(str, str_user);
-
-	// tokenize string by delimiters ',' and '|'
-	const char* delimiters = ",|";
-	char* token = strtok(str, delimiters);
-
-	if( !token )
-		return OVERLAY_DEFAULT;
-
-	// look for the tokens:  "box", "label", "default", and "none"
-	uint32_t flags = OVERLAY_NONE;
-
-	while( token != NULL )
-	{
-		if( strcasecmp(token, "box") == 0 )
-			flags |= OVERLAY_BOX;
-		else if( strcasecmp(token, "label") == 0 || strcasecmp(token, "labels") == 0 )
-			flags |= OVERLAY_LABEL;
-		else if( strcasecmp(token, "conf") == 0 || strcasecmp(token, "confidence") == 0 )
-			flags |= OVERLAY_CONFIDENCE;
-		else if( strcasecmp(token, "track") == 0 || strcasecmp(token, "tracking") == 0 )
-			flags |= OVERLAY_TRACKING;
-		else if( strcasecmp(token, "line") == 0 || strcasecmp(token, "lines") == 0 )
-			flags |= OVERLAY_LINES;
-		else if( strcasecmp(token, "default") == 0 )
-			flags |= OVERLAY_DEFAULT;
-
-		token = strtok(NULL, delimiters);
-	}	
-
-	return flags;
-}
-
-
-// SetOverlayAlpha
-void detectNet::SetOverlayAlpha( float alpha )
-{
-	const uint32_t numClasses = GetNumClasses();
-
-	for( uint32_t n=0; n < numClasses; n++ )
-		mClassColors[n].w = alpha;
-	
-	mOverlayAlpha = alpha;
-}
+/*
+ * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+ 
+#include "detectNet.h"
+#include "objectTracker.h"
+#include "tensorConvert.h"
+#include "modelDownloader.h"
+
+#include "cudaMappedMemory.h"
+#include "cudaFont.h"
+#include "cudaDraw.h"
+
+#include "commandLine.h"
+#include "filesystem.h"
+#include "logging.h"
+
+// YOLO dnn
+//#include <opencv2/cudaimgproc.hpp>
+#include <limits.h>
+
+
+#define OUTPUT_CVG  0	// Caffe has output coverage (confidence) heat map
+#define OUTPUT_BBOX 1	// Caffe has separate output layer for bounding box
+
+#define OUTPUT_UFF  0	// UFF has primary output containing detection results
+#define OUTPUT_NUM	1	// UFF has secondary output containing one detection count
+
+#define OUTPUT_CONF 0	// ONNX SSD-Mobilenet has confidence as first, bbox second
+
+#define CHECK_NULL_STR(x)	(x != NULL) ? x : "NULL"
+//#define DEBUG_CLUSTERING
+
+
+// constructor
+detectNet::detectNet( float meanPixel ) : tensorNet()
+{
+	mTracker   = NULL;
+	mMeanPixel = meanPixel;
+	mLineWidth = 2.0f;
+
+	mNumClasses  = 0;
+	mClassColors = NULL;
+	
+	mDetectionSets = NULL;
+	mDetectionSet  = 0;
+	mMaxDetections = 0;
+	mOverlayAlpha  = DETECTNET_DEFAULT_ALPHA;
+	
+	mConfidenceThreshold = DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD;
+	mClusteringThreshold = DETECTNET_DEFAULT_CLUSTERING_THRESHOLD;
+}
+
+
+// destructor
+detectNet::~detectNet()
+{
+	SAFE_DELETE(mTracker);
+	
+	CUDA_FREE_HOST(mDetectionSets);
+	CUDA_FREE_HOST(mClassColors);
+}
+
+
+// init
+bool detectNet::init( const char* prototxt, const char* model, const char* class_labels, const char* class_colors,
+			 	  float threshold, const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
+				  uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
+{
+	LogInfo("\n");
+	LogInfo("detectNet -- loading detection network model from:\n");
+	LogInfo("          -- prototxt     %s\n", CHECK_NULL_STR(prototxt));
+	LogInfo("          -- model        %s\n", CHECK_NULL_STR(model));
+	LogInfo("          -- input_blob   '%s'\n", CHECK_NULL_STR(input_blob));
+	LogInfo("          -- output_cvg   '%s'\n", CHECK_NULL_STR(coverage_blob));
+	LogInfo("          -- output_bbox  '%s'\n", CHECK_NULL_STR(bbox_blob));
+	LogInfo("          -- mean_pixel   %f\n", mMeanPixel);
+	LogInfo("          -- class_labels %s\n", CHECK_NULL_STR(class_labels));
+	LogInfo("          -- class_colors %s\n", CHECK_NULL_STR(class_colors));
+	LogInfo("          -- threshold    %f\n", threshold);
+	LogInfo("          -- batch_size   %u\n\n", maxBatchSize);
+
+	// create list of output names	
+	std::vector<std::string> output_blobs;
+
+	if( coverage_blob != NULL )
+		output_blobs.push_back(coverage_blob);
+
+	if( bbox_blob != NULL )
+		output_blobs.push_back(bbox_blob);
+	
+	// ONNX SSD models require larger workspace size
+	if( modelTypeFromPath(model) == MODEL_ONNX )
+	{
+		size_t gpuMemFree = 0;
+		size_t gpuMemTotal = 0;
+		
+		CUDA(cudaMemGetInfo(&gpuMemFree, &gpuMemTotal));
+
+		if( gpuMemTotal <= (2048 << 20) )
+			mWorkspaceSize = 512 << 20;
+		else
+			mWorkspaceSize = 2048 << 20;
+	}
+
+	// load the model
+	if( !LoadNetwork(prototxt, model, NULL, input_blob, output_blobs, 
+				  maxBatchSize, precision, device, allowGPUFallback) )
+	{
+		LogError(LOG_TRT "detectNet -- failed to initialize.\n");
+		return false;
+	}
+	
+	// allocate detection sets
+	if( !allocDetections() )
+		return false;
+
+	// load class descriptions
+	loadClassInfo(class_labels);
+	loadClassColors(class_colors);
+
+	// set the specified threshold
+	SetConfidenceThreshold(threshold);
+
+	return true;
+}
+
+
+// Create
+detectNet* detectNet::Create( const char* prototxt, const char* model, float mean_pixel, 
+						const char* class_labels, float threshold,
+						const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
+						uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
+{
+	return Create(prototxt, model, mean_pixel, class_labels, NULL, threshold, input_blob,
+			    coverage_blob, bbox_blob, maxBatchSize, precision, device, allowGPUFallback);
+}
+
+
+// Create
+detectNet* detectNet::Create( const char* prototxt, const char* model, float mean_pixel, 
+						const char* class_labels, const char* class_colors, float threshold,
+						const char* input_blob, const char* coverage_blob, const char* bbox_blob, 
+						uint32_t maxBatchSize, precisionType precision, deviceType device, bool allowGPUFallback )
+{
+	// check for built-in model string
+	if( FindModel(DETECTNET_MODEL_TYPE, model) )
+	{
+		return Create(model, threshold, maxBatchSize, precision, device, allowGPUFallback);
+	}
+	else if( fileExtension(model).length() == 0 )
+	{
+		LogError(LOG_TRT "couldn't find built-in detection model '%s'\n", model);
+		return NULL;
+	}
+
+	// load custom model
+	detectNet* net = new detectNet(mean_pixel);
+	
+	if( !net )
+		return NULL;
+
+	if( !net->init(prototxt, model, class_labels, class_colors, threshold, input_blob, coverage_blob, bbox_blob,
+				maxBatchSize, precision, device, allowGPUFallback) )
+		return NULL;
+
+	return net;
+}
+
+
+// Create (UFF)
+detectNet* detectNet::Create( const char* model, const char* class_labels, float threshold, 
+						const char* input, const Dims3& inputDims, 
+						const char* output, const char* numDetections,
+						uint32_t maxBatchSize, precisionType precision,
+				   		deviceType device, bool allowGPUFallback )
+{
+	detectNet* net = new detectNet();
+	
+	if( !net )
+		return NULL;
+
+	LogInfo("\n");
+	LogInfo("detectNet -- loading detection network model from:\n");
+	LogInfo("          -- model        %s\n", CHECK_NULL_STR(model));
+	LogInfo("          -- input_blob   '%s'\n", CHECK_NULL_STR(input));
+	LogInfo("          -- output_blob  '%s'\n", CHECK_NULL_STR(output));
+	LogInfo("          -- output_count '%s'\n", CHECK_NULL_STR(numDetections));
+	LogInfo("          -- class_labels %s\n", CHECK_NULL_STR(class_labels));
+	LogInfo("          -- threshold    %f\n", threshold);
+	LogInfo("          -- batch_size   %u\n\n", maxBatchSize);
+	
+	// create list of output names	
+	std::vector<std::string> output_blobs;
+
+	if( output != NULL )
+		output_blobs.push_back(output);
+
+	if( numDetections != NULL )
+		output_blobs.push_back(numDetections);
+	
+	// load the model
+	if( !net->LoadNetwork(NULL, model, NULL, input, inputDims, output_blobs, 
+					  maxBatchSize, precision, device, allowGPUFallback) )
+	{
+		LogError(LOG_TRT "detectNet -- failed to initialize.\n");
+		return NULL;
+	}
+	
+	// allocate detection sets
+	if( !net->allocDetections() )
+		return NULL;
+
+	// load class descriptions
+	net->loadClassInfo(class_labels);
+	net->loadClassColors(NULL);
+
+	// set the specified threshold
+	net->SetConfidenceThreshold(threshold);
+
+	return net;
+}
+
+
+// Create
+detectNet* detectNet::Create( const char* network, float threshold, uint32_t maxBatchSize, 
+						precisionType precision, deviceType device, bool allowGPUFallback )
+{
+	nlohmann::json model;
+	
+	if( !DownloadModel(DETECTNET_MODEL_TYPE, network, model) )
+		return NULL;
+	
+	std::string model_dir = "networks/" + model["dir"].get<std::string>() + "/";
+	std::string model_path = model_dir + model["model"].get<std::string>();
+	std::string prototxt = JSON_STR(model["prototxt"]);
+	std::string labels = JSON_STR(model["labels"]);
+	std::string colors = JSON_STR(model["colors"]);
+	
+	if( prototxt.length() > 0 )
+		prototxt = model_dir + prototxt;
+	
+	if( locateFile(labels).length() == 0 )
+		labels = model_dir + labels;
+	
+	if( locateFile(colors).length() == 0 )
+		colors = model_dir + colors;
+	
+	// get model input/output layers
+	std::string input = JSON_STR_DEFAULT(model["input"], DETECTNET_DEFAULT_INPUT);
+	std::string output_cvg = DETECTNET_DEFAULT_COVERAGE;
+	std::string output_bbox = DETECTNET_DEFAULT_BBOX;
+	std::string output_count = "";  // uff
+	
+	nlohmann::json output = model["output"];
+	
+	if( output.is_object() )
+	{
+		if( output["cvg"].is_string() )
+			output_cvg = output["cvg"].get<std::string>();
+		else if( output["scores"].is_string() )
+			output_cvg = output["scores"].get<std::string>();
+		
+		if( output["bbox"].is_string() )
+			output_bbox = output["bbox"].get<std::string>();
+		
+		if( output["count"].is_string() )
+			output_count = output["count"].get<std::string>();
+	}
+	
+	// some older model use the mean_pixel setting
+	float mean_pixel = 0.0f;
+	
+	if( model["mean_pixel"].is_number() )
+		mean_pixel = model["mean_pixel"].get<float>();
+		
+	// UFF models need the input dims parsed
+	Dims3 input_dims;
+	nlohmann::json dims = model["input_dims"];
+	
+	if( dims.is_array() && dims.size() == 3 )
+	{
+		for( uint32_t n=0; n < 3; n++ )
+			input_dims.d[n] = dims[n].get<int>();
+		
+		return Create(model_path.c_str(), labels.c_str(), threshold, input.c_str(), input_dims, 
+				    output_bbox.c_str(), output_count.c_str(), maxBatchSize, precision, device, allowGPUFallback);
+	}
+	
+	return Create(prototxt.c_str(), model_path.c_str(), mean_pixel, labels.c_str(), colors.c_str(), threshold,
+			    input.c_str(), output_cvg.c_str(), output_bbox.c_str(), maxBatchSize, precision, device, allowGPUFallback);
+}
+
+
+// Create
+detectNet* detectNet::Create( int argc, char** argv )
+{
+	return Create(commandLine(argc, argv));
+}
+
+
+// Create
+detectNet* detectNet::Create( const commandLine& cmdLine )
+{
+	detectNet* net = NULL;
+
+	// parse command line parameters
+	const char* modelName = cmdLine.GetString("network");
+	
+	if( !modelName )
+		modelName = cmdLine.GetString("model", "ssd-mobilenet-v2");
+
+	int maxBatchSize = cmdLine.GetInt("batch_size");
+	
+	if( maxBatchSize < 1 )
+		maxBatchSize = DEFAULT_MAX_BATCH_SIZE;
+	
+	// confidence used to be called threshold (support both)
+	float threshold = cmdLine.GetFloat("threshold");
+	
+	if( threshold == 0.0f )
+	{
+		threshold = cmdLine.GetFloat("confidence"); 
+		
+		if( threshold == 0.0f )
+			threshold = DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD;
+	}
+
+	// parse the model type
+if( !FindModel(DETECTNET_MODEL_TYPE, modelName) )
+	{
+		const char* prototxt     = cmdLine.GetString("prototxt");
+		const char* input        = cmdLine.GetString("input_blob");
+		const char* out_blob     = cmdLine.GetString("output_blob");
+		const char* out_cvg      = cmdLine.GetString("output_cvg");
+		const char* out_bbox     = cmdLine.GetString("output_bbox");
+		const char* class_labels = cmdLine.GetString("class_labels");
+		const char* class_colors = cmdLine.GetString("class_colors");
+		
+		if( !input ) 	
+			input = DETECTNET_DEFAULT_INPUT;
+
+		if( !out_blob )
+		{
+			if( !out_cvg )  out_cvg  = DETECTNET_DEFAULT_COVERAGE;
+			if( !out_bbox ) out_bbox = DETECTNET_DEFAULT_BBOX;
+		}
+
+		if( !class_labels )
+			class_labels = cmdLine.GetString("labels");
+
+		if( !class_colors )
+			class_colors = cmdLine.GetString("colors");
+		
+		float meanPixel = cmdLine.GetFloat("mean_pixel");
+
+		net = detectNet::Create(prototxt, modelName, meanPixel, class_labels, class_colors, threshold, input, 
+							out_blob ? NULL : out_cvg, out_blob ? out_blob : out_bbox, maxBatchSize);
+	}
+	else
+	{
+		// create detectNet from pretrained model
+		net = detectNet::Create(modelName, threshold, maxBatchSize);
+	}
+
+	if( !net )
+		return NULL;
+
+	// enable layer profiling if desired
+	if( cmdLine.GetFlag("profile") )
+		net->EnableLayerProfiler();
+
+	// set some additional options
+	net->SetOverlayAlpha(cmdLine.GetFloat("alpha", DETECTNET_DEFAULT_ALPHA));
+	net->SetClusteringThreshold(cmdLine.GetFloat("clustering", DETECTNET_DEFAULT_CLUSTERING_THRESHOLD));
+	
+	// enable tracking if requested
+	net->SetTracker(objectTracker::Create(cmdLine));
+	
+	return net;
+}
+	
+
+// allocDetections
+bool detectNet::allocDetections()
+{
+	// determine max detections
+	if( IsModelType(MODEL_UFF) )	// TODO:  fixme
+	{
+		LogInfo(LOG_TRT "W = %u  H = %u  C = %u\n", DIMS_W(mOutputs[OUTPUT_UFF].dims), DIMS_H(mOutputs[OUTPUT_UFF].dims), DIMS_C(mOutputs[OUTPUT_UFF].dims));
+		mMaxDetections = DIMS_H(mOutputs[OUTPUT_UFF].dims) * DIMS_C(mOutputs[OUTPUT_UFF].dims);
+	}
+	else if( IsModelType(MODEL_ONNX) )
+	{
+		if(mONNXKind == ONNX_SSD) {
+			mNumClasses = DIMS_H(mOutputs[OUTPUT_CONF].dims);
+			mMaxDetections = DIMS_C(mOutputs[OUTPUT_CONF].dims) /** mNumClasses*/;
+		} else { //ONNX_YOLO
+			mNumClasses = DIMS_C(mOutputs[OUTPUT_CONF].dims) - 4; // first 4 floats are bbox, then scores for each class
+			mMaxDetections = DIMS_H(mOutputs[OUTPUT_CONF].dims);
+		}
+		LogInfo(LOG_TRT "detectNet -- number of object classes: %u\n", mNumClasses);
+	}	
+	else
+	{
+		mNumClasses = DIMS_C(mOutputs[OUTPUT_CVG].dims);
+		mMaxDetections = DIMS_W(mOutputs[OUTPUT_CVG].dims) * DIMS_H(mOutputs[OUTPUT_CVG].dims) * mNumClasses;
+		LogInfo(LOG_TRT "detectNet -- number of object classes: %u\n", mNumClasses);
+	}
+
+	LogVerbose(LOG_TRT "detectNet -- maximum bounding boxes:   %u\n", mMaxDetections);
+
+	// allocate array to store detection results
+	const size_t det_size = sizeof(Detection) * mNumDetectionSets * mMaxDetections;
+	
+	if( !cudaAllocMapped((void**)&mDetectionSets, det_size) )
+		return false;
+	
+	memset(mDetectionSets, 0, det_size);
+	return true;
+}
+
+
+// loadClassInfo
+bool detectNet::loadClassInfo( const char* filename )
+{
+	if( !LoadClassLabels(filename, mClassDesc, mClassSynset, mNumClasses) )
+		return false;
+
+	if( IsModelType(MODEL_UFF) )
+		mNumClasses = mClassDesc.size();
+
+	LogInfo(LOG_TRT "detectNet -- number of object classes:  %u\n", mNumClasses);
+	
+	if( filename != NULL )
+		mClassPath = locateFile(filename);	
+	
+	return true;
+}
+
+
+// loadClassColors
+bool detectNet::loadClassColors( const char* filename )
+{
+	return LoadClassColors(filename, &mClassColors, mNumClasses, DETECTNET_DEFAULT_ALPHA);
+}
+
+
+// Detect
+int detectNet::Detect( float* input, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay )
+{
+	return Detect((void*)input, width, height, IMAGE_RGBA32F, detections, overlay);
+}
+
+
+// Detect
+int detectNet::Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection** detections, uint32_t overlay )
+{
+	Detection* det = mDetectionSets + mDetectionSet * GetMaxDetections();
+
+	if( detections != NULL )
+		*detections = det;
+
+	mDetectionSet++;
+
+	if( mDetectionSet >= mNumDetectionSets )
+		mDetectionSet = 0;
+	
+	return Detect(input, width, height, format, det, overlay);
+}
+
+
+// Detect
+int detectNet::Detect( float* input, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay )
+{
+	return Detect((void*)input, width, height, IMAGE_RGBA32F, detections, overlay);
+}
+
+
+// Detect
+int detectNet::Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t overlay )
+{
+	// verify parameters
+	if( !input || width == 0 || height == 0 || !detections )
+	{
+		LogError(LOG_TRT "detectNet::Detect( 0x%p, %u, %u ) -> invalid parameters\n", input, width, height);
+		return -1;
+	}
+	
+	if( !imageFormatIsRGB(format) )
+	{
+		LogError(LOG_TRT "detectNet::Detect() -- unsupported image format (%s)\n", imageFormatToStr(format));
+		LogError(LOG_TRT "                       supported formats are:\n");
+		LogError(LOG_TRT "                          * rgb8\n");		
+		LogError(LOG_TRT "                          * rgba8\n");		
+		LogError(LOG_TRT "                          * rgb32f\n");		
+		LogError(LOG_TRT "                          * rgba32f\n");
+
+		return false;
+	}
+	
+	// apply input pre-processing
+	if( !preProcess(input, width, height, format) )
+		return -1;
+	
+	// process model with TensorRT 
+	PROFILER_BEGIN(PROFILER_NETWORK);
+
+	if( !ProcessNetwork() )
+		return -1;
+	
+	PROFILER_END(PROFILER_NETWORK);
+	
+	// post-processing / clustering
+	const int numDetections = postProcess(input, width, height, format, detections);
+
+	// render the overlay
+	if( overlay != 0 && numDetections > 0 )
+	{
+		if( !Overlay(input, input, width, height, format, detections, numDetections, overlay) )
+			LogError(LOG_TRT "detectNet::Detect() -- failed to render overlay\n");
+	}
+	
+	// wait for GPU to complete work			
+	//CUDA(cudaDeviceSynchronize());	// BUG is this needed here?
+
+	// return the number of detections
+	return numDetections;
+}
+
+
+// preProcess
+bool detectNet::preProcess( void* input, uint32_t width, uint32_t height, imageFormat format )
+{
+	PROFILER_BEGIN(PROFILER_PREPROCESS);
+
+	if( IsModelType(MODEL_UFF) )
+	{
+		// SSD (TensorFlow / UFF)
+		if( CUDA_FAILED(cudaTensorNormBGR(input, format, width, height, 
+								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
+								    make_float2(-1.0f, 1.0f), GetStream())) )
+		{
+			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorNormBGR() failed\n");
+			return false;
+		}
+	}
+	else if( IsModelType(MODEL_ONNX) )
+	{
+		// SSD (PyTorch / ONNX)
+		bool b_failed = true;
+		if(mONNXKind == ONNX_SSD) {
+			b_failed = CUDA_FAILED(cudaTensorNormMeanRGB(input, format, width, height,
+				mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
+				make_float2(0.0f, 1.0f),
+				make_float3(0.5f, 0.5f, 0.5f),
+				make_float3(0.5f, 0.5f, 0.5f),
+				GetStream()));
+		} else { // ONNX_YOLO
+			// I do not know why they keep failing aspec ration of the input images, but I am not like they!
+			b_failed = CUDA_FAILED(cudaTensorNormMeanKeepAspectRGB(input, format, width, height,
+				mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
+				make_float2(0.0f, 1.0f),
+				make_float3(0.0f, 0.0f, 0.0f),
+				make_float3(1.0f, 1.0f, 1.0f),
+				GetStream()));
+		}
+
+		if (b_failed)
+		{
+			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorNormMeanRGB() failed\n");
+			return false;
+		}
+	}
+	else if( IsModelType(MODEL_CAFFE) )
+	{
+		// DetectNet (Caffe)
+		if( CUDA_FAILED(cudaTensorMeanBGR(input, format, width, height,
+								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
+								    make_float3(mMeanPixel, mMeanPixel, mMeanPixel), 
+								    GetStream())) )
+		{
+			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorMeanBGR() failed\n");
+			return false;
+		}
+	}
+	else if( IsModelType(MODEL_ENGINE) )
+	{
+		// https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet
+		if( CUDA_FAILED(cudaTensorNormRGB(input, format, width, height,
+								    mInputs[0].CUDA, GetInputWidth(), GetInputHeight(),
+								    make_float2(0.0f, 1.0f), 
+								    GetStream())) )
+		{
+			LogError(LOG_TRT "detectNet::Detect() -- cudaTensorMeanRGB() failed\n");
+			return false;
+		}
+	}
+	
+	PROFILER_END(PROFILER_PREPROCESS);
+	return true;
+}
+
+
+// postProcess
+int detectNet::postProcess( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections )
+{
+	PROFILER_BEGIN(PROFILER_POSTPROCESS);
+	
+	// parse the bounding boxes
+	int numDetections = 0;
+
+	if( IsModelType(MODEL_UFF) )	
+		numDetections = postProcessSSD_UFF(detections, width, height);
+	else if( IsModelType(MODEL_ONNX) ) {
+		if(mONNXKind == ONNX_SSD) {
+			numDetections = postProcessSSD_ONNX(detections, width, height);
+		} else {
+			numDetections = postProcessYOLO_ONNX(detections, width, height);
+		}
+	} else if( IsModelType(MODEL_CAFFE) )
+		numDetections = postProcessDetectNet(detections, width, height);
+	else if( IsModelType(MODEL_ENGINE) )
+		numDetections = postProcessDetectNet_v2(detections, width, height);
+	else
+		return -1;
+
+	// sort the detections by area
+	sortDetections(detections, numDetections);
+	
+	// verify the bounding boxes are within the bounds of the image
+	for( int n=0; n < numDetections; n++ )
+	{
+		if( detections[n].Top < 0 )
+			detections[n].Top = 0;
+		
+		if( detections[n].Left < 0 )
+			detections[n].Left = 0;
+		
+		if( detections[n].Right >= width )
+			detections[n].Right = width - 1;
+		
+		if( detections[n].Bottom >= height )
+			detections[n].Bottom = height - 1;
+	}
+	
+	// update tracking
+	if( mTracker != NULL && mTracker->IsEnabled() )
+		numDetections = mTracker->Process(input, width, height, format, detections, numDetections);
+	
+	PROFILER_END(PROFILER_POSTPROCESS);	
+	return numDetections;
+}
+
+
+// postProcessSSD_UFF
+int detectNet::postProcessSSD_UFF( Detection* detections, uint32_t width, uint32_t height )
+{
+	int numDetections = 0;
+	
+	const int rawDetections = *(int*)mOutputs[OUTPUT_NUM].CPU;
+	const int rawParameters = DIMS_W(mOutputs[OUTPUT_UFF].dims);
+
+	for( int n=0; n < rawDetections; n++ )
+	{
+		float* object_data = mOutputs[OUTPUT_UFF].CPU + n * rawParameters;
+
+		if( object_data[2] < mConfidenceThreshold )
+			continue;
+
+		detections[numDetections].TrackID   = -1; //numDetections; //(uint32_t)object_data[0];
+		detections[numDetections].ClassID    = (uint32_t)object_data[1];
+		detections[numDetections].Confidence = object_data[2];
+		detections[numDetections].Left       = object_data[3] * width;
+		detections[numDetections].Top        = object_data[4] * height;
+		detections[numDetections].Right      = object_data[5] * width;
+		detections[numDetections].Bottom	  = object_data[6] * height;
+
+		if( detections[numDetections].ClassID >= mNumClasses )
+		{
+			LogError(LOG_TRT "detectNet::Detect() -- detected object has invalid classID (%u)\n", detections[numDetections].ClassID);
+			detections[numDetections].ClassID = 0;
+		}
+
+		if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
+			continue;
+
+		numDetections += clusterDetections(detections, numDetections);
+	}
+
+	return numDetections;
+}
+
+
+// postProcessSSD_ONNX
+int detectNet::postProcessSSD_ONNX( Detection* detections, uint32_t width, uint32_t height )
+{
+	int numDetections = 0;
+	
+	float* conf = mOutputs[OUTPUT_CONF].CPU;
+	float* bbox = mOutputs[OUTPUT_BBOX].CPU;
+
+	const uint32_t numBoxes = DIMS_C(mOutputs[OUTPUT_BBOX].dims);
+	const uint32_t numCoord = DIMS_H(mOutputs[OUTPUT_BBOX].dims);
+
+	for( uint32_t n=0; n < numBoxes; n++ )
+	{
+		uint32_t maxClass = 0;
+		float    maxScore = -1000.0f;
+
+		// class #0 in ONNX-SSD is BACKGROUND (ignored)
+		for( uint32_t m=1; m < mNumClasses; m++ )	
+		{
+			const float score = conf[n * mNumClasses + m];
+
+			if( score < mConfidenceThreshold )
+				continue;
+
+			if( score > maxScore )
+			{
+				maxScore = score;
+				maxClass = m;
+			}
+		}
+
+		// check if there was a detection
+		if( maxClass <= 0 )
+			continue; 
+
+		// populate a new detection entry
+		const float* coord = bbox + n * numCoord;
+
+		detections[numDetections].TrackID   = -1; //numDetections;
+		detections[numDetections].ClassID    = maxClass;
+		detections[numDetections].Confidence = maxScore;
+		detections[numDetections].Left       = coord[0] * width;
+		detections[numDetections].Top        = coord[1] * height;
+		detections[numDetections].Right      = coord[2] * width;
+		detections[numDetections].Bottom	  = coord[3] * height;
+
+		if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
+			continue;
+		
+		numDetections += clusterDetections(detections, numDetections);
+	}
+
+	return numDetections;
+}
+
+// shamlessly stolen form OpenCV
+template <typename T>
+static inline bool SortScorePairDescend(const std::pair<float, T>& pair1, const std::pair<float, T>& pair2)
+{
+	return pair1.first > pair2.first;
+}
+
+// Get max scores with corresponding indices.
+//    scores: a set of scores.
+//    threshold: only consider scores higher than the threshold.
+//    top_k: if -1, keep all; otherwise, keep at most top_k.
+//    score_index_vec: store the sorted (score, index) pair.
+inline void GetMaxScoreIndex(const detectNet::Detection* det, const int count, const float threshold, const int top_k,
+	std::vector<std::pair<float, int> >& score_index_vec)
+{
+	// Generate index score pairs.
+	for (int i = 0; i < count; ++i) {
+		if (det[i].Confidence > threshold) {
+			score_index_vec.push_back(std::make_pair(det[i].Confidence, i));
+		}
+	}
+
+	// Sort the score pair according to the scores in descending order
+	std::stable_sort(score_index_vec.begin(), score_index_vec.end(),
+		SortScorePairDescend<int>);
+
+	// Keep top_k scores if needed.
+	if (top_k > 0 && top_k < (int)score_index_vec.size())
+	{
+		score_index_vec.resize(top_k);
+	}
+}
+
+/**
+ * @brief measure dissimilarity between two sample sets
+ *
+ * computes the complement of the Jaccard Index as described in <https://en.wikipedia.org/wiki/Jaccard_index>.
+ * For rectangles this reduces to computing the intersection over the union.
+ */
+static inline double jaccardDistance(const detectNet::Detection& a, const detectNet::Detection& b) {
+	float Aa = a.Area();
+	float Ab = b.Area();
+
+	if ((Aa + Ab) <= std::numeric_limits<float>::epsilon()) {
+		// jaccard_index = 1 -> distance = 0
+		return 0.0;
+	}
+
+	double Aab = a.IntersectionArea(b);
+	// distance = 1 - jaccard_index
+	return 1.0 - Aab / (Aa + Ab - Aab);
+}
+
+float computeOverlap(const detectNet::Detection& a, const detectNet::Detection& b) {
+	return 1.f - static_cast<float>(jaccardDistance(a, b));
+}
+
+inline void NMSFast_(const detectNet::Detection* det, const int count, const float score_threshold, const float nms_threshold,
+	const float eta, const int top_k, std::vector<int>& indices, int limit = INT_MAX)
+{
+	// Get top_k scores (with corresponding indices).
+	std::vector<std::pair<float, int> > score_index_vec;
+	GetMaxScoreIndex(det, count, score_threshold, top_k, score_index_vec);
+
+	// Do nms.
+	float adaptive_threshold = nms_threshold;
+	indices.clear();
+
+	for (size_t i = 0; i < score_index_vec.size(); ++i) {
+		const int idx = score_index_vec[i].second;
+		bool keep = true;
+		for (int k = 0; k < (int)indices.size() && keep; ++k) {
+			const int kept_idx = indices[k];
+			float overlap = computeOverlap(det[idx], det[kept_idx]);
+			float overlap2 = det[idx].IOU(det[kept_idx]);
+			keep = overlap <= adaptive_threshold;
+		}
+		if (keep) {
+			indices.push_back(idx);
+			if (indices.size() >= limit) {
+				break;
+			}
+		}
+		if (keep && eta < 1 && adaptive_threshold > 0.5) {
+			adaptive_threshold *= eta;
+		}
+	}
+}
+
+int detectNet::postProcessYOLO_ONNX( Detection* detections, uint32_t width, uint32_t height )
+{
+    const std::vector<std::string> classNames = {
+            "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
+            "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
+            "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
+            "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+            "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
+            "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
+            "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
+            "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear",
+            "hair drier", "toothbrush"
+    };
+
+	const uint32_t numChannels = DIMS_C(mOutputs[0].dims);
+	const uint32_t numAnchors = DIMS_H(mOutputs[0].dims);
+
+	//auto numClasses = classNames.size();
+	int numClasses = GetNumClasses();
+
+	if(numClasses > numChannels - 4) {
+		LogError(LOG_TRT "detectNet -- postProcessYOLO_ONNX() numClasses > numAnchors, which is an error\n");
+		return 0;
+	}
+
+	int numDetections = 0;
+
+	const float* output = mOutputs[0].CPU;
+	//cv::Mat output = cv::Mat(numChannels, numAnchors, CV_32F, featureVector.data());
+	//output = output.t();
+
+	int rowStride = numAnchors;
+	int colStride = 1;
+
+	const float ratio = 1.0f / fminf(float(GetInputWidth()) / float(width), float(GetInputHeight())/float(height));
+	const float scale_x = ratio;
+	const float scale_y = ratio;
+
+	//const float scale_x = float(width) / float(GetInputWidth());
+	//const float scale_y = float(height)/float(GetInputHeight());
+
+	// Get all the YOLO proposals
+	for (int i = 0; i < numAnchors; i++) {
+		//const float* rowPtr = output + i * numChannels;
+		const float* rowPtr = output + i * colStride;
+		const float* bboxesPtr = rowPtr;
+		const float* scoresPtr = rowPtr + 4*rowStride;
+
+		//const float* maxSPtr = std::max_element(scoresPtr, scoresPtr + numClasses);
+		//float score = *maxSPtr;
+		//ptrdiff_t class_id = maxSPtr - scoresPtr;
+
+		float score = *scoresPtr;
+		ptrdiff_t class_id = 0;
+		for(int j=1;j<numClasses;++j) {
+			float new_score = *(scoresPtr + j * rowStride);
+			if(new_score > score) {
+				score = new_score;
+				class_id = j;
+			}
+		}
+
+		float x = *(bboxesPtr + 0*rowStride);
+		float y = *(bboxesPtr + 1*rowStride);
+		float w = *(bboxesPtr + 2*rowStride);
+		float h = *(bboxesPtr + 3*rowStride);
+
+		if (score > mConfidenceThreshold) {
+#if 0
+			printf("Class: %d x: %.2f y: %.2f w: %.2f h: %.2f\n", class_id, x, y, w, h);
+			for (int j = 0; j < numClasses; ++j) {
+				printf("%.3f ", *(scoresPtr + j*rowStride));
+			}
+			printf("\n");
+#endif
+
+			float x0 = fminf(fmaxf((x - 0.5f * w)*scale_x, 0.f), width);
+			float y0 = fminf(fmaxf((y - 0.5f * h)*scale_y, 0.f), height);
+			float x1 = fminf(fmaxf((x + 0.5f * w)*scale_x, 0.f), width);
+			float y1 = fminf(fmaxf((y + 0.5f * h)*scale_y, 0.f), height);
+
+			detections[numDetections].TrackID   = -1;
+			detections[numDetections].ClassID    = (uint32_t)class_id;
+			detections[numDetections].Confidence = score;
+
+			detections[numDetections].Left   = x0;
+			detections[numDetections].Top    = y0;
+			detections[numDetections].Right  = x1;
+			detections[numDetections].Bottom = y1;
+
+			numDetections++;
+		}
+	}
+
+	const float nmsThreshold = 0.65f;
+	const int topK = 0;
+	const float eta = 1.0f;
+
+	std::vector<int> indices;
+
+	NMSFast_(detections, numDetections, mConfidenceThreshold, nmsThreshold, eta, topK, indices);
+	const int num_indices = (int)indices.size();
+	if(num_indices != numDetections) {
+		std::sort(indices.begin(), indices.end());
+		int cur_idx = 0;
+		for (int i = 0; i < num_indices; ++i) {
+			detections[cur_idx++] = detections[indices[i]];
+		}
+		numDetections = num_indices;
+	}
+
+	return numDetections;
+}
+
+// postProcessDetectNet
+int detectNet::postProcessDetectNet( Detection* detections, uint32_t width, uint32_t height )
+{
+	float* net_cvg   = mOutputs[OUTPUT_CVG].CPU;
+	float* net_rects = mOutputs[OUTPUT_BBOX].CPU;
+	
+	const int ow  = DIMS_W(mOutputs[OUTPUT_BBOX].dims);	// number of columns in bbox grid in X dimension
+	const int oh  = DIMS_H(mOutputs[OUTPUT_BBOX].dims);	// number of rows in bbox grid in Y dimension
+	const int owh = ow * oh;							// total number of bbox in grid
+	const int cls = GetNumClasses();					// number of object classes in coverage map
+	
+	const float cell_width  = /*width*/ GetInputWidth() / ow;
+	const float cell_height = /*height*/ GetInputHeight() / oh;
+	
+	const float scale_x = float(width) / float(GetInputWidth());
+	const float scale_y = float(height) / float(GetInputHeight());
+
+#ifdef DEBUG_CLUSTERING	
+	LogDebug(LOG_TRT "input width %u height %u\n", GetInputWidth(), GetInputHeight());
+	LogDebug(LOG_TRT "cells x %i  y %i\n", ow, oh);
+	LogDebug(LOG_TRT "cell width %f  height %f\n", cell_width, cell_height);
+	LogDebug(LOG_TRT "scale x %f  y %f\n", scale_x, scale_y);
+#endif
+
+	// extract and cluster the raw bounding boxes that meet the coverage threshold
+	int numDetections = 0;
+
+	for( uint32_t z=0; z < cls; z++ )	// z = current object class
+	{
+		for( uint32_t y=0; y < oh; y++ )
+		{
+			for( uint32_t x=0; x < ow; x++)
+			{
+				const float coverage = net_cvg[z * owh + y * ow + x];
+				
+				if( coverage < mConfidenceThreshold )
+					continue;
+
+				const float mx = x * cell_width;
+				const float my = y * cell_height;
+				
+				const float x1 = (net_rects[0 * owh + y * ow + x] + mx) * scale_x;	// left
+				const float y1 = (net_rects[1 * owh + y * ow + x] + my) * scale_y;	// top
+				const float x2 = (net_rects[2 * owh + y * ow + x] + mx) * scale_x;	// right
+				const float y2 = (net_rects[3 * owh + y * ow + x] + my) * scale_y;	// bottom 
+				
+			#ifdef DEBUG_CLUSTERING
+				LogDebug(LOG_TRT "rect x=%u y=%u  conf=%f  (%f, %f)  (%f, %f) \n", x, y, coverage, x1, y1, x2, y2);
+			#endif		
+
+				// merge with list, checking for overlaps
+				bool detectionMerged = false;
+
+				for( uint32_t n=0; n < numDetections; n++ )
+				{
+					if( detections[n].ClassID == z && detections[n].Expand(x1, y1, x2, y2) )
+					{
+						detectionMerged = true;
+						break;
+					}
+				}
+
+				// create new entry if the detection wasn't merged with another detection
+				if( !detectionMerged )
+				{
+					detections[numDetections].TrackID   = -1; //numDetections;
+					detections[numDetections].ClassID    = z;
+					detections[numDetections].Confidence = coverage;
+				
+					detections[numDetections].Left   = x1;
+					detections[numDetections].Top    = y1;
+					detections[numDetections].Right  = x2;
+					detections[numDetections].Bottom = y2;
+				
+					numDetections++;
+				}
+			}
+		}
+	}
+	
+	return numDetections;
+}
+
+
+// postProcessDetectNet_v2
+int detectNet::postProcessDetectNet_v2( Detection* detections, uint32_t width, uint32_t height )
+{
+	int numDetections = 0;
+	
+	float* conf = mOutputs[OUTPUT_CONF].CPU;
+	float* bbox = mOutputs[OUTPUT_BBOX].CPU;
+
+	const int cells_x  = DIMS_W(mOutputs[OUTPUT_BBOX].dims);	// number of columns in bbox grid in X dimension
+	const int cells_y  = DIMS_H(mOutputs[OUTPUT_BBOX].dims);	// number of rows in bbox grid in Y dimension
+	const int numCells = cells_x * cells_y;					// total number of bbox in grid
+
+	const float cell_width  = GetInputWidth() / cells_x;
+	const float cell_height = GetInputHeight() / cells_y;
+	
+	const float scale_x = float(width) / float(GetInputWidth());
+	const float scale_y = float(height) / float(GetInputHeight());
+
+	const float bbox_norm = 35.0f;  // https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps/blob/edd383cb2e4c7d18ee95ddbf9fdcf4db7803bb6e/tao_triton/python/postprocessing/detectnet_processor.py#L78
+	const float offset = 0.5f;      // https://github.com/NVIDIA-AI-IOT/tao-toolkit-triton-apps/blob/edd383cb2e4c7d18ee95ddbf9fdcf4db7803bb6e/tao_triton/python/postprocessing/detectnet_processor.py#L79
+	
+#ifdef DEBUG_CLUSTERING	
+	LogDebug(LOG_TRT "input width %u height %u\n", GetInputWidth(), GetInputHeight());
+	LogDebug(LOG_TRT "cells x %i  y %i\n", cells_x, cells_y);
+	LogDebug(LOG_TRT "cell width %f  height %f\n", cell_width, cell_height);
+	LogDebug(LOG_TRT "scale x %f  y %f\n", scale_x, scale_y);
+#endif
+
+	for( uint32_t c=0; c < mNumClasses; c++ )   // c = current object class
+	{
+		for( uint32_t y=0; y < cells_y; y++ )
+		{
+			for( uint32_t x=0; x < cells_x; x++)
+			{
+				const float confidence = conf[c * numCells + y * cells_x + x];
+				
+				if( confidence < mConfidenceThreshold )
+					continue;
+
+				const float cx = float(x * cell_width + offset) / bbox_norm;
+				const float cy = float(y * cell_height + offset) / bbox_norm;
+				
+				const float x1 = (bbox[(c * 4 + 0) * numCells + y * cells_x + x] - cx) * -bbox_norm * scale_x;
+				const float y1 = (bbox[(c * 4 + 1) * numCells + y * cells_x + x] - cy) * -bbox_norm * scale_y;
+				const float x2 = (bbox[(c * 4 + 2) * numCells + y * cells_x + x] + cx) *  bbox_norm * scale_x;
+				const float y2 = (bbox[(c * 4 + 3) * numCells + y * cells_x + x] + cy) *  bbox_norm * scale_y;
+								
+			#ifdef DEBUG_CLUSTERING
+				LogDebug(LOG_TRT "rect x=%u y=%u  conf=%f  (%f, %f)  (%f, %f) \n", x, y, confidence, x1, y1, x2, y2);
+			#endif
+				
+				detections[numDetections].TrackID   = -1; //numDetections;
+				detections[numDetections].ClassID    = c;
+				detections[numDetections].Confidence = confidence;
+				detections[numDetections].Left       = x1;
+				detections[numDetections].Top        = y1;
+				detections[numDetections].Right      = x2;
+				detections[numDetections].Bottom	  = y2;
+
+				if( strcmp(GetClassDesc(detections[numDetections].ClassID), "void") == 0 )
+					continue;
+		
+				numDetections += clusterDetections(detections, numDetections);
+			}
+		}
+	}
+	
+	return numDetections;
+}
+	
+	
+// clusterDetections
+int detectNet::clusterDetections( Detection* detections, int n )
+{
+	if( n == 0 )
+		return 1;
+
+	// test each detection to see if it intersects
+	for( int m=0; m < n; m++ )
+	{
+		if( detections[n].Intersects(detections[m], mClusteringThreshold) )	// TODO NMS or different threshold for same classes?
+		{
+			// if the intersecting detections have different classes, pick the one with highest confidence
+			// otherwise if they have the same object class, expand the detection bounding box
+		#ifdef CLUSTER_INTERCLASS
+			if( detections[n].ClassID != detections[m].ClassID )
+			{
+				if( detections[n].Confidence > detections[m].Confidence )
+				{
+					detections[m] = detections[n];
+
+					detections[m].TrackID = -1; //m;
+					detections[m].ClassID = detections[n].ClassID;
+					detections[m].Confidence = detections[n].Confidence;	
+				}
+				
+				return 0; // merged detection
+			}
+			else
+		#else
+			if( detections[n].ClassID == detections[m].ClassID )
+		#endif
+			{
+				detections[m].Expand(detections[n]);
+				detections[m].Confidence = fmaxf(detections[n].Confidence, detections[m].Confidence);
+
+				return 0; // merged detection
+			}
+		}
+	}
+
+	return 1;	// new detection
+}
+
+
+// sortDetections (by area)
+void detectNet::sortDetections( Detection* detections, int numDetections )
+{
+	if( numDetections < 2 )
+		return;
+
+	// order by area (descending) or confidence (ascending)
+	for( int i=0; i < numDetections-1; i++ )
+	{
+		for( int j=0; j < numDetections-i-1; j++ )
+		{
+			if( detections[j].Area() < detections[j+1].Area() ) //if( detections[j].Confidence > detections[j+1].Confidence )
+			{
+				const Detection det = detections[j];
+				detections[j] = detections[j+1];
+				detections[j+1] = det;
+			}
+		}
+	}
+
+	// renumber the instance ID's
+	//for( int i=0; i < numDetections; i++ )
+	//	detections[i].TrackID = i;	
+}
+
+
+// from detectNet.cu
+cudaError_t cudaDetectionOverlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, detectNet::Detection* detections, int numDetections, float4* colors );
+
+// Overlay
+bool detectNet::Overlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t numDetections, uint32_t flags )
+{
+	PROFILER_BEGIN(PROFILER_VISUALIZE);
+
+	if( flags == 0 )
+	{
+		LogError(LOG_TRT "detectNet -- Overlay() was called with OVERLAY_NONE, returning false\n");
+		return false;
+	}
+
+	// if input and output are different images, copy the input to the output first
+	// then overlay the bounding boxes, ect. on top of the output image
+	if( input != output )
+	{
+		if( CUDA_FAILED(cudaMemcpy(output, input, imageFormatSize(format, width, height), cudaMemcpyDeviceToDevice)) )
+		{
+			LogError(LOG_TRT "detectNet -- Overlay() failed to copy input image to output image\n");
+			return false;
+		}
+	}
+
+	// make sure there are actually detections
+	if( numDetections <= 0 )
+	{
+		PROFILER_END(PROFILER_VISUALIZE);
+		return true;
+	}
+
+	// bounding box overlay
+	if( flags & OVERLAY_BOX )
+	{
+		if( CUDA_FAILED(cudaDetectionOverlay(input, output, width, height, format, detections, numDetections, mClassColors)) )
+			return false;
+	}
+	
+	// bounding box lines
+	if( flags & OVERLAY_LINES )
+	{
+		for( uint32_t n=0; n < numDetections; n++ )
+		{
+			const Detection* d = detections + n;
+			const float4& color = mClassColors[d->ClassID];
+
+			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Top, d->Right, d->Top, color, mLineWidth));
+			CUDA(cudaDrawLine(input, output, width, height, format, d->Right, d->Top, d->Right, d->Bottom, color, mLineWidth));
+			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Bottom, d->Right, d->Bottom, color, mLineWidth));
+			CUDA(cudaDrawLine(input, output, width, height, format, d->Left, d->Top, d->Left, d->Bottom, color, mLineWidth));
+		}
+	}
+			
+	// class label overlay
+	if( (flags & OVERLAY_LABEL) || (flags & OVERLAY_CONFIDENCE) || (flags & OVERLAY_TRACKING) )
+	{
+		static cudaFont* font = NULL;
+
+		// make sure the font object is created
+		if( !font )
+		{
+			font = cudaFont::Create(adaptFontSize(width));  // 20.0f
+	
+			if( !font )
+			{
+				LogError(LOG_TRT "detectNet -- Overlay() was called with OVERLAY_FONT, but failed to create cudaFont()\n");
+				return false;
+			}
+		}
+
+		// draw each object's description
+	#ifdef BATCH_TEXT
+		std::vector<std::pair<std::string, int2>> labels;
+	#endif 
+		for( uint32_t n=0; n < numDetections; n++ )
+		{
+			const char* className  = GetClassDesc(detections[n].ClassID);
+			const float confidence = detections[n].Confidence * 100.0f;
+			const int2  position   = make_int2(detections[n].Left+5, detections[n].Top+3);
+			
+			char buffer[256];
+			char* str = buffer;
+			
+			if( flags & OVERLAY_LABEL )
+				str += sprintf(str, "%s ", className);
+			
+			if( flags & OVERLAY_TRACKING && detections[n].TrackID >= 0 )
+				str += sprintf(str, "%i ", detections[n].TrackID);
+			
+			if( flags & OVERLAY_CONFIDENCE )
+				str += sprintf(str, "%.1f%%", confidence);
+
+		#ifdef BATCH_TEXT
+			labels.push_back(std::pair<std::string, int2>(buffer, position));
+		#else
+			float4 color = make_float4(255,255,255,255);
+		
+			if( detections[n].TrackID >= 0 )
+				color.w *= 1.0f - (fminf(detections[n].TrackLost, 15.0f) / 15.0f);
+			
+			font->OverlayText(output, format, width, height, buffer, position.x, position.y, color);
+		#endif
+		}
+
+	#ifdef BATCH_TEXT
+		font->OverlayText(output, format, width, height, labels, make_float4(255,255,255,255));
+	#endif
+	}
+	
+	PROFILER_END(PROFILER_VISUALIZE);
+	return true;
+}
+
+
+// OverlayFlagsFromStr
+uint32_t detectNet::OverlayFlagsFromStr( const char* str_user )
+{
+	if( !str_user )
+		return OVERLAY_DEFAULT;
+
+	// copy the input string into a temporary array,
+	// because strok modifies the string
+	const size_t str_length = strlen(str_user);
+	const size_t max_length = 256;
+	
+	if( str_length == 0 )
+		return OVERLAY_DEFAULT;
+
+	if( str_length >= max_length )
+	{
+		LogError(LOG_TRT "detectNet::OverlayFlagsFromStr() overlay string exceeded max length of %zu characters ('%s')", max_length, str_user);
+		return OVERLAY_DEFAULT;
+	}
+	
+	char str[max_length];
+	strcpy(str, str_user);
+
+	// tokenize string by delimiters ',' and '|'
+	const char* delimiters = ",|";
+	char* token = strtok(str, delimiters);
+
+	if( !token )
+		return OVERLAY_DEFAULT;
+
+	// look for the tokens:  "box", "label", "default", and "none"
+	uint32_t flags = OVERLAY_NONE;
+
+	while( token != NULL )
+	{
+		if( strcasecmp(token, "box") == 0 )
+			flags |= OVERLAY_BOX;
+		else if( strcasecmp(token, "label") == 0 || strcasecmp(token, "labels") == 0 )
+			flags |= OVERLAY_LABEL;
+		else if( strcasecmp(token, "conf") == 0 || strcasecmp(token, "confidence") == 0 )
+			flags |= OVERLAY_CONFIDENCE;
+		else if( strcasecmp(token, "track") == 0 || strcasecmp(token, "tracking") == 0 )
+			flags |= OVERLAY_TRACKING;
+		else if( strcasecmp(token, "line") == 0 || strcasecmp(token, "lines") == 0 )
+			flags |= OVERLAY_LINES;
+		else if( strcasecmp(token, "default") == 0 )
+			flags |= OVERLAY_DEFAULT;
+
+		token = strtok(NULL, delimiters);
+	}	
+
+	return flags;
+}
+
+
+// SetOverlayAlpha
+void detectNet::SetOverlayAlpha( float alpha )
+{
+	const uint32_t numClasses = GetNumClasses();
+
+	for( uint32_t n=0; n < numClasses; n++ )
+		mClassColors[n].w = alpha;
+	
+	mOverlayAlpha = alpha;
+}
diff --git a/c/detectNet.h b/c/detectNet.h
index 7bc0f293..9d77e0ca 100644
--- a/c/detectNet.h
+++ b/c/detectNet.h
@@ -1,585 +1,587 @@
-/*
- * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
- * DEALINGS IN THE SOFTWARE.
- */
- 
-#ifndef __DETECT_NET_H__
-#define __DETECT_NET_H__
-
-
-#include "tensorNet.h"
-
-
-/**
- * Name of default input blob for DetectNet caffe model.
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_INPUT   "data"
-
-/**
- * Name of default output blob of the coverage map for DetectNet caffe model.
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_COVERAGE  "coverage"
-
-/**
- * Name of default output blob of the grid of bounding boxes for DetectNet caffe model.
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_BBOX  "bboxes"
-
-/**
- * Default value of the minimum detection threshold
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD 0.5f
-
-/**
- * Default value of the clustering area-of-overlap threshold
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_CLUSTERING_THRESHOLD 0.75f
-
-/**
- * Default value of the minimum detection threshold
- * @deprecated please use DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD instead
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_THRESHOLD  DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD
-
-/**
- * Default alpha blending value used during overlay
- * @ingroup detectNet
- */
-#define DETECTNET_DEFAULT_ALPHA 120
-
-/**
- * The model type for detectNet in data/networks/models.json
- * @ingroup detectNet
- */
-#define DETECTNET_MODEL_TYPE "detection"
-
-/**
- * Standard command-line options able to be passed to detectNet::Create()
- * @ingroup detectNet
- */
-#define DETECTNET_USAGE_STRING  "detectNet arguments: \n" 					\
-		  "  --network=NETWORK     pre-trained model to load, one of the following:\n" 		\
-		  "                            * ssd-mobilenet-v1\n" 							\
-		  "                            * ssd-mobilenet-v2 (default)\n" 					\
-		  "                            * ssd-inception-v2\n" 							\
-		  "                            * peoplenet\n"                                        \
-		  "                            * peoplenet-pruned\n"                                 \
-		  "                            * dashcamnet\n"                                       \
-		  "                            * trafficcamnet\n"                                    \
-		  "                            * facedetect\n"                                       \
-		  "  --model=MODEL         path to custom model to load (caffemodel, uff, or onnx)\n" 					\
-		  "  --prototxt=PROTOTXT   path to custom prototxt to load (for .caffemodel only)\n" 					\
-		  "  --labels=LABELS       path to text file containing the labels for each class\n" 					\
-		  "  --input-blob=INPUT    name of the input layer (default is '" DETECTNET_DEFAULT_INPUT "')\n" 			\
-		  "  --output-cvg=COVERAGE name of the coverage/confidence output layer (default is '" DETECTNET_DEFAULT_COVERAGE "')\n" 	\
-		  "  --output-bbox=BOXES   name of the bounding output layer (default is '" DETECTNET_DEFAULT_BBOX "')\n" 	\
-		  "  --mean-pixel=PIXEL    mean pixel value to subtract from input (default is 0.0)\n"					\
-		  "  --confidence=CONF     minimum confidence threshold for detection (default is 0.5)\n"		           	\
-		  "  --clustering=CLUSTER  minimum overlapping area threshold for clustering (default is 0.75)\n"             \
-            "  --alpha=ALPHA         overlay alpha blending value, range 0-255 (default: 120)\n"					\
-		  "  --overlay=OVERLAY     detection overlay flags (e.g. --overlay=box,labels,conf)\n"					\
-		  "                        valid combinations are:  'box', 'lines', 'labels', 'conf', 'none'\n"			\
-		  "  --profile             enable layer profiling in TensorRT\n\n"				\
-
-
-// forward declarations
-class objectTracker;
-
-
-/**
- * Object recognition and localization networks with TensorRT support.
- * @ingroup detectNet
- */
-class detectNet : public tensorNet
-{
-public:
-	/**
-	 * Object Detection result.
-	 */
-	struct Detection
-	{
-		// Detection Info
-		uint32_t ClassID;	/**< Class index of the detected object. */
-		float Confidence;	/**< Confidence value of the detected object. */
-
-		// Tracking Info
-		int TrackID;		/**< Unique tracking ID (or -1 if untracked) */
-		int TrackStatus;	/**< -1 for dropped, 0 for initializing, 1 for active/valid */ 
-		int TrackFrames;	/**< The number of frames the object has been re-identified for */
-		int TrackLost;   	/**< The number of consecutive frames tracking has been lost for */
-		
-		// Bounding Box Coordinates
-		float Left;		/**< Left bounding box coordinate (in pixels) */
-		float Right;		/**< Right bounding box coordinate (in pixels) */
-		float Top;		/**< Top bounding box cooridnate (in pixels) */
-		float Bottom;		/**< Bottom bounding box coordinate (in pixels) */
-
-		/**< Calculate the width of the object */
-		inline float Width() const											{ return Right - Left; }
-
-		/**< Calculate the height of the object */
-		inline float Height() const											{ return Bottom - Top; }
-
-		/**< Calculate the area of the object */
-		inline float Area() const											{ return Width() * Height(); }
-
-		/**< Calculate the width of the bounding box */
-		static inline float Width( float x1, float x2 )							{ return x2 - x1; }
-
-		/**< Calculate the height of the bounding box */
-		static inline float Height( float y1, float y2 )							{ return y2 - y1; }
-
-		/**< Calculate the area of the bounding box */
-		static inline float Area( float x1, float y1, float x2, float y2 )			{ return Width(x1,x2) * Height(y1,y2); }
-
-		/**< Return the center of the object */
-		inline void Center( float* x, float* y ) const							{ if(x) *x = Left + Width() * 0.5f; if(y) *y = Top + Height() * 0.5f; }
-
-		/**< Return true if the coordinate is inside the bounding box */
-		inline bool Contains( float x, float y ) const							{ return x >= Left && x <= Right && y >= Top && y <= Bottom; }
-		
-		/**< Return true if the bounding boxes intersect and exceeds area % threshold */	
-		inline bool Intersects( const Detection& det, float areaThreshold=0.0f ) const  { return (IntersectionArea(det) / fmaxf(Area(), det.Area()) > areaThreshold); }
-	
-		/**< Return true if the bounding boxes intersect and exceeds area % threshold */	
-		inline bool Intersects( float x1, float y1, float x2, float y2, float areaThreshold=0.0f ) const  { return (IntersectionArea(x1,y1,x2,y2) / fmaxf(Area(), Area(x1,y1,x2,y2)) > areaThreshold); }
-	
-		/**< Return the area of the bounding box intersection */
-		inline float IntersectionArea( const Detection& det ) const					{ return IntersectionArea(det.Left, det.Top, det.Right, det.Bottom); }
-
-		/**< Return the area of the bounding box intersection */
-		inline float IntersectionArea( float x1, float y1, float x2, float y2 ) const	{ if(!Overlaps(x1,y1,x2,y2)) return 0.0f; return (fminf(Right, x2) - fmaxf(Left, x1)) * (fminf(Bottom, y2) - fmaxf(Top, y1)); }
-
-		/**< Calculate the Intersection-Over-Union (IOU) ratio */
-		inline float IOU( const Detection& det ) const							{ return IOU(det.Left, det.Top, det.Right, det.Bottom); }
-		
-		/**< Calculate the Intersection-Over-Union (IOU) ratio */
-		inline float IOU( float x1, float y1, float x2, float y2 ) const;
-		
-		/**< Return true if the bounding boxes overlap */
-		inline bool Overlaps( const Detection& det ) const						{ return !(det.Left > Right || det.Right < Left || det.Top > Bottom || det.Bottom < Top); }
-		
-		/**< Return true if the bounding boxes overlap */
-		inline bool Overlaps( float x1, float y1, float x2, float y2 ) const			{ return !(x1 > Right || x2 < Left || y1 > Bottom || y2 < Top); }
-		
-		/**< Expand the bounding box if they overlap (return true if so) */
-		inline bool Expand( float x1, float y1, float x2, float y2 ) 	     		{ if(!Overlaps(x1, y1, x2, y2)) return false; Left = fminf(x1, Left); Top = fminf(y1, Top); Right = fmaxf(x2, Right); Bottom = fmaxf(y2, Bottom); return true; }
-		
-		/**< Expand the bounding box if they overlap (return true if so) */
-		inline bool Expand( const Detection& det )      							{ if(!Overlaps(det)) return false; Left = fminf(det.Left, Left); Top = fminf(det.Top, Top); Right = fmaxf(det.Right, Right); Bottom = fmaxf(det.Bottom, Bottom); return true; }
-
-		/**< Reset all member variables to zero */
-		inline void Reset()													{ ClassID = 0; Confidence = 0; TrackID = -1; TrackStatus = -1; TrackFrames = 0; TrackLost = 0; Left = 0; Right = 0; Top = 0; Bottom = 0; } 								
-
-		/**< Default constructor */
-		inline Detection()													{ Reset(); }
-	};
-
-	/**
-	 * Overlay flags (can be OR'd together).
-	 */
-	enum OverlayFlags
-	{
-		OVERLAY_NONE       = 0,			/**< No overlay. */
-		OVERLAY_BOX        = (1 << 0),	/**< Overlay the object bounding boxes (filled) */
-		OVERLAY_LABEL 	    = (1 << 1),	/**< Overlay the class description labels */
-		OVERLAY_CONFIDENCE = (1 << 2),	/**< Overlay the detection confidence values */
-		OVERLAY_TRACKING   = (1 << 3),	/**< Overlay tracking information (like track ID) */
-		OVERLAY_LINES      = (1 << 4),     /**< Overlay the bounding box lines (unfilled) */
-		OVERLAY_DEFAULT    = OVERLAY_BOX|OVERLAY_LABEL|OVERLAY_CONFIDENCE, /**< The default choice of overlay */
-	};
-
-	/**
-	 * Parse a string sequence into OverlayFlags enum.
-	 * Valid flags are "none", "box", "label", and "conf" and it is possible to combine flags
-	 * (bitwise OR) together with commas or pipe (|) symbol.  For example, the string sequence
-	 * "box,label,conf" would return the flags `OVERLAY_BOX|OVERLAY_LABEL|OVERLAY_CONFIDENCE`.
-	 */
-	static uint32_t OverlayFlagsFromStr( const char* flags );
-
-	/**
-	 * Load a pre-trained model
-	 * @param network the pre-trained model to load (@see DETECTNET_USAGE_STRING for models)
-	 * @param threshold default minimum threshold for detection
-	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
-	 */
-	static detectNet* Create( const char* network="ssd-mobilenet-v2", float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
-						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST, 
-						 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
-						 
-	/**
-	 * Load a custom network instance
-	 * @param prototxt_path File path to the deployable network prototxt
-	 * @param model_path File path to the caffemodel
-	 * @param mean_pixel Input transform subtraction value (use 0.0 if the network already does this)
-	 * @param class_labels File path to list of class name labels
-	 * @param threshold default minimum threshold for detection
-	 * @param input Name of the input layer blob.
-	 * @param coverage Name of the output coverage classifier layer blob, which contains the confidence values for each bbox.
-	 * @param bboxes Name of the output bounding box layer blob, which contains a grid of rectangles in the image.
-	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
-	 */
-	static detectNet* Create( const char* prototxt_path, const char* model_path, float mean_pixel=0.0f, 
-						 const char* class_labels=NULL, float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
-						 const char* input = DETECTNET_DEFAULT_INPUT, 
-						 const char* coverage = DETECTNET_DEFAULT_COVERAGE, 
-						 const char* bboxes = DETECTNET_DEFAULT_BBOX,
-						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-						 precisionType precision=TYPE_FASTEST,
-				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
-	
-	/**
-	 * Load a custom network instance
-	 * @param prototxt_path File path to the deployable network prototxt
-	 * @param model_path File path to the caffemodel
-	 * @param mean_pixel Input transform subtraction value (use 0.0 if the network already does this)
-	 * @param class_labels File path to list of class name labels
-	 * @param class_colors File path to list of class colors
-	 * @param threshold default minimum threshold for detection
-	 * @param input Name of the input layer blob.
-	 * @param coverage Name of the output coverage classifier layer blob, which contains the confidence values for each bbox.
-	 * @param bboxes Name of the output bounding box layer blob, which contains a grid of rectangles in the image.
-	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
-	 */
-	static detectNet* Create( const char* prototxt_path, const char* model_path, float mean_pixel, 
-						 const char* class_labels, const char* class_colors,
-						 float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
-						 const char* input = DETECTNET_DEFAULT_INPUT, 
-						 const char* coverage = DETECTNET_DEFAULT_COVERAGE, 
-						 const char* bboxes = DETECTNET_DEFAULT_BBOX,
-						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-						 precisionType precision=TYPE_FASTEST,
-				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
-						 
-	/**
-	 * Load a custom network instance of a UFF model
-	 * @param model_path File path to the UFF model
-	 * @param class_labels File path to list of class name labels
-	 * @param threshold default minimum threshold for detection
-	 * @param input Name of the input layer blob.
-	 * @param inputDims Dimensions of the input layer blob.
-	 * @param output Name of the output layer blob containing the bounding boxes, ect.
-	 * @param numDetections Name of the output layer blob containing the detection count.
-	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
-	 */
-	static detectNet* Create( const char* model_path, const char* class_labels, float threshold, 
-						 const char* input, const Dims3& inputDims, 
-						 const char* output, const char* numDetections,
-						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-						 precisionType precision=TYPE_FASTEST,
-				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
-
-	/**
-	 * Load a new network instance by parsing the command line.
-	 */
-	static detectNet* Create( int argc, char** argv );
-	
-	/**
-	 * Load a new network instance by parsing the command line.
-	 */
-	static detectNet* Create( const commandLine& cmdLine );
-	
-	/**
-	 * Usage string for command line arguments to Create()
-	 */
-	static inline const char* Usage() 		{ return DETECTNET_USAGE_STRING; }
-
-	/**
-	 * Destory
-	 */
-	virtual ~detectNet();
-	
-	/**
-	 * Detect object locations from an image, returning an array containing the detection results.
-	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	template<typename T> int Detect( T* image, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT )		{ return Detect((void*)image, width, height, imageFormatFromType<T>(), detections, overlay); }
-	
-	/**
-	 * Detect object locations in an image, into an array of the results allocated by the user.
-	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
-	 *                        @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	template<typename T> int Detect( T* image, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT )		{ return Detect((void*)image, width, height, imageFormatFromType<T>(), detections, overlay); }
-	
-	/**
-	 * Detect object locations from an image, returning an array containing the detection results.
-	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	int Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT );
-
-	/**
-	 * Detect object locations from an image, into an array of the results allocated by the user.
-	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
-	 *             @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	int Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT );
-	
-	/**
-	 * Detect object locations from an RGBA image, returning an array containing the detection results.
-      * @deprecated this overload of Detect() provides legacy compatibility with `float*` type (RGBA32F). 
-	 * @param[in]  input float4 RGBA input image in CUDA device memory.
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	int Detect( float* input, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT );
-
-	/**
-	 * Detect object locations in an RGBA image, into an array of the results allocated by the user.
-	 * @deprecated this overload of Detect() provides legacy compatibility with `float*` type (RGBA32F). 
-	 * @param[in]  input float4 RGBA input image in CUDA device memory.
-	 * @param[in]  width width of the input image in pixels.
-	 * @param[in]  height height of the input image in pixels.
-	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
-	 *                        @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
-	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
-	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
-	 */
-	int Detect( float* input, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT );
-	
-	/**
-	 * Draw the detected bounding boxes overlayed on an RGBA image.
-	 * @note Overlay() will automatically be called by default by Detect(), if the overlay parameter is true 
-	 * @param input input image in CUDA device memory.
-	 * @param output output image in CUDA device memory.
-	 * @param detections Array of detections allocated in CUDA device memory.
-	 */
-	template<typename T> bool Overlay( T* input, T* output, uint32_t width, uint32_t height, Detection* detections, uint32_t numDetections, uint32_t flags=OVERLAY_DEFAULT )			{ return Overlay(input, output, width, height, imageFormatFromType<T>(), detections, numDetections, flags); }
-	
-	/**
-	 * Draw the detected bounding boxes overlayed on an RGBA image.
-	 * @note Overlay() will automatically be called by default by Detect(), if the overlay parameter is true 
-	 * @param input input image in CUDA device memory.
-	 * @param output output image in CUDA device memory.
-	 * @param detections Array of detections allocated in CUDA device memory.
-	 */
-	bool Overlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t numDetections, uint32_t flags=OVERLAY_DEFAULT );
-	
-	/**
-	 * Retrieve the minimum threshold for detection.
-	 * @deprecated please use GetConfidenceThreshold() instead
-	 */
-	inline float GetThreshold() const							{ return mConfidenceThreshold; }
-
-	/**
-	 * Set the minimum threshold for detection.
-	 * @deprecated please use SetConfidenceThreshold() instead
-	 */
-	inline void SetThreshold( float threshold ) 					{ mConfidenceThreshold = threshold; }
-
-	/**
-	 * Retrieve the minimum threshold for detection.
-	 */
-	inline float GetConfidenceThreshold() const					{ return mConfidenceThreshold; }
-
-	/**
-	 * Set the minimum threshold for detection.
-	 */
-	inline void SetConfidenceThreshold( float threshold ) 			{ mConfidenceThreshold = threshold; }
-
-	/**
-	 * Retrieve the overlapping area % threshold for clustering.
-	 */
-	inline float GetClusteringThreshold() const					{ return mClusteringThreshold; }
-
-	/**
-	 * Set the overlapping area % threshold for clustering.
-	 */
-	inline void SetClusteringThreshold( float threshold ) 			{ mClusteringThreshold = threshold; }
-
-	/**
-	 * Get the object tracker being used.
-	 */
-	inline objectTracker* GetTracker() const					{ return mTracker; }
-	
-	/**
-	 * Set the object tracker to be used.
-	 */
-	inline void SetTracker( objectTracker* tracker ) 				{ mTracker = tracker; }
-	
-	/**
-	 * Retrieve the maximum number of simultaneous detections the network supports.
-	 * Knowing this is useful for allocating the buffers to store the output detection results.
-	 */
-	inline uint32_t GetMaxDetections() const					{ return mMaxDetections; } 
-		
-	/**
-	 * Retrieve the number of object classes supported in the detector
-	 */
-	inline uint32_t GetNumClasses() const						{ return mNumClasses; }
-
-	/**
-	 * Retrieve the description of a particular class.
-	 */
-	inline const char* GetClassLabel( uint32_t index ) const		{ return GetClassDesc(index); }
-	
-	/**
-	 * Retrieve the description of a particular class.
-	 */
-	inline const char* GetClassDesc( uint32_t index )	const		{ if(index >= mClassDesc.size()) { printf("invalid class %u\n", index); return "Invalid"; } return mClassDesc[index].c_str(); }
-	
-	/**
-	 * Retrieve the class synset category of a particular class.
-	 */
-	inline const char* GetClassSynset( uint32_t index ) const		{ return mClassSynset[index].c_str(); }
-	
-	/**
- 	 * Retrieve the path to the file containing the class descriptions.
-	 */
-	inline const char* GetClassPath() const						{ return mClassPath.c_str(); }
-
-	/**
-	 * Retrieve the RGBA visualization color a particular class.
-	 */
-	inline float4 GetClassColor( uint32_t classIndex ) const		{ return mClassColors[classIndex]; }
-
-	/**
-	 * Set the visualization color of a particular class of object.
-	 */
-	inline void SetClassColor( uint32_t classIndex, const float4& color )						{ mClassColors[classIndex] = color; }
-	
-	/**
-	 * Set the visualization color of a particular class of object.
-	 */
-	inline void SetClassColor( uint32_t classIndex, float r, float g, float b, float a=255.0f )	{ mClassColors[classIndex] = make_float4(r,g,b,a); }
-	
-	/**
-	 * Retrieve the line width used during overlay when OVERLAY_LINES is used.
-	 */
-	inline float GetLineWidth() const							{ return mLineWidth; }
-	
-	/**
-	 * Set the line width used during overlay when OVERLAY_LINES is used.
-	 */
-	inline void SetLineWidth( float width )						{ mLineWidth = width; }
-	
-	/**
-	 * Retrieve the overlay alpha blending value for classes that don't have it explicitly set (between 0-255).
-	 */
-	inline float GetOverlayAlpha() const						{ return mOverlayAlpha; }
-	
-	/**
- 	 * Set overlay alpha blending value for all classes (between 0-255).
-	 */
-	void SetOverlayAlpha( float alpha );
-
-protected:
-
-	// constructor
-	detectNet( float meanPixel=0.0f );
-
-	bool allocDetections();
-
-	bool loadClassInfo( const char* filename );
-	bool loadClassColors( const char* filename );
-	
-	bool init( const char* prototxt_path, const char* model_path, const char* class_labels, const char* class_colors,
-			 float threshold, const char* input, const char* coverage, const char* bboxes, uint32_t maxBatchSize, 
-			 precisionType precision, deviceType device, bool allowGPUFallback );
-	
-	bool preProcess( void* input, uint32_t width, uint32_t height, imageFormat format );
-	int postProcess( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections );
-	
-	int postProcessSSD_UFF( Detection* detections, uint32_t width, uint32_t height );
-	int postProcessSSD_ONNX( Detection* detections, uint32_t width, uint32_t height );
-	int postProcessDetectNet( Detection* detections, uint32_t width, uint32_t height );
-	int postProcessDetectNet_v2( Detection* detections, uint32_t width, uint32_t height );
-	
-	int clusterDetections( Detection* detections, int n );
-	void sortDetections( Detection* detections, int numDetections );
-
-	objectTracker* mTracker;
-	
-	float mConfidenceThreshold;	 // TODO change this to per-class
-	float mClusteringThreshold;	 // TODO change this to per-class
-	
-	float mMeanPixel;
-	float mLineWidth;
-	float mOverlayAlpha;
-	
-	float4* mClassColors;
-	
-	std::vector<std::string> mClassDesc;
-	std::vector<std::string> mClassSynset;
-
-	std::string mClassPath;
-	uint32_t	  mNumClasses;
-
-	Detection* mDetectionSets;	// list of detections, mNumDetectionSets * mMaxDetections
-	uint32_t   mDetectionSet;	// index of next detection set to use
-	uint32_t	 mMaxDetections;	// number of raw detections in the grid
-
-	static const uint32_t mNumDetectionSets = 16; // size of detection ringbuffer
-};
-
-
-// bounding box IOU
-inline float detectNet::Detection::IOU( float x1, float y1, float x2, float y2 ) const		
-{
-	const float overlap_x0 = fmaxf(Left, x1);
-	const float overlap_y0 = fmaxf(Top, y1);
-	const float overlap_x1 = fminf(Right, x2);
-	const float overlap_y1 = fminf(Bottom, y2);
-	
-	// check if there is an overlap
-	if( (overlap_x1 - overlap_x0 <= 0) || (overlap_y1 - overlap_y0 <= 0) )
-		return 0;
-	
-	// calculate the ratio of the overlap to each ROI size and the unified size
-	const float size_1 = Area();
-	const float size_2 = Area(x1, y1, x2, y2);
-	
-	const float size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0);
-	const float size_union = size_1 + size_2 - size_intersection;
-	
-	return size_intersection / size_union;
-}
-
-
-#endif
+/*
+ * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+ 
+#ifndef __DETECT_NET_H__
+#define __DETECT_NET_H__
+
+
+#include "tensorNet.h"
+
+
+/**
+ * Name of default input blob for DetectNet caffe model.
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_INPUT   "data"
+
+/**
+ * Name of default output blob of the coverage map for DetectNet caffe model.
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_COVERAGE  "coverage"
+
+/**
+ * Name of default output blob of the grid of bounding boxes for DetectNet caffe model.
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_BBOX  "bboxes"
+
+/**
+ * Default value of the minimum detection threshold
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD 0.5f
+
+/**
+ * Default value of the clustering area-of-overlap threshold
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_CLUSTERING_THRESHOLD 0.75f
+
+/**
+ * Default value of the minimum detection threshold
+ * @deprecated please use DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD instead
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_THRESHOLD  DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD
+
+/**
+ * Default alpha blending value used during overlay
+ * @ingroup detectNet
+ */
+#define DETECTNET_DEFAULT_ALPHA 120
+
+/**
+ * The model type for detectNet in data/networks/models.json
+ * @ingroup detectNet
+ */
+#define DETECTNET_MODEL_TYPE "detection"
+
+/**
+ * Standard command-line options able to be passed to detectNet::Create()
+ * @ingroup detectNet
+ */
+#define DETECTNET_USAGE_STRING  "detectNet arguments: \n" 					\
+		  "  --network=NETWORK     pre-trained model to load, one of the following:\n" 		\
+		  "                            * ssd-mobilenet-v1\n" 							\
+		  "                            * ssd-mobilenet-v2 (default)\n" 					\
+		  "                            * ssd-inception-v2\n" 							\
+		  "                            * peoplenet\n"                                        \
+		  "                            * peoplenet-pruned\n"                                 \
+		  "                            * dashcamnet\n"                                       \
+		  "                            * trafficcamnet\n"                                    \
+		  "                            * facedetect\n"                                       \
+		  "  --model=MODEL         path to custom model to load (caffemodel, uff, or onnx)\n" 					\
+		  "  --prototxt=PROTOTXT   path to custom prototxt to load (for .caffemodel only)\n" 					\
+		  "  --labels=LABELS       path to text file containing the labels for each class\n" 					\
+		  "  --input-blob=INPUT    name of the input layer (default is '" DETECTNET_DEFAULT_INPUT "')\n" 			\
+		  "  --output-cvg=COVERAGE name of the coverage/confidence output layer (default is '" DETECTNET_DEFAULT_COVERAGE "')\n" 	\
+		  "  --output-bbox=BOXES   name of the bounding output layer (default is '" DETECTNET_DEFAULT_BBOX "')\n" 	\
+		  "  --mean-pixel=PIXEL    mean pixel value to subtract from input (default is 0.0)\n"					\
+		  "  --confidence=CONF     minimum confidence threshold for detection (default is 0.5)\n"		           	\
+		  "  --clustering=CLUSTER  minimum overlapping area threshold for clustering (default is 0.75)\n"             \
+            "  --alpha=ALPHA         overlay alpha blending value, range 0-255 (default: 120)\n"					\
+		  "  --overlay=OVERLAY     detection overlay flags (e.g. --overlay=box,labels,conf)\n"					\
+		  "                        valid combinations are:  'box', 'lines', 'labels', 'conf', 'none'\n"			\
+		  "  --profile             enable layer profiling in TensorRT\n\n"				\
+
+
+// forward declarations
+class objectTracker;
+
+
+/**
+ * Object recognition and localization networks with TensorRT support.
+ * @ingroup detectNet
+ */
+class detectNet : public tensorNet
+{
+public:
+	/**
+	 * Object Detection result.
+	 */
+	struct Detection
+	{
+		// Detection Info
+		uint32_t ClassID;	/**< Class index of the detected object. */
+		float Confidence;	/**< Confidence value of the detected object. */
+
+		// Tracking Info
+		int TrackID;		/**< Unique tracking ID (or -1 if untracked) */
+		int TrackStatus;	/**< -1 for dropped, 0 for initializing, 1 for active/valid */ 
+		int TrackFrames;	/**< The number of frames the object has been re-identified for */
+		int TrackLost;   	/**< The number of consecutive frames tracking has been lost for */
+		
+		// Bounding Box Coordinates
+		float Left;		/**< Left bounding box coordinate (in pixels) */
+		float Right;		/**< Right bounding box coordinate (in pixels) */
+		float Top;		/**< Top bounding box cooridnate (in pixels) */
+		float Bottom;		/**< Bottom bounding box coordinate (in pixels) */
+
+		/**< Calculate the width of the object */
+		inline float Width() const											{ return Right - Left; }
+
+		/**< Calculate the height of the object */
+		inline float Height() const											{ return Bottom - Top; }
+
+		/**< Calculate the area of the object */
+		inline float Area() const											{ return Width() * Height(); }
+
+		/**< Calculate the width of the bounding box */
+		static inline float Width( float x1, float x2 )							{ return x2 - x1; }
+
+		/**< Calculate the height of the bounding box */
+		static inline float Height( float y1, float y2 )							{ return y2 - y1; }
+
+		/**< Calculate the area of the bounding box */
+		static inline float Area( float x1, float y1, float x2, float y2 )			{ return Width(x1,x2) * Height(y1,y2); }
+
+		/**< Return the center of the object */
+		inline void Center( float* x, float* y ) const							{ if(x) *x = Left + Width() * 0.5f; if(y) *y = Top + Height() * 0.5f; }
+
+		/**< Return true if the coordinate is inside the bounding box */
+		inline bool Contains( float x, float y ) const							{ return x >= Left && x <= Right && y >= Top && y <= Bottom; }
+		
+		/**< Return true if the bounding boxes intersect and exceeds area % threshold */	
+		inline bool Intersects( const Detection& det, float areaThreshold=0.0f ) const  { return (IntersectionArea(det) / fmaxf(Area(), det.Area()) > areaThreshold); }
+	
+		/**< Return true if the bounding boxes intersect and exceeds area % threshold */	
+		inline bool Intersects( float x1, float y1, float x2, float y2, float areaThreshold=0.0f ) const  { return (IntersectionArea(x1,y1,x2,y2) / fmaxf(Area(), Area(x1,y1,x2,y2)) > areaThreshold); }
+	
+		/**< Return the area of the bounding box intersection */
+		inline float IntersectionArea( const Detection& det ) const					{ return IntersectionArea(det.Left, det.Top, det.Right, det.Bottom); }
+
+		/**< Return the area of the bounding box intersection */
+		inline float IntersectionArea( float x1, float y1, float x2, float y2 ) const	{ if(!Overlaps(x1,y1,x2,y2)) return 0.0f; return (fminf(Right, x2) - fmaxf(Left, x1)) * (fminf(Bottom, y2) - fmaxf(Top, y1)); }
+
+		/**< Calculate the Intersection-Over-Union (IOU) ratio */
+		inline float IOU( const Detection& det ) const							{ return IOU(det.Left, det.Top, det.Right, det.Bottom); }
+		
+		/**< Calculate the Intersection-Over-Union (IOU) ratio */
+		inline float IOU( float x1, float y1, float x2, float y2 ) const;
+		
+		/**< Return true if the bounding boxes overlap */
+		inline bool Overlaps( const Detection& det ) const						{ return !(det.Left > Right || det.Right < Left || det.Top > Bottom || det.Bottom < Top); }
+		
+		/**< Return true if the bounding boxes overlap */
+		inline bool Overlaps( float x1, float y1, float x2, float y2 ) const			{ return !(x1 > Right || x2 < Left || y1 > Bottom || y2 < Top); }
+		
+		/**< Expand the bounding box if they overlap (return true if so) */
+		inline bool Expand( float x1, float y1, float x2, float y2 ) 	     		{ if(!Overlaps(x1, y1, x2, y2)) return false; Left = fminf(x1, Left); Top = fminf(y1, Top); Right = fmaxf(x2, Right); Bottom = fmaxf(y2, Bottom); return true; }
+		
+		/**< Expand the bounding box if they overlap (return true if so) */
+		inline bool Expand( const Detection& det )      							{ if(!Overlaps(det)) return false; Left = fminf(det.Left, Left); Top = fminf(det.Top, Top); Right = fmaxf(det.Right, Right); Bottom = fmaxf(det.Bottom, Bottom); return true; }
+
+		/**< Reset all member variables to zero */
+		inline void Reset()													{ ClassID = 0; Confidence = 0; TrackID = -1; TrackStatus = -1; TrackFrames = 0; TrackLost = 0; Left = 0; Right = 0; Top = 0; Bottom = 0; } 								
+
+		/**< Default constructor */
+		inline Detection()													{ Reset(); }
+	};
+
+	/**
+	 * Overlay flags (can be OR'd together).
+	 */
+	enum OverlayFlags
+	{
+		OVERLAY_NONE       = 0,			/**< No overlay. */
+		OVERLAY_BOX        = (1 << 0),	/**< Overlay the object bounding boxes (filled) */
+		OVERLAY_LABEL 	    = (1 << 1),	/**< Overlay the class description labels */
+		OVERLAY_CONFIDENCE = (1 << 2),	/**< Overlay the detection confidence values */
+		OVERLAY_TRACKING   = (1 << 3),	/**< Overlay tracking information (like track ID) */
+		OVERLAY_LINES      = (1 << 4),     /**< Overlay the bounding box lines (unfilled) */
+		OVERLAY_DEFAULT    = OVERLAY_BOX|OVERLAY_LABEL|OVERLAY_CONFIDENCE, /**< The default choice of overlay */
+	};
+
+	/**
+	 * Parse a string sequence into OverlayFlags enum.
+	 * Valid flags are "none", "box", "label", and "conf" and it is possible to combine flags
+	 * (bitwise OR) together with commas or pipe (|) symbol.  For example, the string sequence
+	 * "box,label,conf" would return the flags `OVERLAY_BOX|OVERLAY_LABEL|OVERLAY_CONFIDENCE`.
+	 */
+	static uint32_t OverlayFlagsFromStr( const char* flags );
+
+	/**
+	 * Load a pre-trained model
+	 * @param network the pre-trained model to load (@see DETECTNET_USAGE_STRING for models)
+	 * @param threshold default minimum threshold for detection
+	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
+	 */
+	static detectNet* Create( const char* network="ssd-mobilenet-v2", float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
+						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST, 
+						 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
+						 
+	/**
+	 * Load a custom network instance
+	 * @param prototxt_path File path to the deployable network prototxt
+	 * @param model_path File path to the caffemodel
+	 * @param mean_pixel Input transform subtraction value (use 0.0 if the network already does this)
+	 * @param class_labels File path to list of class name labels
+	 * @param threshold default minimum threshold for detection
+	 * @param input Name of the input layer blob.
+	 * @param coverage Name of the output coverage classifier layer blob, which contains the confidence values for each bbox.
+	 * @param bboxes Name of the output bounding box layer blob, which contains a grid of rectangles in the image.
+	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
+	 */
+	static detectNet* Create( const char* prototxt_path, const char* model_path, float mean_pixel=0.0f, 
+						 const char* class_labels=NULL, float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
+						 const char* input = DETECTNET_DEFAULT_INPUT, 
+						 const char* coverage = DETECTNET_DEFAULT_COVERAGE, 
+						 const char* bboxes = DETECTNET_DEFAULT_BBOX,
+						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+						 precisionType precision=TYPE_FASTEST,
+				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
+	
+	/**
+	 * Load a custom network instance
+	 * @param prototxt_path File path to the deployable network prototxt
+	 * @param model_path File path to the caffemodel
+	 * @param mean_pixel Input transform subtraction value (use 0.0 if the network already does this)
+	 * @param class_labels File path to list of class name labels
+	 * @param class_colors File path to list of class colors
+	 * @param threshold default minimum threshold for detection
+	 * @param input Name of the input layer blob.
+	 * @param coverage Name of the output coverage classifier layer blob, which contains the confidence values for each bbox.
+	 * @param bboxes Name of the output bounding box layer blob, which contains a grid of rectangles in the image.
+	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
+	 */
+	static detectNet* Create( const char* prototxt_path, const char* model_path, float mean_pixel, 
+						 const char* class_labels, const char* class_colors,
+						 float threshold=DETECTNET_DEFAULT_CONFIDENCE_THRESHOLD, 
+						 const char* input = DETECTNET_DEFAULT_INPUT, 
+						 const char* coverage = DETECTNET_DEFAULT_COVERAGE, 
+						 const char* bboxes = DETECTNET_DEFAULT_BBOX,
+						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+						 precisionType precision=TYPE_FASTEST,
+				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
+						 
+	/**
+	 * Load a custom network instance of a UFF model
+	 * @param model_path File path to the UFF model
+	 * @param class_labels File path to list of class name labels
+	 * @param threshold default minimum threshold for detection
+	 * @param input Name of the input layer blob.
+	 * @param inputDims Dimensions of the input layer blob.
+	 * @param output Name of the output layer blob containing the bounding boxes, ect.
+	 * @param numDetections Name of the output layer blob containing the detection count.
+	 * @param maxBatchSize The maximum batch size that the network will support and be optimized for.
+	 */
+	static detectNet* Create( const char* model_path, const char* class_labels, float threshold, 
+						 const char* input, const Dims3& inputDims, 
+						 const char* output, const char* numDetections,
+						 uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+						 precisionType precision=TYPE_FASTEST,
+				   		 deviceType device=DEVICE_GPU, bool allowGPUFallback=true );
+
+	/**
+	 * Load a new network instance by parsing the command line.
+	 */
+	static detectNet* Create( int argc, char** argv );
+	
+	/**
+	 * Load a new network instance by parsing the command line.
+	 */
+	static detectNet* Create( const commandLine& cmdLine );
+	
+	/**
+	 * Usage string for command line arguments to Create()
+	 */
+	static inline const char* Usage() 		{ return DETECTNET_USAGE_STRING; }
+
+	/**
+	 * Destory
+	 */
+	virtual ~detectNet();
+	
+	/**
+	 * Detect object locations from an image, returning an array containing the detection results.
+	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	template<typename T> int Detect( T* image, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT )		{ return Detect((void*)image, width, height, imageFormatFromType<T>(), detections, overlay); }
+	
+	/**
+	 * Detect object locations in an image, into an array of the results allocated by the user.
+	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
+	 *                        @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	template<typename T> int Detect( T* image, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT )		{ return Detect((void*)image, width, height, imageFormatFromType<T>(), detections, overlay); }
+	
+	/**
+	 * Detect object locations from an image, returning an array containing the detection results.
+	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	int Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT );
+
+	/**
+	 * Detect object locations from an image, into an array of the results allocated by the user.
+	 * @param[in]  input input image in CUDA device memory (uchar3/uchar4/float3/float4)
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
+	 *             @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	int Detect( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT );
+	
+	/**
+	 * Detect object locations from an RGBA image, returning an array containing the detection results.
+      * @deprecated this overload of Detect() provides legacy compatibility with `float*` type (RGBA32F). 
+	 * @param[in]  input float4 RGBA input image in CUDA device memory.
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer that will be set to array of detection results (residing in shared CPU/GPU memory)
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	int Detect( float* input, uint32_t width, uint32_t height, Detection** detections, uint32_t overlay=OVERLAY_DEFAULT );
+
+	/**
+	 * Detect object locations in an RGBA image, into an array of the results allocated by the user.
+	 * @deprecated this overload of Detect() provides legacy compatibility with `float*` type (RGBA32F). 
+	 * @param[in]  input float4 RGBA input image in CUDA device memory.
+	 * @param[in]  width width of the input image in pixels.
+	 * @param[in]  height height of the input image in pixels.
+	 * @param[out] detections pointer to user-allocated array that will be filled with the detection results.
+	 *                        @see GetMaxDetections() for the number of detection results that should be allocated in this buffer.
+	 * @param[in]  overlay bitwise OR combination of overlay flags (@see OverlayFlags and @see Overlay()), or OVERLAY_NONE.
+	 * @returns    The number of detected objects, 0 if there were no detected objects, and -1 if an error was encountered.
+	 */
+	int Detect( float* input, uint32_t width, uint32_t height, Detection* detections, uint32_t overlay=OVERLAY_DEFAULT );
+	
+	/**
+	 * Draw the detected bounding boxes overlayed on an RGBA image.
+	 * @note Overlay() will automatically be called by default by Detect(), if the overlay parameter is true 
+	 * @param input input image in CUDA device memory.
+	 * @param output output image in CUDA device memory.
+	 * @param detections Array of detections allocated in CUDA device memory.
+	 */
+	template<typename T> bool Overlay( T* input, T* output, uint32_t width, uint32_t height, Detection* detections, uint32_t numDetections, uint32_t flags=OVERLAY_DEFAULT )			{ return Overlay(input, output, width, height, imageFormatFromType<T>(), detections, numDetections, flags); }
+	
+	/**
+	 * Draw the detected bounding boxes overlayed on an RGBA image.
+	 * @note Overlay() will automatically be called by default by Detect(), if the overlay parameter is true 
+	 * @param input input image in CUDA device memory.
+	 * @param output output image in CUDA device memory.
+	 * @param detections Array of detections allocated in CUDA device memory.
+	 */
+	bool Overlay( void* input, void* output, uint32_t width, uint32_t height, imageFormat format, Detection* detections, uint32_t numDetections, uint32_t flags=OVERLAY_DEFAULT );
+	
+	/**
+	 * Retrieve the minimum threshold for detection.
+	 * @deprecated please use GetConfidenceThreshold() instead
+	 */
+	inline float GetThreshold() const							{ return mConfidenceThreshold; }
+
+	/**
+	 * Set the minimum threshold for detection.
+	 * @deprecated please use SetConfidenceThreshold() instead
+	 */
+	inline void SetThreshold( float threshold ) 					{ mConfidenceThreshold = threshold; }
+
+	/**
+	 * Retrieve the minimum threshold for detection.
+	 */
+	inline float GetConfidenceThreshold() const					{ return mConfidenceThreshold; }
+
+	/**
+	 * Set the minimum threshold for detection.
+	 */
+	inline void SetConfidenceThreshold( float threshold ) 			{ mConfidenceThreshold = threshold; }
+
+	/**
+	 * Retrieve the overlapping area % threshold for clustering.
+	 */
+	inline float GetClusteringThreshold() const					{ return mClusteringThreshold; }
+
+	/**
+	 * Set the overlapping area % threshold for clustering.
+	 */
+	inline void SetClusteringThreshold( float threshold ) 			{ mClusteringThreshold = threshold; }
+
+	/**
+	 * Get the object tracker being used.
+	 */
+	inline objectTracker* GetTracker() const					{ return mTracker; }
+	
+	/**
+	 * Set the object tracker to be used.
+	 */
+	inline void SetTracker( objectTracker* tracker ) 				{ mTracker = tracker; }
+	
+	/**
+	 * Retrieve the maximum number of simultaneous detections the network supports.
+	 * Knowing this is useful for allocating the buffers to store the output detection results.
+	 */
+	inline uint32_t GetMaxDetections() const					{ return mMaxDetections; } 
+		
+	/**
+	 * Retrieve the number of object classes supported in the detector
+	 */
+	inline uint32_t GetNumClasses() const						{ return mNumClasses; }
+
+	/**
+	 * Retrieve the description of a particular class.
+	 */
+	inline const char* GetClassLabel( uint32_t index ) const		{ return GetClassDesc(index); }
+	
+	/**
+	 * Retrieve the description of a particular class.
+	 */
+	inline const char* GetClassDesc( uint32_t index )	const		{ if(index >= mClassDesc.size()) { printf("invalid class %u\n", index); return "Invalid"; } return mClassDesc[index].c_str(); }
+	
+	/**
+	 * Retrieve the class synset category of a particular class.
+	 */
+	inline const char* GetClassSynset( uint32_t index ) const		{ return mClassSynset[index].c_str(); }
+	
+	/**
+ 	 * Retrieve the path to the file containing the class descriptions.
+	 */
+	inline const char* GetClassPath() const						{ return mClassPath.c_str(); }
+
+	/**
+	 * Retrieve the RGBA visualization color a particular class.
+	 */
+	inline float4 GetClassColor( uint32_t classIndex ) const		{ return mClassColors[classIndex]; }
+
+	/**
+	 * Set the visualization color of a particular class of object.
+	 */
+	inline void SetClassColor( uint32_t classIndex, const float4& color )						{ mClassColors[classIndex] = color; }
+	
+	/**
+	 * Set the visualization color of a particular class of object.
+	 */
+	inline void SetClassColor( uint32_t classIndex, float r, float g, float b, float a=255.0f )	{ mClassColors[classIndex] = make_float4(r,g,b,a); }
+	
+	/**
+	 * Retrieve the line width used during overlay when OVERLAY_LINES is used.
+	 */
+	inline float GetLineWidth() const							{ return mLineWidth; }
+	
+	/**
+	 * Set the line width used during overlay when OVERLAY_LINES is used.
+	 */
+	inline void SetLineWidth( float width )						{ mLineWidth = width; }
+	
+	/**
+	 * Retrieve the overlay alpha blending value for classes that don't have it explicitly set (between 0-255).
+	 */
+	inline float GetOverlayAlpha() const						{ return mOverlayAlpha; }
+	
+	/**
+ 	 * Set overlay alpha blending value for all classes (between 0-255).
+	 */
+	void SetOverlayAlpha( float alpha );
+
+protected:
+
+	// constructor
+	detectNet( float meanPixel=0.0f );
+
+	bool allocDetections();
+
+	bool loadClassInfo( const char* filename );
+	bool loadClassColors( const char* filename );
+	
+	bool init( const char* prototxt_path, const char* model_path, const char* class_labels, const char* class_colors,
+			 float threshold, const char* input, const char* coverage, const char* bboxes, uint32_t maxBatchSize, 
+			 precisionType precision, deviceType device, bool allowGPUFallback );
+	
+	bool preProcess( void* input, uint32_t width, uint32_t height, imageFormat format );
+	int postProcess( void* input, uint32_t width, uint32_t height, imageFormat format, Detection* detections );
+	
+	int postProcessSSD_UFF( Detection* detections, uint32_t width, uint32_t height );
+	int postProcessSSD_ONNX( Detection* detections, uint32_t width, uint32_t height );
+	//sebi:
+	int postProcessYOLO_ONNX(Detection* detections, uint32_t width, uint32_t height);
+	int postProcessDetectNet( Detection* detections, uint32_t width, uint32_t height );
+	int postProcessDetectNet_v2( Detection* detections, uint32_t width, uint32_t height );
+	
+	int clusterDetections( Detection* detections, int n );
+	void sortDetections( Detection* detections, int numDetections );
+
+	objectTracker* mTracker;
+	
+	float mConfidenceThreshold;	 // TODO change this to per-class
+	float mClusteringThreshold;	 // TODO change this to per-class
+	
+	float mMeanPixel;
+	float mLineWidth;
+	float mOverlayAlpha;
+	
+	float4* mClassColors;
+	
+	std::vector<std::string> mClassDesc;
+	std::vector<std::string> mClassSynset;
+
+	std::string mClassPath;
+	uint32_t	  mNumClasses;
+
+	Detection* mDetectionSets;	// list of detections, mNumDetectionSets * mMaxDetections
+	uint32_t   mDetectionSet;	// index of next detection set to use
+	uint32_t	 mMaxDetections;	// number of raw detections in the grid
+
+	static const uint32_t mNumDetectionSets = 16; // size of detection ringbuffer
+};
+
+
+// bounding box IOU
+inline float detectNet::Detection::IOU( float x1, float y1, float x2, float y2 ) const		
+{
+	const float overlap_x0 = fmaxf(Left, x1);
+	const float overlap_y0 = fmaxf(Top, y1);
+	const float overlap_x1 = fminf(Right, x2);
+	const float overlap_y1 = fminf(Bottom, y2);
+	
+	// check if there is an overlap
+	if( (overlap_x1 - overlap_x0 <= 0) || (overlap_y1 - overlap_y0 <= 0) )
+		return 0;
+	
+	// calculate the ratio of the overlap to each ROI size and the unified size
+	const float size_1 = Area();
+	const float size_2 = Area(x1, y1, x2, y2);
+	
+	const float size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0);
+	const float size_union = size_1 + size_2 - size_intersection;
+	
+	return size_intersection / size_union;
+}
+
+
+#endif
diff --git a/c/tensorConvert.cu b/c/tensorConvert.cu
index e8d6772c..50282355 100644
--- a/c/tensorConvert.cu
+++ b/c/tensorConvert.cu
@@ -1,258 +1,331 @@
-/*
- * Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
- * DEALINGS IN THE SOFTWARE.
- */
- 
-#include "tensorConvert.h"
-
-
-// gpuTensorMean
-template<typename T, bool isBGR>
-__global__ void gpuTensorMean( float2 scale, T* input, int iWidth, float* output, int oWidth, int oHeight, float3 mean_value )
-{
-	const int x = blockIdx.x * blockDim.x + threadIdx.x;
-	const int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-	if( x >= oWidth || y >= oHeight )
-		return;
-
-	const int n = oWidth * oHeight;
-	const int m = y * oWidth + x;
-
-	const int dx = ((float)x * scale.x);
-	const int dy = ((float)y * scale.y);
-
-	const T px = input[ dy * iWidth + dx ];
-
-	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
-						: make_float3(px.x, px.y, px.z);
-	
-	output[n * 0 + m] = rgb.x - mean_value.x;
-	output[n * 1 + m] = rgb.y - mean_value.y;
-	output[n * 2 + m] = rgb.z - mean_value.z;
-}
-
-template<bool isBGR>
-cudaError_t launchTensorMean( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						float* output, size_t outputWidth, size_t outputHeight, 
-						const float3& mean_value, cudaStream_t stream )
-{
-	if( !input || !output )
-		return cudaErrorInvalidDevicePointer;
-
-	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
-		return cudaErrorInvalidValue;
-
-	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
-							    float(inputHeight) / float(outputHeight) );
-
-	// launch kernel
-	const dim3 blockDim(8, 8);
-	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
-
-	if( format == IMAGE_RGB8 )
-		gpuTensorMean<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar3*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
-	else if( format == IMAGE_RGBA8 )
-		gpuTensorMean<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar4*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
-	else if( format == IMAGE_RGB32F )
-		gpuTensorMean<float3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float3*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
-	else if( format == IMAGE_RGBA32F )
-		gpuTensorMean<float4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float4*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
-	else
-		return cudaErrorInvalidValue;
-
-	return CUDA(cudaGetLastError());
-}
-
-// cudaTensorMeanRGB
-cudaError_t cudaTensorMeanRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-				           float* output, size_t outputWidth, size_t outputHeight, 
-						 const float3& mean_value, cudaStream_t stream )
-{
-	return launchTensorMean<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, mean_value, stream);
-}
-
-// cudaTensorMeanBGR
-cudaError_t cudaTensorMeanBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-				           float* output, size_t outputWidth, size_t outputHeight, 
-						 const float3& mean_value, cudaStream_t stream )
-{
-	return launchTensorMean<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, mean_value, stream);
-}
-
-
-// gpuTensorNorm
-template<typename T, bool isBGR>
-__global__ void gpuTensorNorm( float2 scale, T* input, int iWidth, float* output, int oWidth, int oHeight, float multiplier, float min_value )
-{
-	const int x = blockIdx.x * blockDim.x + threadIdx.x;
-	const int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-	if( x >= oWidth || y >= oHeight )
-		return;
-
-	const int n = oWidth * oHeight;
-	const int m = y * oWidth + x;
-
-	const int dx = ((float)x * scale.x);
-	const int dy = ((float)y * scale.y);
-
-	const T px = input[ dy * iWidth + dx ];
-
-	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
-						: make_float3(px.x, px.y, px.z);
-	
-	output[n * 0 + m] = rgb.x * multiplier + min_value;
-	output[n * 1 + m] = rgb.y * multiplier + min_value;
-	output[n * 2 + m] = rgb.z * multiplier + min_value;
-}
-
-template<bool isBGR>
-cudaError_t launchTensorNorm( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						float* output, size_t outputWidth, size_t outputHeight, 
-						const float2& range, cudaStream_t stream )
-{
-	if( !input || !output )
-		return cudaErrorInvalidDevicePointer;
-
-	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
-		return cudaErrorInvalidValue;
-
-	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
-							    float(inputHeight) / float(outputHeight) );
-
-	const float multiplier = (range.y - range.x) / 255.0f;
-	
-	// launch kernel
-	const dim3 blockDim(8, 8);
-	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
-
-	if( format == IMAGE_RGB8 )
-		gpuTensorNorm<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar3*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
-	else if( format == IMAGE_RGBA8 )
-		gpuTensorNorm<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar4*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
-	else if( format == IMAGE_RGB32F )
-		gpuTensorNorm<float3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float3*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
-	else if( format == IMAGE_RGBA32F )
-		gpuTensorNorm<float4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float4*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
-	else
-		return cudaErrorInvalidValue;
-
-	return CUDA(cudaGetLastError());
-}
-
-// cudaTensorNormRGB
-cudaError_t cudaTensorNormRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						 float* output, size_t outputWidth, size_t outputHeight,
-						 const float2& range, cudaStream_t stream )
-{
-	return launchTensorNorm<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, stream);
-}
-
-// cudaTensorNormBGR
-cudaError_t cudaTensorNormBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						 float* output, size_t outputWidth, size_t outputHeight,
-						 const float2& range, cudaStream_t stream )
-{
-	return launchTensorNorm<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, stream);
-}
-
-
-// gpuTensorNormMean
-template<typename T, bool isBGR>
-__global__ void gpuTensorNormMean( T* input, int iWidth, float* output, int oWidth, int oHeight, int stride, float2 scale, float multiplier, float min_value, const float3 mean, const float3 stdDev )
-{
-	const int x = blockIdx.x * blockDim.x + threadIdx.x;
-	const int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-	if( x >= oWidth || y >= oHeight )
-		return;
-
-	const int m  = y * oWidth + x;
-	const int dx = ((float)x * scale.x);
-	const int dy = ((float)y * scale.y);
-
-	const T px = input[ dy * iWidth + dx ];
-
-	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
-						: make_float3(px.x, px.y, px.z);
-	
-	output[stride * 0 + m] = ((rgb.x * multiplier + min_value) - mean.x) / stdDev.x;
-	output[stride * 1 + m] = ((rgb.y * multiplier + min_value) - mean.y) / stdDev.y;
-	output[stride * 2 + m] = ((rgb.z * multiplier + min_value) - mean.z) / stdDev.z;
-}
-
-template<bool isBGR>
-cudaError_t launchTensorNormMean( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						    float* output, size_t outputWidth, size_t outputHeight, 
-						    const float2& range, const float3& mean, const float3& stdDev,
-						    cudaStream_t stream, size_t channelStride )
-{
-	if( !input || !output )
-		return cudaErrorInvalidDevicePointer;
-
-	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
-		return cudaErrorInvalidValue;
-
-	if( channelStride == 0 )
-		channelStride = outputWidth * outputHeight;
-
-	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
-							    float(inputHeight) / float(outputHeight) );
-
-	const float multiplier = (range.y - range.x) / 255.0f;
-	
-	// launch kernel
-	const dim3 blockDim(8, 8);
-	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
-
-	if( format == IMAGE_RGB8 )
-		gpuTensorNormMean<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar3*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
-	else if( format == IMAGE_RGBA8 )
-		gpuTensorNormMean<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar4*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
-	else if( format == IMAGE_RGB32F )
-		gpuTensorNormMean<float3, isBGR><<<gridDim, blockDim, 0, stream>>>((float3*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
-	else if( format == IMAGE_RGBA32F )
-		gpuTensorNormMean<float4, isBGR><<<gridDim, blockDim, 0, stream>>>((float4*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
-	else
-		return cudaErrorInvalidValue;
-
-	return CUDA(cudaGetLastError());
-}
-
-// cudaTensorNormMeanRGB
-cudaError_t cudaTensorNormMeanRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						     float* output, size_t outputWidth, size_t outputHeight, 
-						     const float2& range, const float3& mean, const float3& stdDev,
-						     cudaStream_t stream, size_t channelStride )
-{
-	return launchTensorNormMean<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, mean, stdDev, stream, channelStride );
-}
-
-// cudaTensorNormMeanRGB
-cudaError_t cudaTensorNormMeanBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
-						     float* output, size_t outputWidth, size_t outputHeight, 
-						     const float2& range, const float3& mean, const float3& stdDev,
-						     cudaStream_t stream, size_t channelStride )
-{
-	return launchTensorNormMean<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, mean, stdDev, stream, channelStride);
-}
-
-
+/*
+ * Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+ 
+#include "tensorConvert.h"
+
+
+// gpuTensorMean
+template<typename T, bool isBGR>
+__global__ void gpuTensorMean( float2 scale, T* input, int iWidth, float* output, int oWidth, int oHeight, float3 mean_value )
+{
+	const int x = blockIdx.x * blockDim.x + threadIdx.x;
+	const int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+	if( x >= oWidth || y >= oHeight )
+		return;
+
+	const int n = oWidth * oHeight;
+	const int m = y * oWidth + x;
+
+	const int dx = ((float)x * scale.x);
+	const int dy = ((float)y * scale.y);
+
+	const T px = input[ dy * iWidth + dx ];
+
+	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
+						: make_float3(px.x, px.y, px.z);
+	
+	output[n * 0 + m] = rgb.x - mean_value.x;
+	output[n * 1 + m] = rgb.y - mean_value.y;
+	output[n * 2 + m] = rgb.z - mean_value.z;
+}
+
+template<bool isBGR>
+cudaError_t launchTensorMean( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						float* output, size_t outputWidth, size_t outputHeight, 
+						const float3& mean_value, cudaStream_t stream )
+{
+	if( !input || !output )
+		return cudaErrorInvalidDevicePointer;
+
+	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
+		return cudaErrorInvalidValue;
+
+	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
+							    float(inputHeight) / float(outputHeight) );
+
+	// launch kernel
+	const dim3 blockDim(8, 8);
+	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
+
+	if( format == IMAGE_RGB8 )
+		gpuTensorMean<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar3*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
+	else if( format == IMAGE_RGBA8 )
+		gpuTensorMean<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar4*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
+	else if( format == IMAGE_RGB32F )
+		gpuTensorMean<float3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float3*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
+	else if( format == IMAGE_RGBA32F )
+		gpuTensorMean<float4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float4*)input, inputWidth, output, outputWidth, outputHeight, mean_value);
+	else
+		return cudaErrorInvalidValue;
+
+	return CUDA(cudaGetLastError());
+}
+
+// cudaTensorMeanRGB
+cudaError_t cudaTensorMeanRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+				           float* output, size_t outputWidth, size_t outputHeight, 
+						 const float3& mean_value, cudaStream_t stream )
+{
+	return launchTensorMean<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, mean_value, stream);
+}
+
+// cudaTensorMeanBGR
+cudaError_t cudaTensorMeanBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+				           float* output, size_t outputWidth, size_t outputHeight, 
+						 const float3& mean_value, cudaStream_t stream )
+{
+	return launchTensorMean<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, mean_value, stream);
+}
+
+
+// gpuTensorNorm
+template<typename T, bool isBGR>
+__global__ void gpuTensorNorm( float2 scale, T* input, int iWidth, float* output, int oWidth, int oHeight, float multiplier, float min_value )
+{
+	const int x = blockIdx.x * blockDim.x + threadIdx.x;
+	const int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+	if( x >= oWidth || y >= oHeight )
+		return;
+
+	const int n = oWidth * oHeight;
+	const int m = y * oWidth + x;
+
+	const int dx = ((float)x * scale.x);
+	const int dy = ((float)y * scale.y);
+
+	const T px = input[ dy * iWidth + dx ];
+
+	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
+						: make_float3(px.x, px.y, px.z);
+	
+	output[n * 0 + m] = rgb.x * multiplier + min_value;
+	output[n * 1 + m] = rgb.y * multiplier + min_value;
+	output[n * 2 + m] = rgb.z * multiplier + min_value;
+}
+
+template<bool isBGR>
+cudaError_t launchTensorNorm( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						float* output, size_t outputWidth, size_t outputHeight, 
+						const float2& range, cudaStream_t stream )
+{
+	if( !input || !output )
+		return cudaErrorInvalidDevicePointer;
+
+	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
+		return cudaErrorInvalidValue;
+
+	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
+							    float(inputHeight) / float(outputHeight) );
+
+	const float multiplier = (range.y - range.x) / 255.0f;
+	
+	// launch kernel
+	const dim3 blockDim(8, 8);
+	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
+
+	if( format == IMAGE_RGB8 )
+		gpuTensorNorm<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar3*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
+	else if( format == IMAGE_RGBA8 )
+		gpuTensorNorm<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (uchar4*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
+	else if( format == IMAGE_RGB32F )
+		gpuTensorNorm<float3, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float3*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
+	else if( format == IMAGE_RGBA32F )
+		gpuTensorNorm<float4, isBGR><<<gridDim, blockDim, 0, stream>>>(scale, (float4*)input, inputWidth, output, outputWidth, outputHeight, multiplier, range.x);
+	else
+		return cudaErrorInvalidValue;
+
+	return CUDA(cudaGetLastError());
+}
+
+// cudaTensorNormRGB
+cudaError_t cudaTensorNormRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						 float* output, size_t outputWidth, size_t outputHeight,
+						 const float2& range, cudaStream_t stream )
+{
+	return launchTensorNorm<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, stream);
+}
+
+// cudaTensorNormBGR
+cudaError_t cudaTensorNormBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						 float* output, size_t outputWidth, size_t outputHeight,
+						 const float2& range, cudaStream_t stream )
+{
+	return launchTensorNorm<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, stream);
+}
+
+
+// gpuTensorNormMean
+template<typename T, bool isBGR>
+__global__ void gpuTensorNormMean( T* input, int iWidth, float* output, int oWidth, int oHeight, int stride, float2 scale, float multiplier, float min_value, const float3 mean, const float3 stdDev )
+{
+	const int x = blockIdx.x * blockDim.x + threadIdx.x;
+	const int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+	if( x >= oWidth || y >= oHeight )
+		return;
+
+	const int m  = y * oWidth + x;
+	const int dx = ((float)x * scale.x);
+	const int dy = ((float)y * scale.y);
+
+	const T px = input[ dy * iWidth + dx ];
+
+	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
+						: make_float3(px.x, px.y, px.z);
+	
+	output[stride * 0 + m] = ((rgb.x * multiplier + min_value) - mean.x) / stdDev.x;
+	output[stride * 1 + m] = ((rgb.y * multiplier + min_value) - mean.y) / stdDev.y;
+	output[stride * 2 + m] = ((rgb.z * multiplier + min_value) - mean.z) / stdDev.z;
+}
+
+template<typename T, bool isBGR>
+__global__ void gpuTensorNormMeanKeepAspect( T* input, int iWidth, int iHeight, float* output, int oWidth, int oHeight, int stride, float ratio, float multiplier, float min_value, const float3 mean, const float3 stdDev )
+{
+	const int x = blockIdx.x * blockDim.x + threadIdx.x;
+	const int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+	if( x >= oWidth || y >= oHeight )
+		return;
+
+	const int m  = y * oWidth + x;
+	// will keep last pixel value for the right or bottom depending on max dim
+	// maybe better to just put black value
+	const int dx = min((float)x * ratio, (float)(iWidth-1));
+	const int dy = min((float)y * ratio, (float)(iHeight-1));
+
+	const T px = input[ dy * iWidth + dx ];
+
+	const float3 rgb = isBGR ? make_float3(px.z, px.y, px.x)
+						: make_float3(px.x, px.y, px.z);
+
+	output[stride * 0 + m] = ((rgb.x * multiplier + min_value) - mean.x) / stdDev.x;
+	output[stride * 1 + m] = ((rgb.y * multiplier + min_value) - mean.y) / stdDev.y;
+	output[stride * 2 + m] = ((rgb.z * multiplier + min_value) - mean.z) / stdDev.z;
+}
+
+
+template<bool isBGR>
+cudaError_t launchTensorNormMean( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						    float* output, size_t outputWidth, size_t outputHeight, 
+						    const float2& range, const float3& mean, const float3& stdDev,
+						    cudaStream_t stream, size_t channelStride )
+{
+	if( !input || !output )
+		return cudaErrorInvalidDevicePointer;
+
+	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
+		return cudaErrorInvalidValue;
+
+	if( channelStride == 0 )
+		channelStride = outputWidth * outputHeight;
+
+	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
+							    float(inputHeight) / float(outputHeight) );
+
+	const float multiplier = (range.y - range.x) / 255.0f;
+	
+	// launch kernel
+	const dim3 blockDim(8, 8);
+	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
+
+	if( format == IMAGE_RGB8 )
+		gpuTensorNormMean<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar3*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGBA8 )
+		gpuTensorNormMean<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar4*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGB32F )
+		gpuTensorNormMean<float3, isBGR><<<gridDim, blockDim, 0, stream>>>((float3*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGBA32F )
+		gpuTensorNormMean<float4, isBGR><<<gridDim, blockDim, 0, stream>>>((float4*)input, inputWidth, output, outputWidth, outputHeight, channelStride, scale, multiplier, range.x, mean, stdDev);
+	else
+		return cudaErrorInvalidValue;
+
+	return CUDA(cudaGetLastError());
+}
+
+//sebi:
+template<bool isBGR>
+cudaError_t launchTensorNormMeanKeepAspect( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						    float* output, size_t outputWidth, size_t outputHeight,
+						    const float2& range, const float3& mean, const float3& stdDev,
+						    cudaStream_t stream, size_t channelStride )
+{
+	if( !input || !output )
+		return cudaErrorInvalidDevicePointer;
+
+	if( inputWidth == 0 || outputWidth == 0 || inputHeight == 0 || outputHeight == 0 )
+		return cudaErrorInvalidValue;
+
+	if( channelStride == 0 )
+		channelStride = outputWidth * outputHeight;
+
+	const float2 scale = make_float2( float(inputWidth) / float(outputWidth),
+							    float(inputHeight) / float(outputHeight) );
+	const float ratio = 1.0f/min(float(outputWidth) / float(inputWidth), float(outputHeight) / float(inputHeight));
+
+	const float multiplier = (range.y - range.x) / 255.0f;
+
+	// launch kernel
+	const dim3 blockDim(8, 8);
+	const dim3 gridDim(iDivUp(outputWidth,blockDim.x), iDivUp(outputHeight,blockDim.y));
+
+	if( format == IMAGE_RGB8 )
+		gpuTensorNormMeanKeepAspect<uchar3, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar3*)input, inputWidth, inputHeight, output, outputWidth, outputHeight, channelStride, ratio, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGBA8 )
+		gpuTensorNormMeanKeepAspect<uchar4, isBGR><<<gridDim, blockDim, 0, stream>>>((uchar4*)input, inputWidth, inputHeight, output, outputWidth, outputHeight, channelStride, ratio, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGB32F )
+		gpuTensorNormMeanKeepAspect<float3, isBGR><<<gridDim, blockDim, 0, stream>>>((float3*)input, inputWidth, inputHeight, output, outputWidth, outputHeight, channelStride, ratio, multiplier, range.x, mean, stdDev);
+	else if( format == IMAGE_RGBA32F )
+		gpuTensorNormMeanKeepAspect<float4, isBGR><<<gridDim, blockDim, 0, stream>>>((float4*)input, inputWidth, inputHeight, output, outputWidth, outputHeight, channelStride, ratio, multiplier, range.x, mean, stdDev);
+	else
+		return cudaErrorInvalidValue;
+
+	return CUDA(cudaGetLastError());
+}
+
+// cudaTensorNormMeanRGB
+cudaError_t cudaTensorNormMeanRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						     float* output, size_t outputWidth, size_t outputHeight, 
+						     const float2& range, const float3& mean, const float3& stdDev,
+						     cudaStream_t stream, size_t channelStride )
+{
+	return launchTensorNormMean<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, mean, stdDev, stream, channelStride );
+}
+
+// cudaTensorNormMeanRGB
+cudaError_t cudaTensorNormMeanBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						     float* output, size_t outputWidth, size_t outputHeight, 
+						     const float2& range, const float3& mean, const float3& stdDev,
+						     cudaStream_t stream, size_t channelStride )
+{
+	return launchTensorNormMean<true>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, mean, stdDev, stream, channelStride);
+}
+
+// cudaTensorNormMeanRGB
+cudaError_t cudaTensorNormMeanKeepAspectRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight,
+						     float* output, size_t outputWidth, size_t outputHeight,
+						     const float2& range, const float3& mean, const float3& stdDev,
+						     cudaStream_t stream, size_t channelStride )
+{
+	return launchTensorNormMeanKeepAspect<false>(input, format, inputWidth, inputHeight, output, outputWidth, outputHeight, range, mean, stdDev, stream, channelStride );
+}
diff --git a/c/tensorConvert.h b/c/tensorConvert.h
index f732f7c1..67827d10 100644
--- a/c/tensorConvert.h
+++ b/c/tensorConvert.h
@@ -46,6 +46,8 @@ cudaError_t cudaTensorNormBGR( void* input, imageFormat format, size_t inputWidt
 cudaError_t cudaTensorNormMeanRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight, float* output, size_t outputWidth, size_t outputHeight, const float2& range, const float3& mean, const float3& stdDev, cudaStream_t stream, size_t channelStride=0 );
 cudaError_t cudaTensorNormMeanBGR( void* input, imageFormat format, size_t inputWidth, size_t inputHeight, float* output, size_t outputWidth, size_t outputHeight, const float2& range, const float3& mean, const float3& stdDev, cudaStream_t stream, size_t channelStride=0 );
 
+cudaError_t cudaTensorNormMeanKeepAspectRGB( void* input, imageFormat format, size_t inputWidth, size_t inputHeight, float* output, size_t outputWidth, size_t outputHeight, const float2& range, const float3& mean, const float3& stdDev, cudaStream_t stream, size_t channelStride=0 );
+
 
 #endif
 
diff --git a/c/tensorNet.cpp b/c/tensorNet.cpp
index 05515e5d..c6a7dd71 100644
--- a/c/tensorNet.cpp
+++ b/c/tensorNet.cpp
@@ -1,2038 +1,2052 @@
-/*
- * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
- * DEALINGS IN THE SOFTWARE.
- */
- 
-#include "tensorNet.h"
-#include "randInt8Calibrator.h"
-#include "cudaMappedMemory.h"
-#include "cudaResize.h"
-#include "filesystem.h"
-
-#include "NvCaffeParser.h"
-
-#if NV_TENSORRT_MAJOR >= 5
-#include "NvOnnxParser.h"
-#include "NvUffParser.h"
-#include "NvInferPlugin.h"
-#endif
-
-#include <iostream>
-#include <fstream>
-#include <map>
-
-
-#if NV_TENSORRT_MAJOR > 1
-	#define CREATE_INFER_BUILDER nvinfer1::createInferBuilder
-	#define CREATE_INFER_RUNTIME nvinfer1::createInferRuntime
-#else
-	#define CREATE_INFER_BUILDER createInferBuilder
-	#define CREATE_INFER_RUNTIME createInferRuntime
-#endif
-
-#define LOG_DOWNLOADER_TOOL "        if loading a built-in model, maybe it wasn't downloaded before.\n\n"    \
-					   "        Run the Model Downloader tool again and select it for download:\n\n"   \
-					   "           $ cd <jetson-inference>/tools\n" 	  	\
-					   "           $ ./download-models.sh\n"
-
-#define USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
-#define CHECKSUM_TYPE "sha256sum"
-
-//---------------------------------------------------------------------
-const char* precisionTypeToStr( precisionType type )
-{
-	switch(type)
-	{
-		case TYPE_DISABLED:	return "DISABLED";
-		case TYPE_FASTEST:	return "FASTEST";
-		case TYPE_FP32:	return "FP32";
-		case TYPE_FP16:	return "FP16";
-		case TYPE_INT8:	return "INT8";
-	}
-    return nullptr;
-}
-
-precisionType precisionTypeFromStr( const char* str )
-{
-	if( !str )
-		return TYPE_DISABLED;
-
-	for( int n=0; n < NUM_PRECISIONS; n++ )
-	{
-		if( strcasecmp(str, precisionTypeToStr((precisionType)n)) == 0 )
-			return (precisionType)n;
-	}
-
-	return TYPE_DISABLED;
-}
-
-static inline nvinfer1::DataType precisionTypeToTRT( precisionType type )
-{
-	switch(type)
-	{
-		case TYPE_FP16:	return nvinfer1::DataType::kHALF;
-#if NV_TENSORRT_MAJOR >= 4
-		case TYPE_INT8:	return nvinfer1::DataType::kINT8;
-#endif
-	}
-
-	return nvinfer1::DataType::kFLOAT;
-}
-
-#if NV_TENSORRT_MAJOR >= 8
-static inline bool isFp16Enabled( nvinfer1::IBuilderConfig* config )
-{
-	return config->getFlag(nvinfer1::BuilderFlag::kFP16);
-}
-
-static inline bool isInt8Enabled( nvinfer1::IBuilderConfig* config )
-{
-	return config->getFlag(nvinfer1::BuilderFlag::kINT8);
-}
-#else // NV_TENSORRT_MAJOR <= 7
-static inline bool isFp16Enabled( nvinfer1::IBuilder* builder )
-{
-#if NV_TENSORRT_MAJOR < 4
-	return builder->getHalf2Mode();
-#else
-	return builder->getFp16Mode();
-#endif
-}
-
-static inline bool isInt8Enabled( nvinfer1::IBuilder* builder )
-{
-#if NV_TENSORRT_MAJOR >= 4
-	return builder->getInt8Mode();
-#else
-	return false;
-#endif
-}
-#endif
-
-#if NV_TENSORRT_MAJOR >= 4
-static inline const char* dataTypeToStr( nvinfer1::DataType type )
-{
-	switch(type)
-	{
-		case nvinfer1::DataType::kFLOAT:	return "FP32";
-		case nvinfer1::DataType::kHALF:	return "FP16";
-		case nvinfer1::DataType::kINT8:	return "INT8";
-		case nvinfer1::DataType::kINT32:	return "INT32";
-	}
-
-	LogWarning(LOG_TRT "warning -- unknown nvinfer1::DataType (%i)\n", (int)type);
-	return "UNKNOWN";
-}
-
-#if NV_TENSORRT_MAJOR <= 7
-static inline const char* dimensionTypeToStr( nvinfer1::DimensionType type )
-{
-	switch(type)
-	{
-		case nvinfer1::DimensionType::kSPATIAL:	 return "SPATIAL";
-		case nvinfer1::DimensionType::kCHANNEL:	 return "CHANNEL";
-		case nvinfer1::DimensionType::kINDEX:	 return "INDEX";
-		case nvinfer1::DimensionType::kSEQUENCE: return "SEQUENCE";
-	}
-
-	LogWarning(LOG_TRT "warning -- unknown nvinfer1::DimensionType (%i)\n", (int)type);
-	return "UNKNOWN";
-}
-#endif
-#endif
-
-#if NV_TENSORRT_MAJOR > 1 
-static inline nvinfer1::Dims validateDims( const nvinfer1::Dims& dims )
-{
-	if( dims.nbDims == nvinfer1::Dims::MAX_DIMS )
-		return dims;
-	
-	nvinfer1::Dims dims_out = dims;
-
-	// TRT doesn't set the higher dims, so make sure they are 1
-	for( int n=dims_out.nbDims; n < nvinfer1::Dims::MAX_DIMS; n++ )
-		dims_out.d[n] = 1;
-
-	return dims_out;
-}
-
-static inline void copyDims( Dims3* dest, const nvinfer1::Dims* src )
-{
-	for( int n=0; n < src->nbDims; n++ )
-		dest->d[n] = src->d[n];
-	
-	dest->nbDims = src->nbDims;
-}
-
-static inline size_t sizeDims( const nvinfer1::Dims& dims, const size_t elementSize=1 )
-{
-	size_t sz = dims.d[0];
-	
-	for ( int n=1; n < dims.nbDims; n++ )
-		sz *= dims.d[n];
-
-	return sz * elementSize;
-}
-#else
-static inline nvinfer1::Dims3 validateDims( const nvinfer1::Dims3& dims )
-{
-	nvinfer1::Dims3 out = dims;
-	
-	if( DIMS_C(out) == 0 )
-		DIMS_C(out) = 1;
-	
-	if( DIMS_H(out) == 0 )
-		DIMS_H(out) = 1;
-	
-	if( DIMS_W(out) == 0 )
-		DIMS_W(out) = 1;
-	
-	return out;
-}
-
-static inline void copyDims( Dims3* dest, const nvinfer1::Dims3* src )
-{
-	memcpy(dest, src, sizeof(nvinfer1::Dims));
-}
-
-static inline size_t sizeDims( const Dims3& dims, const size_t elementSize=1 )
-{
-	return DIMS_C(dims) * DIMS_H(dims) * DIMS_W(dims) * elementSize;
-}
-#endif	
-
-#if NV_TENSORRT_MAJOR >= 7
-static inline nvinfer1::Dims shiftDims( const nvinfer1::Dims& dims )
-{
-	// TensorRT 7.0 requires EXPLICIT_BATCH flag for ONNX models,
-	// which adds a batch dimension (4D NCHW), whereas historically
-	// 3D CHW was expected.  Remove the batch dim (it is typically 1)
-	nvinfer1::Dims out = dims;
-	
-	/*out.d[0] = dims.d[1];
-	out.d[1] = dims.d[2];
-	out.d[2] = dims.d[3];
-	out.d[3] = 1;*/
-	
-	if( dims.nbDims == 1 )
-		return out;
-	
-	for( int n=0; n < dims.nbDims; n++ )
-		out.d[n] = dims.d[n+1];
-	
-	for( int n=dims.nbDims; n < nvinfer1::Dims::MAX_DIMS; n++ )
-		out.d[n] = 1;
-	
-	out.nbDims -= 1;
-	return out;
-}
-#endif
-
-const char* deviceTypeToStr( deviceType type )
-{
-	switch(type)
-	{
-		case DEVICE_GPU:	return "GPU";	
-		case DEVICE_DLA_0:	return "DLA_0";
-		case DEVICE_DLA_1:	return "DLA_1";
-	}
-    return nullptr;
-}
-
-deviceType deviceTypeFromStr( const char* str )
-{
-	if( !str )
-		return DEVICE_GPU;
-
- 	for( int n=0; n < NUM_DEVICES; n++ )
-	{
-		if( strcasecmp(str, deviceTypeToStr((deviceType)n)) == 0 )
-			return (deviceType)n;
-	}
-
-	if( strcasecmp(str, "DLA") == 0 )
-		return DEVICE_DLA;
-
-	return DEVICE_GPU;
-}
-
-#if NV_TENSORRT_MAJOR >= 5
-static inline nvinfer1::DeviceType deviceTypeToTRT( deviceType type )
-{
-	switch(type)
-	{
-		case DEVICE_GPU:	return nvinfer1::DeviceType::kGPU;
-		//case DEVICE_DLA:	return nvinfer1::DeviceType::kDLA;
-#if NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0
-		case DEVICE_DLA_0:	return nvinfer1::DeviceType::kDLA0;
-		case DEVICE_DLA_1:	return nvinfer1::DeviceType::kDLA1;
-#else
-		case DEVICE_DLA_0:	return nvinfer1::DeviceType::kDLA;
-		case DEVICE_DLA_1:	return nvinfer1::DeviceType::kDLA;
-#endif
-        default:            return nvinfer1::DeviceType::kGPU;
-	}
-}
-#endif
-
-const char* modelTypeToStr( modelType format )
-{
-	switch(format)
-	{
-		case MODEL_CUSTOM:	return "custom";	
-		case MODEL_CAFFE:	return "caffe";
-		case MODEL_ONNX:	return "ONNX";
-		case MODEL_UFF:	return "UFF";
-		case MODEL_ENGINE:	return "engine";
-	}
-    return nullptr;
-}
-
-modelType modelTypeFromStr( const char* str )
-{
-	if( !str )
-		return MODEL_CUSTOM;
-
-	if( strcasecmp(str, "caffemodel") == 0 || strcasecmp(str, "caffe") == 0 )
-		return MODEL_CAFFE;
-	else if( strcasecmp(str, "onnx") == 0 )
-		return MODEL_ONNX;
-	else if( strcasecmp(str, "uff") == 0 )
-		return MODEL_UFF;
-	else if( strcasecmp(str, "engine") == 0 || strcasecmp(str, "plan") == 0 || strcasecmp(str, "trt") == 0 )
-		return MODEL_ENGINE;
-
-	return MODEL_CUSTOM;
-}
-
-modelType modelTypeFromPath( const char* path )
-{
-	if( !path )
-		return MODEL_CUSTOM;
-	
-	return modelTypeFromStr(fileExtension(path).c_str());
-}
-
-const char* profilerQueryToStr( profilerQuery query )
-{
-	switch(query)
-	{
-		case PROFILER_PREPROCESS:  return "Pre-Process";
-		case PROFILER_NETWORK:	  return "Network";
-		case PROFILER_POSTPROCESS: return "Post-Process";
-		case PROFILER_VISUALIZE:	  return "Visualize";
-		case PROFILER_TOTAL:	  return "Total";
-	}
-    return nullptr;
-}
-
-//---------------------------------------------------------------------
-tensorNet::Logger tensorNet::gLogger;
-
-// constructor
-tensorNet::tensorNet()
-{
-	mEngine   = NULL;
-	mInfer    = NULL;
-	mContext  = NULL;
-	mStream   = NULL;
-	mBindings	= NULL;
-
-	mMaxBatchSize   = 0;	
-	mEnableDebug    = false;
-	mEnableProfiler = false;
-
-	mModelType        = MODEL_CUSTOM;
-	mPrecision 	   = TYPE_FASTEST;
-	mDevice    	   = DEVICE_GPU;
-	mAllowGPUFallback = false;
-
-	mProfilerQueriesUsed = 0;
-	mProfilerQueriesDone = 0;
-
-	memset(mEventsCPU, 0, sizeof(mEventsCPU));
-	memset(mEventsGPU, 0, sizeof(mEventsGPU));
-	memset(mProfilerTimes, 0, sizeof(mProfilerTimes));
-
-#if NV_TENSORRT_MAJOR > 5
-	mWorkspaceSize = 32 << 20;
-#else
-	mWorkspaceSize = 16 << 20;
-#endif
-}
-
-
-// Destructor
-tensorNet::~tensorNet()
-{
-	if( mContext != NULL )
-	{
-		mContext->destroy();
-		mContext = NULL;
-	}
-	
-	if( mEngine != NULL )
-	{
-		mEngine->destroy();
-		mEngine = NULL;
-	}
-		
-	if( mInfer != NULL )
-	{
-		mInfer->destroy();
-		mInfer = NULL;
-	}
-	
-	for( size_t n=0; n < mInputs.size(); n++ )
-	{
-	#ifdef USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
-		CUDA_FREE(mInputs[n].CUDA);
-	#else
-		CUDA_FREE_HOST(mInputs[n].CUDA);
-	#endif
-	}
-	
-	for( size_t n=0; n < mOutputs.size(); n++ )
-		CUDA_FREE_HOST(mOutputs[n].CPU);
-	
-	free(mBindings);
-}
-
-
-// EnableProfiler
-void tensorNet::EnableLayerProfiler()
-{
-	mEnableProfiler = true;
-
-	if( mContext != NULL )
-		mContext->setProfiler(&gProfiler);
-}
-
-
-// EnableDebug
-void tensorNet::EnableDebug()
-{
-	mEnableDebug = true;
-}
-
-
-// DetectNativePrecisions()
-std::vector<precisionType> tensorNet::DetectNativePrecisions( deviceType device )
-{
-	std::vector<precisionType> types;
-	Logger logger;
-
-	// create a temporary builder for querying the supported types
-	nvinfer1::IBuilder* builder = CREATE_INFER_BUILDER(logger);
-		
-	if( !builder )
-	{
-		LogError(LOG_TRT "DetectNativePrecisions() failed to create TensorRT IBuilder instance\n");
-		return types;
-	}
-
-#if NV_TENSORRT_MAJOR >= 5 && NV_TENSORRT_MAJOR <= 7
-	if( device == DEVICE_DLA_0 || device == DEVICE_DLA_1 )
-		builder->setFp16Mode(true);
-
-	builder->setDefaultDeviceType( deviceTypeToTRT(device) );
-#endif
-
-	// FP32 is supported on all platforms
-	types.push_back(TYPE_FP32);
-
-	// detect fast (native) FP16
-	if( builder->platformHasFastFp16() )
-		types.push_back(TYPE_FP16);
-
-#if NV_TENSORRT_MAJOR >= 4
-	// detect fast (native) INT8
-	if( builder->platformHasFastInt8() )
-		types.push_back(TYPE_INT8);
-#endif
-
-	// print out supported precisions (optional)
-	const uint32_t numTypes = types.size();
-
-	LogVerbose(LOG_TRT "native precisions detected for %s:  ", deviceTypeToStr(device));
- 
-	for( uint32_t n=0; n < numTypes; n++ )
-	{
-		LogVerbose("%s", precisionTypeToStr(types[n]));
-
-		if( n < numTypes - 1 )
-			LogVerbose(", ");
-	}
-
-	LogVerbose("\n");
-	builder->destroy();
-	return types;
-}
-
-
-// DetectNativePrecision
-bool tensorNet::DetectNativePrecision( const std::vector<precisionType>& types, precisionType type )
-{
-	const uint32_t numTypes = types.size();
-
-	for( uint32_t n=0; n < numTypes; n++ )
-	{
-		if( types[n] == type )
-			return true;
-	}
-
-	return false;
-}
-
-
-// DetectNativePrecision
-bool tensorNet::DetectNativePrecision( precisionType precision, deviceType device )
-{
-	std::vector<precisionType> types = DetectNativePrecisions(device);
-	return DetectNativePrecision(types, precision);
-}
-
-
-// SelectPrecision
-precisionType tensorNet::SelectPrecision( precisionType precision, deviceType device, bool allowInt8 )
-{
-	LogVerbose(LOG_TRT "desired precision specified for %s: %s\n", deviceTypeToStr(device), precisionTypeToStr(precision));
-
-	if( precision == TYPE_DISABLED )
-	{
-		LogWarning(LOG_TRT "skipping network specified with precision TYPE_DISABLE\n");
-		LogWarning(LOG_TRT "please specify a valid precision to create the network\n");
-	}
-	else if( precision == TYPE_FASTEST )
-	{
-		if( !allowInt8 )
-			LogWarning(LOG_TRT "requested fasted precision for device %s without providing valid calibrator, disabling INT8\n", deviceTypeToStr(device));
-
-		precision = FindFastestPrecision(device, allowInt8);
-		LogVerbose(LOG_TRT "selecting fastest native precision for %s:  %s\n", deviceTypeToStr(device), precisionTypeToStr(precision));
-	}
-	else
-	{
-		if( !DetectNativePrecision(precision, device) )
-		{
-			LogWarning(LOG_TRT "precision %s is not supported for device %s\n", precisionTypeToStr(precision), deviceTypeToStr(device));
-			precision = FindFastestPrecision(device, allowInt8);
-			LogWarning(LOG_TRT "falling back to fastest precision for device %s (%s)\n", deviceTypeToStr(device), precisionTypeToStr(precision));
-		}
-
-		if( precision == TYPE_INT8 && !allowInt8 )
-			LogWarning(LOG_TRT "warning:  device %s using INT8 precision with RANDOM calibration\n", deviceTypeToStr(device));
-	}
-
-	return precision;
-}
-
-
-// FindFastestPrecision
-precisionType tensorNet::FindFastestPrecision( deviceType device, bool allowInt8 )
-{
-	std::vector<precisionType> types = DetectNativePrecisions(device);
-
-	if( allowInt8 && DetectNativePrecision(types, TYPE_INT8) )
-		return TYPE_INT8;
-	else if( DetectNativePrecision(types, TYPE_FP16) )
-		return TYPE_FP16;
-	else
-		return TYPE_FP32;
-}
-
-
-// Create an optimized GIE network from caffe prototxt and model file
-bool tensorNet::ProfileModel(const std::string& deployFile,			   // name for caffe prototxt
-					    const std::string& modelFile,			   // name for model 
-					    const std::vector<std::string>& inputs, 
-					    const std::vector<Dims3>& inputDims,
-					    const std::vector<std::string>& outputs,    // network outputs
-					    unsigned int maxBatchSize,			   // batch size - NB must be at least as large as the batch we want to run with
-					    precisionType precision, 
-					    deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, 	
-					    char** engineStream, size_t* engineSize)	   // output stream for the GIE model
-{
-	if( !engineStream || !engineSize )
-		return false;
-
-	// create builder and network definition interfaces
-	nvinfer1::IBuilder* builder = CREATE_INFER_BUILDER(gLogger);
-	
-#if NV_TENSORRT_MAJOR >= 8
-	nvinfer1::INetworkDefinition* network = builder->createNetworkV2(0);
-#else
-	nvinfer1::INetworkDefinition* network = builder->createNetwork();
-#endif
-
-	LogInfo(LOG_TRT "device %s, loading %s %s\n", deviceTypeToStr(device), deployFile.c_str(), modelFile.c_str());
-	
-	// parse the different types of model formats
-	if( mModelType == MODEL_CAFFE )
-	{
-		// parse the caffe model to populate the network, then set the outputs
-		nvcaffeparser1::ICaffeParser* parser = nvcaffeparser1::createCaffeParser();
-
-		nvinfer1::DataType modelDataType = (precision == TYPE_FP16) ? nvinfer1::DataType::kHALF : nvinfer1::DataType::kFLOAT; // import INT8 weights as FP32
-		const nvcaffeparser1::IBlobNameToTensor* blobNameToTensor =
-			parser->parse(deployFile.c_str(),		// caffe deploy file
-						  modelFile.c_str(),	// caffe model file
-						 *network,			// network definition that the parser will populate
-						  modelDataType);
-
-		if( !blobNameToTensor )
-		{
-			LogError(LOG_TRT "device %s, failed to parse caffe network\n", deviceTypeToStr(device));
-			return false;
-		}
-
-		// the caffe file has no notion of outputs, so we need to manually say which tensors the engine should generate	
-		const size_t num_outputs = outputs.size();
-		
-		for( size_t n=0; n < num_outputs; n++ )
-		{
-			nvinfer1::ITensor* tensor = blobNameToTensor->find(outputs[n].c_str());
-		
-			if( !tensor )
-				LogError(LOG_TRT "failed to retrieve tensor for Output \"%s\"\n", outputs[n].c_str());
-			else
-			{
-			#if NV_TENSORRT_MAJOR >= 4
-				nvinfer1::Dims3 dims = static_cast<nvinfer1::Dims3&&>(tensor->getDimensions());
-				LogVerbose(LOG_TRT "retrieved Output tensor \"%s\":  %ix%ix%i\n", tensor->getName(), dims.d[0], dims.d[1], dims.d[2]);
-			#endif
-			}
-
-			network->markOutput(*tensor);
-		}
-
-		//parser->destroy();
-	}
-#if NV_TENSORRT_MAJOR >= 5
-	else if( mModelType == MODEL_ONNX )
-	{
-	#if NV_TENSORRT_MAJOR >= 7
-		network->destroy();
-		network = builder->createNetworkV2(1U << (uint32_t)nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
-
-		if( !network )
-		{
-		  LogError(LOG_TRT "IBuilder::createNetworkV2(EXPLICIT_BATCH) failed\n");
-		  return false;
-		}
-	#endif
-
-		nvonnxparser::IParser* parser = nvonnxparser::createParser(*network, gLogger);
-
-		if( !parser )
-		{
-			LogError(LOG_TRT "failed to create nvonnxparser::IParser instance\n");
-			return false;
-		}
-
-    #if NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0
-        const int parserLogLevel = (int)nvinfer1::ILogger::Severity::kINFO;
-    #else
-        const int parserLogLevel = (int)nvinfer1::ILogger::Severity::kVERBOSE;
-    #endif
-
-		if( !parser->parseFromFile(modelFile.c_str(), parserLogLevel) )
-		{
-			LogError(LOG_TRT "failed to parse ONNX model '%s'\n", modelFile.c_str());
-			return false;
-		}
-
-		//parser->destroy();
-	}
-	else if( mModelType == MODEL_UFF )
-	{
-		// create parser instance
-		nvuffparser::IUffParser* parser = nvuffparser::createUffParser();
-		
-		if( !parser )
-		{
-			LogError(LOG_TRT "failed to create UFF parser\n");
-			return false;
-		}
-		
-		// register inputs
-		const size_t numInputs = inputs.size();
-
-		for( size_t n=0; n < numInputs; n++ )
-		{
-			if( !parser->registerInput(inputs[n].c_str(), inputDims[n], nvuffparser::UffInputOrder::kNCHW) )
-			{
-				LogError(LOG_TRT "failed to register input '%s' for UFF model '%s'\n", inputs[n].c_str(), modelFile.c_str());
-				return false;
-			}
-		}
-
-		// register outputs
-		/*const size_t numOutputs = outputs.size();
-		
-		for( uint32_t n=0; n < numOutputs; n++ )
-		{
-			if( !parser->registerOutput(outputs[n].c_str()) )
-				printf(LOG_TRT "failed to register output '%s' for UFF model '%s'\n", outputs[n].c_str(), modelFile.c_str());
-		}*/
-
-		// UFF outputs are forwarded to 'MarkOutput_0'
-		if( !parser->registerOutput("MarkOutput_0") )
-			LogError(LOG_TRT "failed to register output '%s' for UFF model '%s'\n", "MarkOutput_0", modelFile.c_str());
-
-		// parse network
-		if( !parser->parse(modelFile.c_str(), *network, nvinfer1::DataType::kFLOAT) )
-		{
-			LogError(LOG_TRT "failed to parse UFF model '%s'\n", modelFile.c_str());
-			return false;
-		}
-		
-		//parser->destroy();
-	}
-#endif
-
-#if NV_TENSORRT_MAJOR >= 4
-	if( precision == TYPE_INT8 && !calibrator )
-	{
-		// extract the dimensions of the network input blobs
-		std::map<std::string, nvinfer1::Dims3> inputDimensions;
-
-		for( int i=0, n=network->getNbInputs(); i < n; i++ )
-		{
-			nvinfer1::Dims dims = network->getInput(i)->getDimensions();
-
-		#if NV_TENSORRT_MAJOR >= 7
-			if( mModelType == MODEL_ONNX )
-				dims = shiftDims(dims);  // change NCHW to CHW for EXPLICIT_BATCH
-		#endif
-
-			//nvinfer1::Dims3 dims = static_cast<nvinfer1::Dims3&&>(network->getInput(i)->getDimensions());
-			inputDimensions.insert(std::make_pair(network->getInput(i)->getName(), static_cast<nvinfer1::Dims3&&>(dims)));
-			LogVerbose(LOG_TRT "retrieved Input tensor '%s':  %ix%ix%i\n", network->getInput(i)->getName(), dims.d[0], dims.d[1], dims.d[2]);
-		}
-
-		// default to random calibration
-		calibrator = new randInt8Calibrator(1, mCacheCalibrationPath, inputDimensions);
-		LogWarning(LOG_TRT "warning:  device %s using INT8 precision with RANDOM calibration\n", deviceTypeToStr(device));
-	}
-#endif
-
-	// configure the builder
-#if NV_TENSORRT_MAJOR >= 8
-	nvinfer1::IBuilderConfig* builderConfig = builder->createBuilderConfig();
-	
-	if( !ConfigureBuilder(builder, builderConfig, maxBatchSize, mWorkspaceSize, 
-					  precision, device, allowGPUFallback, calibrator) )
-	{
-		LogError(LOG_TRT "device %s, failed to configure builder\n", deviceTypeToStr(device));
-		return false;
-	}
-	
-	// attempt to load the timing cache
-	const nvinfer1::ITimingCache* timingCache = NULL;
-	
-	char timingCachePath[PATH_MAX];
-	sprintf(timingCachePath, "/usr/local/bin/networks/tensorrt.%i.timingcache", NV_TENSORRT_VERSION);
-
-	if( fileExists(timingCachePath) )
-	{
-		LogInfo(LOG_TRT "loading timing cache from %s\n", timingCachePath);
-		
-		void* timingCacheBuffer = NULL;
-		const size_t timingCacheSize = loadFile(timingCachePath, &timingCacheBuffer);
-		
-		if( timingCacheSize > 0 )
-		{
-			timingCache = builderConfig->createTimingCache(timingCacheBuffer, timingCacheSize);
-			free(timingCacheBuffer);
-		}
-	}
-	
-	if( !timingCache )
-	{
-		timingCache = builderConfig->createTimingCache(NULL, 0);  // create a new cache
-	
-		if( !timingCache )
-			LogWarning(LOG_TRT "couldn't create new timing cache\n");
-	}
-	
-	if( timingCache != NULL && !builderConfig->setTimingCache(*timingCache, false) )
-		LogWarning(LOG_TRT "failed to activate timing cache");
-#else
-	if( !ConfigureBuilder(builder, maxBatchSize, mWorkspaceSize, precision, device, allowGPUFallback, calibrator) )
-	{
-		LogError(LOG_TRT "device %s, failed to configure builder\n", deviceTypeToStr(device));
-		return false;
-	}
-#endif
-
-	// build CUDA engine
-	LogInfo(LOG_TRT "device %s, building CUDA engine (this may take a few minutes the first time a network is loaded)\n", deviceTypeToStr(device));
-
-	if( Log::GetLevel() < Log::VERBOSE )
-		LogInfo(LOG_TRT "info: to see status updates during engine building, enable verbose logging with --verbose\n");
-
-#if NV_TENSORRT_MAJOR >= 8
-	nvinfer1::ICudaEngine* engine = builder->buildEngineWithConfig(*network, *builderConfig);
-#else
-	nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);
-#endif
-
-	if( !engine )
-	{
-		LogError(LOG_TRT "device %s, failed to build CUDA engine\n", deviceTypeToStr(device));
-		return false;
-	}
-
-	LogSuccess(LOG_TRT "device %s, completed building CUDA engine\n", deviceTypeToStr(device));
-
-#if NV_TENSORRT_MAJOR >= 8
-	if( timingCache != NULL )
-	{
-		// save the updated timing cache
-		nvinfer1::IHostMemory* timingCacheMem = timingCache->serialize();
-		
-		if( timingCacheMem != NULL )
-		{
-			const char* timingCacheBuffer = (char*)timingCacheMem->data();
-			const size_t timingCacheSize = timingCacheMem->size();
-		
-			LogVerbose(LOG_TRT "saving timing cache to %s (%zu bytes)\n", timingCachePath, timingCacheSize);
-			
-			// write the cache file
-			FILE* timingCacheFile = NULL;
-			timingCacheFile = fopen(timingCachePath, "wb");
-
-			if( timingCacheFile != NULL )
-			{
-				if( fwrite(timingCacheBuffer,	1, timingCacheSize, timingCacheFile) != timingCacheSize )
-					LogWarning(LOG_TRT "failed to write %zu bytes to timing cache file %s\n", timingCacheSize, timingCachePath);
-			
-				fclose(timingCacheFile);
-			}
-			else
-			{
-				LogWarning(LOG_TRT "failed to open timing cache file for writing %s\n", timingCachePath);
-			}
-			
-			timingCacheMem->destroy();
-		}
-	
-		delete timingCache;
-	}
-	
-	builderConfig->destroy();
-#endif
-	
-	// we don't need the network definition any more, and we can destroy the parser
-	network->destroy();
-	//parser->destroy();
-	
-#if NV_TENSORRT_MAJOR >= 2
-	// serialize the engine
-	nvinfer1::IHostMemory* serMem = engine->serialize();
-
-	if( !serMem )
-	{
-		LogError(LOG_TRT "device %s, failed to serialize CUDA engine\n", deviceTypeToStr(device));
-		return false;
-	}
-
-	const char* serData = (char*)serMem->data();
-	const size_t serSize = serMem->size();
-
-	// allocate memory to store the bitstream
-	char* engineMemory = (char*)malloc(serSize);
-
-	if( !engineMemory )
-	{
-		LogError(LOG_TRT "failed to allocate %zu bytes to store CUDA engine\n", serSize);
-		return false;
-	}
-
-	memcpy(engineMemory, serData, serSize);
-	
-	*engineStream = engineMemory;
-	*engineSize = serSize;
-	
-	serMem->destroy();
-#else
-	engine->serialize(modelStream);
-#endif
-
-	// free builder resources
-	engine->destroy();
-	builder->destroy();
-	
-	return true;
-}
-
-
-// ConfigureBuilder
-#if NV_TENSORRT_MAJOR >= 8
-bool tensorNet::ConfigureBuilder( nvinfer1::IBuilder* builder, nvinfer1::IBuilderConfig* config,  
-				    		    uint32_t maxBatchSize, uint32_t workspaceSize, precisionType precision, 
-				    		    deviceType device, bool allowGPUFallback, 
-				    		    nvinfer1::IInt8Calibrator* calibrator )
-{
-	if( !builder )
-		return false;
-
-	LogVerbose(LOG_TRT "device %s, configuring network builder\n", deviceTypeToStr(device));
-		
-	builder->setMaxBatchSize(maxBatchSize);
-	config->setMaxWorkspaceSize(workspaceSize);
-
-	config->setMinTimingIterations(3); // allow time for GPU to spin up
-	config->setAvgTimingIterations(2);
-
-	if( mEnableDebug )
-		config->setFlag(nvinfer1::BuilderFlag::kDEBUG);
-	
-	// set up the builder for the desired precision
-	if( precision == TYPE_INT8 )
-	{
-		config->setFlag(nvinfer1::BuilderFlag::kINT8);
-		//config->setFlag(nvinfer1::BuilderFlag::kFP16); // TODO:  experiment for benefits of both INT8/FP16
-		
-		if( !calibrator )
-		{
-			LogError(LOG_TRT "device %s, INT8 requested but calibrator is NULL\n", deviceTypeToStr(device));
-			return false;
-		}
-
-		config->setInt8Calibrator(calibrator);
-	}
-	else if( precision == TYPE_FP16 )
-	{
-		config->setFlag(nvinfer1::BuilderFlag::kFP16);
-	}
-	
-	// set the default device type
-	config->setDefaultDeviceType(deviceTypeToTRT(device));
-
-	if( allowGPUFallback )
-		config->setFlag(nvinfer1::BuilderFlag::kGPU_FALLBACK);
-
-	LogInfo(LOG_TRT "device %s, building FP16:  %s\n", deviceTypeToStr(device), isFp16Enabled(config) ? "ON" : "OFF"); 
-	LogInfo(LOG_TRT "device %s, building INT8:  %s\n", deviceTypeToStr(device), isInt8Enabled(config) ? "ON" : "OFF"); 
-	LogInfo(LOG_TRT "device %s, workspace size: %u\n", deviceTypeToStr(device), workspaceSize);
-
-	return true;
-}
-
-#else  // NV_TENSORRT_MAJOR <= 7
-	
-bool tensorNet::ConfigureBuilder( nvinfer1::IBuilder* builder, uint32_t maxBatchSize, 
-				    		    uint32_t workspaceSize, precisionType precision, 
-				    		    deviceType device, bool allowGPUFallback, 
-				    		    nvinfer1::IInt8Calibrator* calibrator )
-{
-	if( !builder )
-		return false;
-
-	LogVerbose(LOG_TRT "device %s, configuring network builder\n", deviceTypeToStr(device));
-		
-	builder->setMaxBatchSize(maxBatchSize);
-	builder->setMaxWorkspaceSize(workspaceSize);
-
-	builder->setDebugSync(mEnableDebug);
-	builder->setMinFindIterations(3);	// allow time for GPU to spin up
-	builder->setAverageFindIterations(2);
-
-	// set up the builder for the desired precision
-	if( precision == TYPE_INT8 )
-	{
-	#if NV_TENSORRT_MAJOR >= 4
-		builder->setInt8Mode(true);
-		//builder->setFp16Mode(true);		// TODO:  experiment for benefits of both INT8/FP16
-		
-		if( !calibrator )
-		{
-			LogError(LOG_TRT "device %s, INT8 requested but calibrator is NULL\n", deviceTypeToStr(device));
-			return false;
-		}
-
-		builder->setInt8Calibrator(calibrator);
-	#else
-		LogError(LOG_TRT "INT8 precision requested, and TensorRT %u.%u doesn't meet minimum version for INT8\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
-		LogError(LOG_TRT "please use minumum version of TensorRT 4.0 or newer for INT8 support\n");
-
-		return false;
-	#endif
-	}
-	else if( precision == TYPE_FP16 )
-	{
-	#if NV_TENSORRT_MAJOR >= 4
-		builder->setFp16Mode(true);
-	#else
-		builder->setHalf2Mode(true);
-	#endif
-	}
-	
-	// set the default device type
-#if NV_TENSORRT_MAJOR >= 5
-	builder->setDefaultDeviceType(deviceTypeToTRT(device));
-
-	if( allowGPUFallback )
-		builder->allowGPUFallback(true);
-	
-#if !(NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0)
-	if( device == DEVICE_DLA_0 )
-		builder->setDLACore(0);
-	else if( device == DEVICE_DLA_1 )
-		builder->setDLACore(1);
-#endif
-#else
-	if( device != DEVICE_GPU )
-	{
-		LogError(LOG_TRT "device %s is not supported in TensorRT %u.%u\n", deviceTypeToStr(device), NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
-		return false;
-	}
-#endif
-
-	LogInfo(LOG_TRT "device %s, building FP16:  %s\n", deviceTypeToStr(device), isFp16Enabled(builder) ? "ON" : "OFF"); 
-	LogInfo(LOG_TRT "device %s, building INT8:  %s\n", deviceTypeToStr(device), isInt8Enabled(builder) ? "ON" : "OFF"); 
-	LogInfo(LOG_TRT "device %s, workspace size: %u\n", deviceTypeToStr(device), workspaceSize);
-
-	return true;
-}
-#endif
-
-
-// LoadNetwork
-bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
-					    const char* input_blob, const char* output_blob, uint32_t maxBatchSize,
-					    precisionType precision, deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
-{
-	std::vector<std::string> outputs;
-	outputs.push_back(output_blob);
-	
-	return LoadNetwork(prototxt_path, model_path, mean_path, input_blob, outputs, maxBatchSize, precision, device, allowGPUFallback );
-}
-
-
-// LoadNetwork
-bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
-					    const char* input_blob, const std::vector<std::string>& output_blobs, 
-					    uint32_t maxBatchSize, precisionType precision,
-				   	    deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
-{
-	return LoadNetwork(prototxt_path, model_path, mean_path,
-				    input_blob, Dims3(1,1,1), output_blobs,
-				    maxBatchSize, precision, device,
-				    allowGPUFallback, calibrator, stream);
-}
-
-
-// LoadNetwork
-bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
-					    const std::vector<std::string>& input_blobs, 
-					    const std::vector<std::string>& output_blobs, 
-					    uint32_t maxBatchSize, precisionType precision,
-				   	    deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
-{
-	std::vector<Dims3> input_dims;
-
-	for( size_t n=0; n < input_blobs.size(); n++ )
-		input_dims.push_back(Dims3(1,1,1));
-
-	return LoadNetwork(prototxt_path, model_path, mean_path,
-				    input_blobs, input_dims, output_blobs,
-				    maxBatchSize, precision, device,
-				    allowGPUFallback, calibrator, stream);
-}
-
-
-// LoadNetwork
-bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
-					    const char* input_blob, const Dims3& input_dim,
-					    const std::vector<std::string>& output_blobs, 
-					    uint32_t maxBatchSize, precisionType precision,
-				   	    deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
-{
-	std::vector<std::string> inputs;
-	std::vector<Dims3> input_dims;
-
-	inputs.push_back(input_blob);
-	input_dims.push_back(input_dim);
-
-	return LoadNetwork(prototxt_path, model_path, mean_path,
-				    inputs, input_dims, output_blobs,
-				    maxBatchSize, precision, device,
-				    allowGPUFallback, calibrator, stream);
-}
-
-		   
-// LoadNetwork
-bool tensorNet::LoadNetwork( const char* prototxt_path_, const char* model_path_, const char* mean_path, 
-					    const std::vector<std::string>& input_blobs, 
-					    const std::vector<Dims3>& input_dims,
-					    const std::vector<std::string>& output_blobs, 
-					    uint32_t maxBatchSize, precisionType precision,
-				   	    deviceType device, bool allowGPUFallback,
-					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
-{
-#if NV_TENSORRT_MAJOR >= 4
-	LogInfo(LOG_TRT "TensorRT version %u.%u.%u\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR, NV_TENSORRT_PATCH);
-#else
-	LogInfo(LOG_TRT "TensorRT version %u.%u\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
-#endif
-
-	/*
-	 * validate arguments
-	 */
-	if( !model_path_ )
-	{
-		LogError(LOG_TRT "model path was NULL - must have valid model path to LoadNetwork()\n");
-		return false;
-	}
-
-	if( input_blobs.size() == 0 || output_blobs.size() == 0 )
-	{
-		LogError(LOG_TRT "requested number of input layers or output layers was zero\n");
-		return false;
-	}
-
-	if( input_blobs.size() != input_dims.size() )
-	{
-		LogError(LOG_TRT "input mismatch - requested %zu input layers, but only %zu input dims\n", input_blobs.size(), input_dims.size());
-		return false;
-	}
-
-	
-	/*
-	 * load NV inference plugins
-	 */
-#if NV_TENSORRT_MAJOR > 4
-	static bool loadedPlugins = false;
-
-	if( !loadedPlugins )
-	{
-		LogVerbose(LOG_TRT "loading NVIDIA plugins...\n");
-
-		loadedPlugins = initLibNvInferPlugins(&gLogger, "");
-
-		if( !loadedPlugins )
-			LogError(LOG_TRT "failed to load NVIDIA plugins\n");
-		else
-			LogVerbose(LOG_TRT "completed loading NVIDIA plugins.\n");
-	}
-#endif
-
-	/*
-	 * verify the prototxt and model paths
-	 */
-	const std::string model_path    = locateFile(model_path_);
-	const std::string prototxt_path = locateFile(prototxt_path_ != NULL ? prototxt_path_ : "");
-	
-	const std::string model_ext = fileExtension(model_path_);
-	const modelType   model_fmt = modelTypeFromStr(model_ext.c_str());
-
-	LogVerbose(LOG_TRT "detected model format - %s  (extension '.%s')\n", modelTypeToStr(model_fmt), model_ext.c_str());
-
-	if( model_fmt == MODEL_CUSTOM )
-	{
-		LogError(LOG_TRT "model format '%s' not supported by jetson-inference\n", modelTypeToStr(model_fmt));
-		return false;
-	}
-#if NV_TENSORRT_MAJOR < 5
-	else if( model_fmt == MODEL_ONNX )
-	{
-		LogError(LOG_TRT "importing ONNX models is not supported in TensorRT %u.%u (version >= 5.0 required)\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
-		return false;
-	}
-	else if( model_fmt == MODEL_UFF )
-	{
-		LogError(LOG_TRT "importing UFF models is not supported in TensorRT %u.%u (version >= 5.0 required)\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
-		return false;
-	}
-#endif
-	else if( model_fmt == MODEL_CAFFE && !prototxt_path_ )
-	{
-		LogError(LOG_TRT "attempted to load caffe model without specifying prototxt file\n");
-		return false;
-	}
-	else if( model_fmt == MODEL_ENGINE )
-	{
-		if( !LoadEngine(model_path.c_str(), input_blobs, output_blobs, NULL, device, stream) )
-		{
-			LogError(LOG_TRT "failed to load %s\n", model_path.c_str());
-			return false;
-		}
-
-		mModelType = model_fmt;
-		mModelPath = model_path;
-		mModelFile = pathFilename(mModelPath);
-		
-		LogSuccess(LOG_TRT "device %s, initialized %s\n", deviceTypeToStr(device), mModelPath.c_str());	
-		return true;
-	}
-
-	mModelType = model_fmt;
-
-
-	/*
-	 * resolve the desired precision to a specific one that's available
-	 */
-	precision = SelectPrecision(precision, device, (calibrator != NULL));
-
-	if( precision == TYPE_DISABLED )
-		return false;
-
-	
-	/*
-	 * attempt to load network engine from cache before profiling with tensorRT
-	 */	
-	char* engineStream = NULL;
-	size_t engineSize = 0;
-
-	char cache_prefix[PATH_MAX];
-	char cache_path[PATH_MAX];
-
-	sprintf(cache_prefix, "%s.%u.%u.%i.%s.%s", model_path.c_str(), maxBatchSize, (uint32_t)allowGPUFallback, NV_TENSORRT_VERSION, deviceTypeToStr(device), precisionTypeToStr(precision));
-	sprintf(cache_path, "%s.calibration", cache_prefix);
-	mCacheCalibrationPath = cache_path;
-	
-	sprintf(cache_path, "%s.%s", model_path.c_str(), CHECKSUM_TYPE);
-	mChecksumPath = cache_path;
-	
-	sprintf(cache_path, "%s.engine", cache_prefix);
-	mCacheEnginePath = cache_path;	
-
-	// check for existence of cache
-	if( !ValidateEngine(model_path.c_str(), cache_path, mChecksumPath.c_str()) )
-	{
-		LogVerbose(LOG_TRT "cache file invalid, profiling network model on device %s\n", deviceTypeToStr(device));
-	
-		// check for existence of model
-		if( model_path.size() == 0 )
-		{
-			LogError("\nerror:  model file '%s' was not found.\n", model_path_);
-			LogInfo("%s\n", LOG_DOWNLOADER_TOOL);
-			return 0;
-		}
-
-		// parse the model and profile the engine
-		if( !ProfileModel(prototxt_path, model_path, input_blobs, input_dims,
-					   output_blobs, maxBatchSize, precision, device, 
-					   allowGPUFallback, calibrator, &engineStream, &engineSize) )
-		{
-			LogError(LOG_TRT "device %s, failed to load %s\n", deviceTypeToStr(device), model_path_);
-			return 0;
-		}
-	
-		LogVerbose(LOG_TRT "network profiling complete, saving engine cache to %s\n", cache_path);
-		
-		// write the cache file
-		FILE* cacheFile = NULL;
-		cacheFile = fopen(cache_path, "wb");
-
-		if( cacheFile != NULL )
-		{
-			if( fwrite(engineStream,	1, engineSize, cacheFile) != engineSize )
-				LogError(LOG_TRT "failed to write %zu bytes to engine cache file %s\n", engineSize, cache_path);
-		
-			fclose(cacheFile);
-		}
-		else
-		{
-			LogError(LOG_TRT "failed to open engine cache file for writing %s\n", cache_path);
-		}
-
-		LogSuccess(LOG_TRT "device %s, completed saving engine cache to %s\n", deviceTypeToStr(device), cache_path);
-		
-		// write the checksum file
-		LogVerbose(LOG_TRT "saving model checksum to %s\n", mChecksumPath.c_str());
-		
-		char cmd[PATH_MAX * 2 + 256];
-		snprintf(cmd, sizeof(cmd), "%s %s | awk '{print $1}' > %s", CHECKSUM_TYPE, model_path.c_str(), mChecksumPath.c_str());
-	
-		LogVerbose(LOG_TRT "%s\n", cmd);
-	
-		const int result = system(cmd);
-		
-		if( result != 0 )
-			LogError(LOG_TRT "failed to save model checksum to %s\n", mChecksumPath.c_str());
-	}
-	else
-	{
-		if( !LoadEngine(cache_path, &engineStream, &engineSize) )
-			return false;
-	}
-
-	LogSuccess(LOG_TRT "device %s, loaded %s\n", deviceTypeToStr(device), model_path.c_str());
-	
-
-	/*
-	 * create the runtime engine instance
-	 */
-	if( !LoadEngine(engineStream, engineSize, input_blobs, output_blobs, NULL, device, stream) )
-	{
-		LogError(LOG_TRT "failed to create TensorRT engine for %s, device %s\n", model_path.c_str(), deviceTypeToStr(device));
-		return false;
-	}
-
-	free(engineStream); // not used anymore
-
-	mPrototxtPath     = prototxt_path;
-	mModelPath        = model_path;
-	mModelFile        = pathFilename(mModelPath);
-	mPrecision        = precision;
-	mAllowGPUFallback = allowGPUFallback;
-	mMaxBatchSize 	   = maxBatchSize;
-
-	if( mean_path != NULL )
-		mMeanPath = mean_path;
-
-	LogInfo(LOG_TRT "\n");
-	LogSuccess(LOG_TRT "device %s, %s initialized.\n", deviceTypeToStr(device), mModelPath.c_str());	
-	
-	return true;
-}
-
-
-// LoadEngine
-bool tensorNet::LoadEngine( char* engine_stream, size_t engine_size,
-			  		   const std::vector<std::string>& input_blobs, 
-			  		   const std::vector<std::string>& output_blobs,
-			  		   nvinfer1::IPluginFactory* pluginFactory,
-					   deviceType device, cudaStream_t stream )
-{
-	/*
-	 * create runtime inference engine execution context
-	 */
-	nvinfer1::IRuntime* infer = CREATE_INFER_RUNTIME(gLogger);
-	
-	if( !infer )
-	{
-		LogError(LOG_TRT "device %s, failed to create TensorRT runtime\n", deviceTypeToStr(device));
-		return false;
-	}
-
-#if NV_TENSORRT_MAJOR >= 5 
-#if !(NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0)
-	// if using DLA, set the desired core before deserialization occurs
-	if( device == DEVICE_DLA_0 )
-	{
-		LogVerbose(LOG_TRT "device %s, enabling DLA core 0\n", deviceTypeToStr(device));
-		infer->setDLACore(0);
-	}
-	else if( device == DEVICE_DLA_1 )
-	{
-		LogVerbose(LOG_TRT "device %s, enabling DLA core 1\n", deviceTypeToStr(device));
-		infer->setDLACore(1);
-	}
-#endif
-#endif
-
-#if NV_TENSORRT_MAJOR > 1
-	nvinfer1::ICudaEngine* engine = infer->deserializeCudaEngine(engine_stream, engine_size, pluginFactory);
-#else
-	nvinfer1::ICudaEngine* engine = infer->deserializeCudaEngine(engine_stream, engine_size); //infer->deserializeCudaEngine(modelStream);
-#endif
-
-	if( !engine )
-	{
-		LogError(LOG_TRT "device %s, failed to create CUDA engine\n", deviceTypeToStr(device));
-		return false;
-	}
-
-	if( !LoadEngine(engine, input_blobs, output_blobs, device, stream) )
-	{
-		LogError(LOG_TRT "device %s, failed to create resources for CUDA engine\n", deviceTypeToStr(device));
-		return false;
-	}	
-
-	mInfer = infer;
-	return true;
-}
-
-
-// LoadEngine
-bool tensorNet::LoadEngine( nvinfer1::ICudaEngine* engine,
- 			  		   const std::vector<std::string>& input_blobs, 
-			  		   const std::vector<std::string>& output_blobs,
-			  		   deviceType device, cudaStream_t stream)
-{
-	if( !engine )
-		return NULL;
-
-	nvinfer1::IExecutionContext* context = engine->createExecutionContext();
-	
-	if( !context )
-	{
-		LogError(LOG_TRT "device %s, failed to create execution context\n", deviceTypeToStr(device));
-		return 0;
-	}
-
-	if( mEnableDebug )
-	{
-		LogVerbose(LOG_TRT "device %s, enabling context debug sync.\n", deviceTypeToStr(device));
-		context->setDebugSync(true);
-	}
-
-	if( mEnableProfiler )
-		context->setProfiler(&gProfiler);
-
-	mMaxBatchSize = engine->getMaxBatchSize();
-
-	LogInfo(LOG_TRT "\n");
-	LogInfo(LOG_TRT "CUDA engine context initialized on device %s:\n", deviceTypeToStr(device));
-	LogInfo(LOG_TRT "   -- layers       %i\n", engine->getNbLayers());
-	LogInfo(LOG_TRT "   -- maxBatchSize %u\n", mMaxBatchSize);
-	
-#if NV_TENSORRT_MAJOR <= 7
-	LogInfo(LOG_TRT "   -- workspace    %zu\n", engine->getWorkspaceSize());
-#endif
-
-#if NV_TENSORRT_MAJOR >= 4
-	LogInfo(LOG_TRT "   -- deviceMemory %zu\n", engine->getDeviceMemorySize());
-	LogInfo(LOG_TRT "   -- bindings     %i\n", engine->getNbBindings());
-
-	/*
-	 * print out binding info
-	 */
-	const int numBindings = engine->getNbBindings();
-	
-	for( int n=0; n < numBindings; n++ )
-	{
-		LogInfo(LOG_TRT "   binding %i\n", n);
-
-		const char* bind_name = engine->getBindingName(n);
-
-		LogInfo("                -- index   %i\n", n);
-		LogInfo("                -- name    '%s'\n", bind_name);
-		LogInfo("                -- type    %s\n", dataTypeToStr(engine->getBindingDataType(n)));
-		LogInfo("                -- in/out  %s\n", engine->bindingIsInput(n) ? "INPUT" : "OUTPUT");
-
-		const nvinfer1::Dims bind_dims = engine->getBindingDimensions(n);
-
-		LogInfo("                -- # dims  %i\n", bind_dims.nbDims);
-		
-		for( int i=0; i < bind_dims.nbDims; i++ )
-		#if NV_TENSORRT_MAJOR >= 8
-			LogInfo("                -- dim #%i  %i\n", i, bind_dims.d[i]);	
-		#else
-			LogInfo("                -- dim #%i  %i (%s)\n", i, bind_dims.d[i], dimensionTypeToStr(bind_dims.type[i]));	
-		#endif
-	}
-
-	LogInfo(LOG_TRT "\n");
-#endif
-
-	/*
-	 * setup network input buffers
-	 */
-	const int numInputs = input_blobs.size();
-	
-	for( int n=0; n < numInputs; n++ )
-	{
-		const int inputIndex = engine->getBindingIndex(input_blobs[n].c_str());	
-		
-		if( inputIndex < 0 )
-		{
-			LogError(LOG_TRT "failed to find requested input layer %s in network\n", input_blobs[n].c_str());
-			return false;
-		}
-
-		LogVerbose(LOG_TRT "binding to input %i %s  binding index:  %i\n", n, input_blobs[n].c_str(), inputIndex);
-
-	#if NV_TENSORRT_MAJOR > 1
-		nvinfer1::Dims inputDims = validateDims(engine->getBindingDimensions(inputIndex));
-
-	#if NV_TENSORRT_MAJOR >= 7
-	    if( mModelType == MODEL_ONNX )
-		   inputDims = shiftDims(inputDims);   // change NCHW to CHW if EXPLICIT_BATCH set
-	#endif
-	#else
-		Dims3 inputDims = engine->getBindingDimensions(inputIndex);
-	#endif
-
-		const size_t inputSize = mMaxBatchSize * sizeDims(inputDims) * sizeof(float);
-		LogVerbose(LOG_TRT "binding to input %i %s  dims (b=%u c=%u h=%u w=%u) size=%zu\n", n, input_blobs[n].c_str(), mMaxBatchSize, DIMS_C(inputDims), DIMS_H(inputDims), DIMS_W(inputDims), inputSize);
-
-		// allocate memory to hold the input buffer
-		void* inputCPU  = NULL;
-		void* inputCUDA = NULL;
-
-	#ifdef USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
-		if( CUDA_FAILED(cudaMalloc((void**)&inputCUDA, inputSize)) )
-		{
-			LogError(LOG_TRT "failed to alloc CUDA device memory for tensor input, %zu bytes\n", inputSize);
-			return false;
-		}
-		
-		CUDA(cudaMemset(inputCUDA, 0, inputSize));
-	#else
-		if( !cudaAllocMapped((void**)&inputCPU, (void**)&inputCUDA, inputSize) )
-		{
-			LogError(LOG_TRT "failed to alloc CUDA mapped memory for tensor input, %zu bytes\n", inputSize);
-			return false;
-		}
-	#endif
-	
-		layerInfo l;
-		
-		l.CPU  = (float*)inputCPU;
-		l.CUDA = (float*)inputCUDA;
-		l.size = inputSize;
-		l.name = input_blobs[n];
-		l.binding = inputIndex;
-		
-		copyDims(&l.dims, &inputDims);
-		mInputs.push_back(l);
-	}
-
-
-	/*
-	 * setup network output buffers
-	 */
-	const int numOutputs = output_blobs.size();
-	
-	for( int n=0; n < numOutputs; n++ )
-	{
-		const int outputIndex = engine->getBindingIndex(output_blobs[n].c_str());
-
-		if( outputIndex < 0 )
-		{
-			LogError(LOG_TRT "failed to find requested output layer %s in network\n", output_blobs[n].c_str());
-			return false;
-		}
-
-		LogVerbose(LOG_TRT "binding to output %i %s  binding index:  %i\n", n, output_blobs[n].c_str(), outputIndex);
-
-	#if NV_TENSORRT_MAJOR > 1
-		nvinfer1::Dims outputDims = validateDims(engine->getBindingDimensions(outputIndex));
-
-	#if NV_TENSORRT_MAJOR >= 7
-		if( mModelType == MODEL_ONNX )
-			outputDims = shiftDims(outputDims);  // change NCHW to CHW if EXPLICIT_BATCH set
-	#endif
-	#else
-		Dims3 outputDims = engine->getBindingDimensions(outputIndex);
-	#endif
-
-		const size_t outputSize = mMaxBatchSize * sizeDims(outputDims) * sizeof(float);
-		LogVerbose(LOG_TRT "binding to output %i %s  dims (b=%u c=%u h=%u w=%u) size=%zu\n", n, output_blobs[n].c_str(), mMaxBatchSize, DIMS_C(outputDims), DIMS_H(outputDims), DIMS_W(outputDims), outputSize);
-	
-		// allocate output memory 
-		void* outputCPU  = NULL;
-		void* outputCUDA = NULL;
-		
-		//if( CUDA_FAILED(cudaMalloc((void**)&outputCUDA, outputSize)) )
-		if( !cudaAllocMapped((void**)&outputCPU, (void**)&outputCUDA, outputSize) )
-		{
-			LogError(LOG_TRT "failed to alloc CUDA mapped memory for tensor output, %zu bytes\n", outputSize);
-			return false;
-		}
-	
-		layerInfo l;
-		
-		l.CPU  = (float*)outputCPU;
-		l.CUDA = (float*)outputCUDA;
-		l.size = outputSize;
-		l.name = output_blobs[n];
-		l.binding = outputIndex;
-		
-		copyDims(&l.dims, &outputDims);
-		mOutputs.push_back(l);
-	}
-	
-	/*
-	 * create list of binding buffers
-	 */
-	const int bindingSize = numBindings * sizeof(void*);
-
-	mBindings = (void**)malloc(bindingSize);
-
-	if( !mBindings )
-	{
-		LogError(LOG_TRT "failed to allocate %u bytes for bindings list\n", bindingSize);
-		return false;
-	}
-
-	memset(mBindings, 0, bindingSize);
-
-	for( uint32_t n=0; n < GetInputLayers(); n++ )
-		mBindings[mInputs[n].binding] = mInputs[n].CUDA;
-
-	for( uint32_t n=0; n < GetOutputLayers(); n++ )
-		mBindings[mOutputs[n].binding] = mOutputs[n].CUDA;
-	
-	// find unassigned bindings and allocate them
-	for( uint32_t n=0; n < numBindings; n++ )
-	{
-		if( mBindings[n] != NULL )
-			continue;
-		
-		const size_t bindingSize = sizeDims(validateDims(engine->getBindingDimensions(n))) * mMaxBatchSize * sizeof(float);
-		
-		if( CUDA_FAILED(cudaMalloc(&mBindings[n], bindingSize)) )
-		{
-			LogError(LOG_TRT "failed to allocate %zu bytes for unused binding %u\n", bindingSize, n);
-			return false;
-		}
-		
-		LogVerbose(LOG_TRT "allocated %zu bytes for unused binding %u\n", bindingSize, n);
-	}
-	
-
-	/*
-	 * create events for timing
-	 */
-	for( int n=0; n < PROFILER_TOTAL * 2; n++ )
-		CUDA(cudaEventCreate(&mEventsGPU[n]));
-	
-	mEngine  = engine;
-	mDevice  = device;
-	mContext = context;
-	
-	SetStream(stream);	// set default device stream
-
-	return true;
-}
-
-
-// LoadEngine
-bool tensorNet::LoadEngine( const char* engine_filename,
-			  		   const std::vector<std::string>& input_blobs, 
-			  		   const std::vector<std::string>& output_blobs,
-			  		   nvinfer1::IPluginFactory* pluginFactory,
-					   deviceType device, cudaStream_t stream )
-{
-	char* engineStream = NULL;
-	size_t engineSize = 0;
-
-	// load the engine file contents
-	if( !LoadEngine(engine_filename, &engineStream, &engineSize) )
-		return false;
-
-	// load engine resources from stream
-	if( !LoadEngine(engineStream, engineSize, input_blobs, output_blobs,
-				 pluginFactory, device, stream) )
-	{
-		free(engineStream);
-		return false;
-	}
-
-	free(engineStream);
-	return true;
-}
-
-
-// LoadEngine
-bool tensorNet::LoadEngine( const char* filename, char** stream, size_t* size )
-{
-	if( !filename || !stream || !size )
-		return false;
-
-	LogInfo(LOG_TRT "loading network plan from engine cache... %s\n", filename);
-		
-	void* engineStream = NULL;
-	const size_t engineSize = loadFile(filename, &engineStream);
-	
-	if( engineSize == 0 )
-	{
-		LogError(LOG_TRT "failed to load engine cache from %s\n", filename);
-		return false;
-	}
-	
-	*stream = (char*)engineStream;
-	*size = engineSize;
-	
-	return true;
-}
-
-
-// ValidateEngine
-bool tensorNet::ValidateEngine( const char* model_path, const char* cache_path, const char* checksum_path )
-{
-	// check for existence of cache
-	if( !fileExists(cache_path) )
-	{
-		LogVerbose(LOG_TRT "could not find engine cache %s\n", cache_path);
-		return false;
-	}
-	
-	LogVerbose(LOG_TRT "found engine cache file %s\n", cache_path);
-	
-	// check for existence of checksum
-	if( !fileExists(checksum_path) )
-	{
-		LogVerbose(LOG_TRT "could not find model checksum %s\n", checksum_path);
-		return false;
-	}
-	
-	LogVerbose(LOG_TRT "found model checksum %s\n", checksum_path);
-	
-	// validate that the checksum matches the original model
-	char cmd[PATH_MAX * 2 + 256];
-	snprintf(cmd, sizeof(cmd), "echo \"$(cat %s) %s\" | %s --check --status", checksum_path, model_path, CHECKSUM_TYPE);  // https://superuser.com/a/1468626
-	
-	LogVerbose(LOG_TRT "%s\n", cmd);
-	
-	const int result = system(cmd);
-	
-	if( result != 0 )
-	{
-		LogVerbose(LOG_TRT "model did not match checksum %s (return code %i)\n", checksum_path, result);
-		return false;
-	}
-	
-	LogVerbose(LOG_TRT "model matched checksum %s\n", checksum_path);
-	return true;
-}
-
-
-// CreateStream
-cudaStream_t tensorNet::CreateStream( bool nonBlocking )
-{
-	uint32_t flags = cudaStreamDefault;
-
-	if( nonBlocking )
-		flags = cudaStreamNonBlocking;
-
-	cudaStream_t stream = NULL;
-
-	if( CUDA_FAILED(cudaStreamCreateWithFlags(&stream, flags)) )
-		return NULL;
-
-	SetStream(stream);
-	return stream;
-}
-
-
-// SetStream
-void tensorNet::SetStream( cudaStream_t stream )
-{
-	mStream = stream;
-
-	if( !mStream )
-		return;
-}	
-
-
-// ProcessNetwork
-bool tensorNet::ProcessNetwork( bool sync )
-{
-	if( TENSORRT_VERSION_CHECK(8,4,1) && mModelType == MODEL_ONNX )
-	{
-		// on TensorRT 8.4.1 (JetPack 5.0.2 / L4T R35.1.0) and newer, this warning appears:
-		// the execute() method has been deprecated when used with engines built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. Please use executeV2() instead.
-		// also, the batchSize argument passed into this function has no effect on changing the input shapes. Please use setBindingDimensions() function to change input shapes instead.
-		if( sync )
-		{
-			if( !mContext->executeV2(mBindings) )
-			{
-				LogError(LOG_TRT "failed to execute TensorRT context on device %s\n", deviceTypeToStr(mDevice));
-				return false;
-			}
-		}
-		else
-		{
-			if( !mContext->enqueueV2(mBindings, mStream, NULL) )
-			{
-				LogError(LOG_TRT "failed to enqueue TensorRT context on device %s\n", deviceTypeToStr(mDevice));
-				return false;
-			}
-		}
-	}
-	else
-	{
-		if( sync )
-		{
-			if( !mContext->execute(1, mBindings) )
-			{
-				LogError(LOG_TRT "failed to execute TensorRT context on device %s\n", deviceTypeToStr(mDevice));
-				return false;
-			}
-		}
-		else
-		{
-			if( !mContext->enqueue(1, mBindings, mStream, NULL) )
-			{
-				LogError(LOG_TRT "failed to enqueue TensorRT context on device %s\n", deviceTypeToStr(mDevice));
-				return false;
-			}
-		}
-	}
-	
-	return true;
-}
-
-
-// validateClassLabels
-static bool validateClassLabels( std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses )
-{
-	const int numLoaded = descriptions.size();
-	LogVerbose(LOG_TRT "loaded %i class labels\n", numLoaded);
-	
-	if( expectedClasses > 0 )
-	{
-		if( numLoaded != expectedClasses )
-			LogError(LOG_TRT "didn't load expected number of class descriptions  (%i of %i)\n", numLoaded, expectedClasses);
-
-		if( numLoaded < expectedClasses )
-		{
-			LogWarning(LOG_TRT "filling in remaining %i class descriptions with default labels\n", (expectedClasses - numLoaded));
-	
-			for( int n=numLoaded; n < expectedClasses; n++ )
-			{
-				char synset[10];
-				sprintf(synset, "n%08i", n);
-
-				char desc[64];
-				sprintf(desc, "Class #%i", n);
-
-				synsets.push_back(synset);
-				descriptions.push_back(desc);
-			}
-		}
-	}
-	else if( numLoaded == 0 )
-	{
-		return false;
-	}
-	
-	/*for( uint32_t n=0; n < descriptions.size(); n++ )
-		LogVerbose(LOG_TRT "detectNet -- class label #%u:  '%s'\n", n, descriptions[n].c_str());*/
-	
-	return true;
-}
-
-	
-// LoadClassLabels
-bool tensorNet::LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses )
-{
-	if( !filename )
-		return validateClassLabels(descriptions, synsets, expectedClasses);
-	
-	// locate the file
-	const std::string path = locateFile(filename);
-
-	if( path.length() == 0 )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassLabels() failed to find %s\n", filename);
-		return validateClassLabels(descriptions, synsets, expectedClasses);
-	}
-
-	// open the file
-	FILE* f = fopen(path.c_str(), "r");
-	
-	if( !f )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassLabels() failed to open %s\n", path.c_str());
-		return validateClassLabels(descriptions, synsets, expectedClasses);
-	}
-	
-	descriptions.clear();
-	synsets.clear();
-
-	// read class descriptions
-	char str[512];
-	uint32_t customClasses = 0;
-
-	while( fgets(str, 512, f) != NULL )
-	{
-		const int syn = 9;  // length of synset prefix (in characters)
-		const int len = strlen(str);
-		
-		if( len > syn && str[0] == 'n' && str[syn] == ' ' )
-		{
-			str[syn]   = 0;
-			str[len-1] = 0;
-	
-			const std::string a = str;
-			const std::string b = (str + syn + 1);
-	
-			//printf("a=%s b=%s\n", a.c_str(), b.c_str());
-
-			synsets.push_back(a);
-			descriptions.push_back(b);
-		}
-		else if( len > 0 )	// no 9-character synset prefix (i.e. from DIGITS snapshot)
-		{
-			char a[10];
-			sprintf(a, "n%08u", customClasses);
-
-			//printf("a=%s b=%s (custom non-synset)\n", a, str);
-			customClasses++;
-
-			if( str[len-1] == '\n' )
-				str[len-1] = 0;
-
-			synsets.push_back(a);
-			descriptions.push_back(str);
-		}
-	}
-	
-	fclose(f);
-	return validateClassLabels(descriptions, synsets, expectedClasses);
-}
-
-
-// LoadClassLabels
-bool tensorNet::LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, int expectedClasses )
-{
-	std::vector<std::string> synsets;
-	return LoadClassLabels(filename, descriptions, synsets, expectedClasses);
-}
-
-
-// validateClassColors
-static bool validateClassColors( float4* colors, int numLoaded, int expectedClasses, float defaultAlpha )
-{
-	LogVerbose(LOG_TRT "loaded %i class colors\n", numLoaded);
-	
-	if( expectedClasses > 0 )
-	{
-		if( numLoaded != expectedClasses )
-			LogWarning(LOG_TRT "didn't load expected number of class colors  (%i of %i)\n", numLoaded, expectedClasses);
-
-		if( numLoaded < expectedClasses )
-		{
-			LogWarning(LOG_TRT "filling in remaining %i class colors with default colors\n", (expectedClasses - numLoaded));
-	
-			for( int n=numLoaded; n < expectedClasses; n++ )
-			{
-				colors[n] = tensorNet::GenerateColor(n, defaultAlpha);
-				//LogVerbose(LOG_TRT "class color %i  (%f %f %f %f\n", n, colors[n].x, colors[n].y, colors[n].z, colors[n].w);
-			}
-		}
-	}
-	else if( numLoaded == 0 )
-	{
-		return false;
-	}
-
-	return true;
-}
-
-
-// LoadClassColors
-bool tensorNet::LoadClassColors( const char* filename, float4* colors, int expectedClasses, float defaultAlpha )
-{
-	// validate parameters
-	if( !colors || expectedClasses <= 0 )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassColors() had invalid/NULL parameters\n");
-		return false;
-	}
-	
-	if( !filename )
-		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
-	
-	// locate the file
-	const std::string path = locateFile(filename);
-
-	if( path.length() == 0 )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassColors() failed to find %s\n", filename);
-		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
-	}
-	
-	// open the file
-	FILE* f = fopen(path.c_str(), "r");
-	
-	if( !f )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassColors() failed to open %s\n", path.c_str());
-		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
-	}
-	
-	// read class colors
-	char str[512];
-	int numLoaded = 0;
-
-	while( fgets(str, 512, f) != NULL && numLoaded < expectedClasses )
-	{
-		const int len = strlen(str);
-		
-		if( len <= 0 )
-			continue;
-		
-		if( str[len-1] == '\n' )
-			str[len-1] = 0;
-
-		float r = 255;
-		float g = 255;
-		float b = 255;
-		float a = defaultAlpha;
-
-		sscanf(str, "%f %f %f %f", &r, &g, &b, &a);
-		LogVerbose(LOG_TRT "class %02i  color %f %f %f %f\n", numLoaded, r, g, b, a);
-		colors[numLoaded] = make_float4(r, g, b, a);
-		numLoaded++; 
-	}
-	
-	fclose(f);
-	return validateClassColors(colors, numLoaded, expectedClasses, defaultAlpha);
-}
-
-
-// LoadClassColors
-bool tensorNet::LoadClassColors( const char* filename, float4** colors, int expectedClasses, float defaultAlpha )
-{
-	// validate parameters
-	if( !colors || expectedClasses <= 0 )
-	{
-		LogError(LOG_TRT "tensorNet::LoadClassColors() had invalid/NULL parameters\n");
-		return false;
-	}
-	
-	// allocate memory
-	if( !cudaAllocMapped((void**)colors, expectedClasses * sizeof(float4)) )
-		return false;
-	
-	// load colors
-	return LoadClassColors(filename, colors[0], expectedClasses, defaultAlpha);
-}
-
-
-// GenerateColor
-float4 tensorNet::GenerateColor( uint32_t classID, float alpha )
-{
-	// the first color is black, skip that one
-	classID += 1;
-
-	// https://github.com/dusty-nv/pytorch-segmentation/blob/16882772bc767511d892d134918722011d1ea771/datasets/sun_remap.py#L90
-	#define bitget(byteval, idx)	((byteval & (1 << idx)) != 0)
-	
-	int r = 0;
-	int g = 0;
-	int b = 0;
-	int c = classID;
-
-	for( int j=0; j < 8; j++ )
-	{
-		r = r | (bitget(c, 0) << 7 - j);
-		g = g | (bitget(c, 1) << 7 - j);
-		b = b | (bitget(c, 2) << 7 - j);
-		c = c >> 3;
-	}
-
-	return make_float4(r, g, b, alpha);
-}
+/*
+ * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+ 
+#include "tensorNet.h"
+#include "randInt8Calibrator.h"
+#include "cudaMappedMemory.h"
+#include "cudaResize.h"
+#include "filesystem.h"
+
+#include "NvCaffeParser.h"
+
+#if NV_TENSORRT_MAJOR >= 5
+#include "NvOnnxParser.h"
+#include "NvUffParser.h"
+#include "NvInferPlugin.h"
+#endif
+
+#include <iostream>
+#include <fstream>
+#include <map>
+
+
+#if NV_TENSORRT_MAJOR > 1
+	#define CREATE_INFER_BUILDER nvinfer1::createInferBuilder
+	#define CREATE_INFER_RUNTIME nvinfer1::createInferRuntime
+#else
+	#define CREATE_INFER_BUILDER createInferBuilder
+	#define CREATE_INFER_RUNTIME createInferRuntime
+#endif
+
+#define LOG_DOWNLOADER_TOOL "        if loading a built-in model, maybe it wasn't downloaded before.\n\n"    \
+					   "        Run the Model Downloader tool again and select it for download:\n\n"   \
+					   "           $ cd <jetson-inference>/tools\n" 	  	\
+					   "           $ ./download-models.sh\n"
+
+#define USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
+#define CHECKSUM_TYPE "sha256sum"
+
+//---------------------------------------------------------------------
+const char* precisionTypeToStr( precisionType type )
+{
+	switch(type)
+	{
+		case TYPE_DISABLED:	return "DISABLED";
+		case TYPE_FASTEST:	return "FASTEST";
+		case TYPE_FP32:	return "FP32";
+		case TYPE_FP16:	return "FP16";
+		case TYPE_INT8:	return "INT8";
+	}
+    return nullptr;
+}
+
+precisionType precisionTypeFromStr( const char* str )
+{
+	if( !str )
+		return TYPE_DISABLED;
+
+	for( int n=0; n < NUM_PRECISIONS; n++ )
+	{
+		if( strcasecmp(str, precisionTypeToStr((precisionType)n)) == 0 )
+			return (precisionType)n;
+	}
+
+	return TYPE_DISABLED;
+}
+
+static inline nvinfer1::DataType precisionTypeToTRT( precisionType type )
+{
+	switch(type)
+	{
+		case TYPE_FP16:	return nvinfer1::DataType::kHALF;
+#if NV_TENSORRT_MAJOR >= 4
+		case TYPE_INT8:	return nvinfer1::DataType::kINT8;
+#endif
+	}
+
+	return nvinfer1::DataType::kFLOAT;
+}
+
+#if NV_TENSORRT_MAJOR >= 8
+static inline bool isFp16Enabled( nvinfer1::IBuilderConfig* config )
+{
+	return config->getFlag(nvinfer1::BuilderFlag::kFP16);
+}
+
+static inline bool isInt8Enabled( nvinfer1::IBuilderConfig* config )
+{
+	return config->getFlag(nvinfer1::BuilderFlag::kINT8);
+}
+#else // NV_TENSORRT_MAJOR <= 7
+static inline bool isFp16Enabled( nvinfer1::IBuilder* builder )
+{
+#if NV_TENSORRT_MAJOR < 4
+	return builder->getHalf2Mode();
+#else
+	return builder->getFp16Mode();
+#endif
+}
+
+static inline bool isInt8Enabled( nvinfer1::IBuilder* builder )
+{
+#if NV_TENSORRT_MAJOR >= 4
+	return builder->getInt8Mode();
+#else
+	return false;
+#endif
+}
+#endif
+
+#if NV_TENSORRT_MAJOR >= 4
+static inline const char* dataTypeToStr( nvinfer1::DataType type )
+{
+	switch(type)
+	{
+		case nvinfer1::DataType::kFLOAT:	return "FP32";
+		case nvinfer1::DataType::kHALF:	return "FP16";
+		case nvinfer1::DataType::kINT8:	return "INT8";
+		case nvinfer1::DataType::kINT32:	return "INT32";
+	}
+
+	LogWarning(LOG_TRT "warning -- unknown nvinfer1::DataType (%i)\n", (int)type);
+	return "UNKNOWN";
+}
+
+#if NV_TENSORRT_MAJOR <= 7
+static inline const char* dimensionTypeToStr( nvinfer1::DimensionType type )
+{
+	switch(type)
+	{
+		case nvinfer1::DimensionType::kSPATIAL:	 return "SPATIAL";
+		case nvinfer1::DimensionType::kCHANNEL:	 return "CHANNEL";
+		case nvinfer1::DimensionType::kINDEX:	 return "INDEX";
+		case nvinfer1::DimensionType::kSEQUENCE: return "SEQUENCE";
+	}
+
+	LogWarning(LOG_TRT "warning -- unknown nvinfer1::DimensionType (%i)\n", (int)type);
+	return "UNKNOWN";
+}
+#endif
+#endif
+
+#if NV_TENSORRT_MAJOR > 1 
+static inline nvinfer1::Dims validateDims( const nvinfer1::Dims& dims )
+{
+	if( dims.nbDims == nvinfer1::Dims::MAX_DIMS )
+		return dims;
+	
+	nvinfer1::Dims dims_out = dims;
+
+	// TRT doesn't set the higher dims, so make sure they are 1
+	for( int n=dims_out.nbDims; n < nvinfer1::Dims::MAX_DIMS; n++ )
+		dims_out.d[n] = 1;
+
+	return dims_out;
+}
+
+static inline void copyDims( Dims3* dest, const nvinfer1::Dims* src )
+{
+	for( int n=0; n < src->nbDims; n++ )
+		dest->d[n] = src->d[n];
+	
+	dest->nbDims = src->nbDims;
+}
+
+static inline size_t sizeDims( const nvinfer1::Dims& dims, const size_t elementSize=1 )
+{
+	size_t sz = dims.d[0];
+	
+	for ( int n=1; n < dims.nbDims; n++ )
+		sz *= dims.d[n];
+
+	return sz * elementSize;
+}
+#else
+static inline nvinfer1::Dims3 validateDims( const nvinfer1::Dims3& dims )
+{
+	nvinfer1::Dims3 out = dims;
+	
+	if( DIMS_C(out) == 0 )
+		DIMS_C(out) = 1;
+	
+	if( DIMS_H(out) == 0 )
+		DIMS_H(out) = 1;
+	
+	if( DIMS_W(out) == 0 )
+		DIMS_W(out) = 1;
+	
+	return out;
+}
+
+static inline void copyDims( Dims3* dest, const nvinfer1::Dims3* src )
+{
+	memcpy(dest, src, sizeof(nvinfer1::Dims));
+}
+
+static inline size_t sizeDims( const Dims3& dims, const size_t elementSize=1 )
+{
+	return DIMS_C(dims) * DIMS_H(dims) * DIMS_W(dims) * elementSize;
+}
+#endif	
+
+#if NV_TENSORRT_MAJOR >= 7
+static inline nvinfer1::Dims shiftDims( const nvinfer1::Dims& dims )
+{
+	// TensorRT 7.0 requires EXPLICIT_BATCH flag for ONNX models,
+	// which adds a batch dimension (4D NCHW), whereas historically
+	// 3D CHW was expected.  Remove the batch dim (it is typically 1)
+	nvinfer1::Dims out = dims;
+	
+	/*out.d[0] = dims.d[1];
+	out.d[1] = dims.d[2];
+	out.d[2] = dims.d[3];
+	out.d[3] = 1;*/
+	
+	if( dims.nbDims == 1 )
+		return out;
+	
+	for( int n=0; n < dims.nbDims; n++ )
+		out.d[n] = dims.d[n+1];
+	
+	for( int n=dims.nbDims; n < nvinfer1::Dims::MAX_DIMS; n++ )
+		out.d[n] = 1;
+	
+	out.nbDims -= 1;
+	return out;
+}
+#endif
+
+const char* deviceTypeToStr( deviceType type )
+{
+	switch(type)
+	{
+		case DEVICE_GPU:	return "GPU";	
+		case DEVICE_DLA_0:	return "DLA_0";
+		case DEVICE_DLA_1:	return "DLA_1";
+	}
+    return nullptr;
+}
+
+deviceType deviceTypeFromStr( const char* str )
+{
+	if( !str )
+		return DEVICE_GPU;
+
+ 	for( int n=0; n < NUM_DEVICES; n++ )
+	{
+		if( strcasecmp(str, deviceTypeToStr((deviceType)n)) == 0 )
+			return (deviceType)n;
+	}
+
+	if( strcasecmp(str, "DLA") == 0 )
+		return DEVICE_DLA;
+
+	return DEVICE_GPU;
+}
+
+#if NV_TENSORRT_MAJOR >= 5
+static inline nvinfer1::DeviceType deviceTypeToTRT( deviceType type )
+{
+	switch(type)
+	{
+		case DEVICE_GPU:	return nvinfer1::DeviceType::kGPU;
+		//case DEVICE_DLA:	return nvinfer1::DeviceType::kDLA;
+#if NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0
+		case DEVICE_DLA_0:	return nvinfer1::DeviceType::kDLA0;
+		case DEVICE_DLA_1:	return nvinfer1::DeviceType::kDLA1;
+#else
+		case DEVICE_DLA_0:	return nvinfer1::DeviceType::kDLA;
+		case DEVICE_DLA_1:	return nvinfer1::DeviceType::kDLA;
+#endif
+        default:            return nvinfer1::DeviceType::kGPU;
+	}
+}
+#endif
+
+const char* modelTypeToStr( modelType format )
+{
+	switch(format)
+	{
+		case MODEL_CUSTOM:	return "custom";	
+		case MODEL_CAFFE:	return "caffe";
+		case MODEL_ONNX:	return "ONNX";
+		case MODEL_UFF:	return "UFF";
+		case MODEL_ENGINE:	return "engine";
+	}
+    return nullptr;
+}
+
+modelType modelTypeFromStr( const char* str )
+{
+	if( !str )
+		return MODEL_CUSTOM;
+
+	if( strcasecmp(str, "caffemodel") == 0 || strcasecmp(str, "caffe") == 0 )
+		return MODEL_CAFFE;
+	else if( strcasecmp(str, "onnx") == 0 )
+		return MODEL_ONNX;
+	else if( strcasecmp(str, "uff") == 0 )
+		return MODEL_UFF;
+	else if( strcasecmp(str, "engine") == 0 || strcasecmp(str, "plan") == 0 || strcasecmp(str, "trt") == 0 )
+		return MODEL_ENGINE;
+
+	return MODEL_CUSTOM;
+}
+
+modelType modelTypeFromPath( const char* path )
+{
+	if( !path )
+		return MODEL_CUSTOM;
+	
+	return modelTypeFromStr(fileExtension(path).c_str());
+}
+
+const char* profilerQueryToStr( profilerQuery query )
+{
+	switch(query)
+	{
+		case PROFILER_PREPROCESS:  return "Pre-Process";
+		case PROFILER_NETWORK:	  return "Network";
+		case PROFILER_POSTPROCESS: return "Post-Process";
+		case PROFILER_VISUALIZE:	  return "Visualize";
+		case PROFILER_TOTAL:	  return "Total";
+	}
+    return nullptr;
+}
+
+ONNXKind ONNXKindFromStr( const char* str )
+{
+	if( !str )
+		return ONNX_SSD;
+
+	if( strcasestr(str, "yolo") )
+		return ONNX_YOLO;
+	return ONNX_SSD;
+}
+
+
+//---------------------------------------------------------------------
+tensorNet::Logger tensorNet::gLogger;
+
+// constructor
+tensorNet::tensorNet()
+{
+	mEngine   = NULL;
+	mInfer    = NULL;
+	mContext  = NULL;
+	mStream   = NULL;
+	mBindings	= NULL;
+
+	mMaxBatchSize   = 0;	
+	mEnableDebug    = false;
+	mEnableProfiler = false;
+
+	mModelType        = MODEL_CUSTOM;
+	mPrecision 	   = TYPE_FASTEST;
+	mDevice    	   = DEVICE_GPU;
+	mAllowGPUFallback = false;
+
+	mProfilerQueriesUsed = 0;
+	mProfilerQueriesDone = 0;
+
+	memset(mEventsCPU, 0, sizeof(mEventsCPU));
+	memset(mEventsGPU, 0, sizeof(mEventsGPU));
+	memset(mProfilerTimes, 0, sizeof(mProfilerTimes));
+
+#if NV_TENSORRT_MAJOR > 5
+	mWorkspaceSize = 32 << 20;
+#else
+	mWorkspaceSize = 16 << 20;
+#endif
+}
+
+
+// Destructor
+tensorNet::~tensorNet()
+{
+	if( mContext != NULL )
+	{
+		mContext->destroy();
+		mContext = NULL;
+	}
+	
+	if( mEngine != NULL )
+	{
+		mEngine->destroy();
+		mEngine = NULL;
+	}
+		
+	if( mInfer != NULL )
+	{
+		mInfer->destroy();
+		mInfer = NULL;
+	}
+	
+	for( size_t n=0; n < mInputs.size(); n++ )
+	{
+	#ifdef USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
+		CUDA_FREE(mInputs[n].CUDA);
+	#else
+		CUDA_FREE_HOST(mInputs[n].CUDA);
+	#endif
+	}
+	
+	for( size_t n=0; n < mOutputs.size(); n++ )
+		CUDA_FREE_HOST(mOutputs[n].CPU);
+	
+	free(mBindings);
+}
+
+
+// EnableProfiler
+void tensorNet::EnableLayerProfiler()
+{
+	mEnableProfiler = true;
+
+	if( mContext != NULL )
+		mContext->setProfiler(&gProfiler);
+}
+
+
+// EnableDebug
+void tensorNet::EnableDebug()
+{
+	mEnableDebug = true;
+}
+
+
+// DetectNativePrecisions()
+std::vector<precisionType> tensorNet::DetectNativePrecisions( deviceType device )
+{
+	std::vector<precisionType> types;
+	Logger logger;
+
+	// create a temporary builder for querying the supported types
+	nvinfer1::IBuilder* builder = CREATE_INFER_BUILDER(logger);
+		
+	if( !builder )
+	{
+		LogError(LOG_TRT "DetectNativePrecisions() failed to create TensorRT IBuilder instance\n");
+		return types;
+	}
+
+#if NV_TENSORRT_MAJOR >= 5 && NV_TENSORRT_MAJOR <= 7
+	if( device == DEVICE_DLA_0 || device == DEVICE_DLA_1 )
+		builder->setFp16Mode(true);
+
+	builder->setDefaultDeviceType( deviceTypeToTRT(device) );
+#endif
+
+	// FP32 is supported on all platforms
+	types.push_back(TYPE_FP32);
+
+	// detect fast (native) FP16
+	if( builder->platformHasFastFp16() )
+		types.push_back(TYPE_FP16);
+
+#if NV_TENSORRT_MAJOR >= 4
+	// detect fast (native) INT8
+	if( builder->platformHasFastInt8() )
+		types.push_back(TYPE_INT8);
+#endif
+
+	// print out supported precisions (optional)
+	const uint32_t numTypes = types.size();
+
+	LogVerbose(LOG_TRT "native precisions detected for %s:  ", deviceTypeToStr(device));
+ 
+	for( uint32_t n=0; n < numTypes; n++ )
+	{
+		LogVerbose("%s", precisionTypeToStr(types[n]));
+
+		if( n < numTypes - 1 )
+			LogVerbose(", ");
+	}
+
+	LogVerbose("\n");
+	builder->destroy();
+	return types;
+}
+
+
+// DetectNativePrecision
+bool tensorNet::DetectNativePrecision( const std::vector<precisionType>& types, precisionType type )
+{
+	const uint32_t numTypes = types.size();
+
+	for( uint32_t n=0; n < numTypes; n++ )
+	{
+		if( types[n] == type )
+			return true;
+	}
+
+	return false;
+}
+
+
+// DetectNativePrecision
+bool tensorNet::DetectNativePrecision( precisionType precision, deviceType device )
+{
+	std::vector<precisionType> types = DetectNativePrecisions(device);
+	return DetectNativePrecision(types, precision);
+}
+
+
+// SelectPrecision
+precisionType tensorNet::SelectPrecision( precisionType precision, deviceType device, bool allowInt8 )
+{
+	LogVerbose(LOG_TRT "desired precision specified for %s: %s\n", deviceTypeToStr(device), precisionTypeToStr(precision));
+
+	if( precision == TYPE_DISABLED )
+	{
+		LogWarning(LOG_TRT "skipping network specified with precision TYPE_DISABLE\n");
+		LogWarning(LOG_TRT "please specify a valid precision to create the network\n");
+	}
+	else if( precision == TYPE_FASTEST )
+	{
+		if( !allowInt8 )
+			LogWarning(LOG_TRT "requested fasted precision for device %s without providing valid calibrator, disabling INT8\n", deviceTypeToStr(device));
+
+		precision = FindFastestPrecision(device, allowInt8);
+		LogVerbose(LOG_TRT "selecting fastest native precision for %s:  %s\n", deviceTypeToStr(device), precisionTypeToStr(precision));
+	}
+	else
+	{
+		if( !DetectNativePrecision(precision, device) )
+		{
+			LogWarning(LOG_TRT "precision %s is not supported for device %s\n", precisionTypeToStr(precision), deviceTypeToStr(device));
+			precision = FindFastestPrecision(device, allowInt8);
+			LogWarning(LOG_TRT "falling back to fastest precision for device %s (%s)\n", deviceTypeToStr(device), precisionTypeToStr(precision));
+		}
+
+		if( precision == TYPE_INT8 && !allowInt8 )
+			LogWarning(LOG_TRT "warning:  device %s using INT8 precision with RANDOM calibration\n", deviceTypeToStr(device));
+	}
+
+	return precision;
+}
+
+
+// FindFastestPrecision
+precisionType tensorNet::FindFastestPrecision( deviceType device, bool allowInt8 )
+{
+	std::vector<precisionType> types = DetectNativePrecisions(device);
+
+	if( allowInt8 && DetectNativePrecision(types, TYPE_INT8) )
+		return TYPE_INT8;
+	else if( DetectNativePrecision(types, TYPE_FP16) )
+		return TYPE_FP16;
+	else
+		return TYPE_FP32;
+}
+
+
+// Create an optimized GIE network from caffe prototxt and model file
+bool tensorNet::ProfileModel(const std::string& deployFile,			   // name for caffe prototxt
+					    const std::string& modelFile,			   // name for model 
+					    const std::vector<std::string>& inputs, 
+					    const std::vector<Dims3>& inputDims,
+					    const std::vector<std::string>& outputs,    // network outputs
+					    unsigned int maxBatchSize,			   // batch size - NB must be at least as large as the batch we want to run with
+					    precisionType precision, 
+					    deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, 	
+					    char** engineStream, size_t* engineSize)	   // output stream for the GIE model
+{
+	if( !engineStream || !engineSize )
+		return false;
+
+	// create builder and network definition interfaces
+	nvinfer1::IBuilder* builder = CREATE_INFER_BUILDER(gLogger);
+	
+#if NV_TENSORRT_MAJOR >= 8
+	nvinfer1::INetworkDefinition* network = builder->createNetworkV2(0);
+#else
+	nvinfer1::INetworkDefinition* network = builder->createNetwork();
+#endif
+
+	LogInfo(LOG_TRT "device %s, loading %s %s\n", deviceTypeToStr(device), deployFile.c_str(), modelFile.c_str());
+	
+	// parse the different types of model formats
+	if( mModelType == MODEL_CAFFE )
+	{
+		// parse the caffe model to populate the network, then set the outputs
+		nvcaffeparser1::ICaffeParser* parser = nvcaffeparser1::createCaffeParser();
+
+		nvinfer1::DataType modelDataType = (precision == TYPE_FP16) ? nvinfer1::DataType::kHALF : nvinfer1::DataType::kFLOAT; // import INT8 weights as FP32
+		const nvcaffeparser1::IBlobNameToTensor* blobNameToTensor =
+			parser->parse(deployFile.c_str(),		// caffe deploy file
+						  modelFile.c_str(),	// caffe model file
+						 *network,			// network definition that the parser will populate
+						  modelDataType);
+
+		if( !blobNameToTensor )
+		{
+			LogError(LOG_TRT "device %s, failed to parse caffe network\n", deviceTypeToStr(device));
+			return false;
+		}
+
+		// the caffe file has no notion of outputs, so we need to manually say which tensors the engine should generate	
+		const size_t num_outputs = outputs.size();
+		
+		for( size_t n=0; n < num_outputs; n++ )
+		{
+			nvinfer1::ITensor* tensor = blobNameToTensor->find(outputs[n].c_str());
+		
+			if( !tensor )
+				LogError(LOG_TRT "failed to retrieve tensor for Output \"%s\"\n", outputs[n].c_str());
+			else
+			{
+			#if NV_TENSORRT_MAJOR >= 4
+				nvinfer1::Dims3 dims = static_cast<nvinfer1::Dims3&&>(tensor->getDimensions());
+				LogVerbose(LOG_TRT "retrieved Output tensor \"%s\":  %ix%ix%i\n", tensor->getName(), dims.d[0], dims.d[1], dims.d[2]);
+			#endif
+			}
+
+			network->markOutput(*tensor);
+		}
+
+		//parser->destroy();
+	}
+#if NV_TENSORRT_MAJOR >= 5
+	else if( mModelType == MODEL_ONNX )
+	{
+	#if NV_TENSORRT_MAJOR >= 7
+		network->destroy();
+		network = builder->createNetworkV2(1U << (uint32_t)nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
+
+		if( !network )
+		{
+		  LogError(LOG_TRT "IBuilder::createNetworkV2(EXPLICIT_BATCH) failed\n");
+		  return false;
+		}
+	#endif
+
+		nvonnxparser::IParser* parser = nvonnxparser::createParser(*network, gLogger);
+
+		if( !parser )
+		{
+			LogError(LOG_TRT "failed to create nvonnxparser::IParser instance\n");
+			return false;
+		}
+
+    #if NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0
+        const int parserLogLevel = (int)nvinfer1::ILogger::Severity::kINFO;
+    #else
+        const int parserLogLevel = (int)nvinfer1::ILogger::Severity::kVERBOSE;
+    #endif
+
+		if( !parser->parseFromFile(modelFile.c_str(), parserLogLevel) )
+		{
+			LogError(LOG_TRT "failed to parse ONNX model '%s'\n", modelFile.c_str());
+			return false;
+		}
+
+		//parser->destroy();
+	}
+	else if( mModelType == MODEL_UFF )
+	{
+		// create parser instance
+		nvuffparser::IUffParser* parser = nvuffparser::createUffParser();
+		
+		if( !parser )
+		{
+			LogError(LOG_TRT "failed to create UFF parser\n");
+			return false;
+		}
+		
+		// register inputs
+		const size_t numInputs = inputs.size();
+
+		for( size_t n=0; n < numInputs; n++ )
+		{
+			if( !parser->registerInput(inputs[n].c_str(), inputDims[n], nvuffparser::UffInputOrder::kNCHW) )
+			{
+				LogError(LOG_TRT "failed to register input '%s' for UFF model '%s'\n", inputs[n].c_str(), modelFile.c_str());
+				return false;
+			}
+		}
+
+		// register outputs
+		/*const size_t numOutputs = outputs.size();
+		
+		for( uint32_t n=0; n < numOutputs; n++ )
+		{
+			if( !parser->registerOutput(outputs[n].c_str()) )
+				printf(LOG_TRT "failed to register output '%s' for UFF model '%s'\n", outputs[n].c_str(), modelFile.c_str());
+		}*/
+
+		// UFF outputs are forwarded to 'MarkOutput_0'
+		if( !parser->registerOutput("MarkOutput_0") )
+			LogError(LOG_TRT "failed to register output '%s' for UFF model '%s'\n", "MarkOutput_0", modelFile.c_str());
+
+		// parse network
+		if( !parser->parse(modelFile.c_str(), *network, nvinfer1::DataType::kFLOAT) )
+		{
+			LogError(LOG_TRT "failed to parse UFF model '%s'\n", modelFile.c_str());
+			return false;
+		}
+		
+		//parser->destroy();
+	}
+#endif
+
+#if NV_TENSORRT_MAJOR >= 4
+	if( precision == TYPE_INT8 && !calibrator )
+	{
+		// extract the dimensions of the network input blobs
+		std::map<std::string, nvinfer1::Dims3> inputDimensions;
+
+		for( int i=0, n=network->getNbInputs(); i < n; i++ )
+		{
+			nvinfer1::Dims dims = network->getInput(i)->getDimensions();
+
+		#if NV_TENSORRT_MAJOR >= 7
+			if( mModelType == MODEL_ONNX )
+				dims = shiftDims(dims);  // change NCHW to CHW for EXPLICIT_BATCH
+		#endif
+
+			//nvinfer1::Dims3 dims = static_cast<nvinfer1::Dims3&&>(network->getInput(i)->getDimensions());
+			inputDimensions.insert(std::make_pair(network->getInput(i)->getName(), static_cast<nvinfer1::Dims3&&>(dims)));
+			LogVerbose(LOG_TRT "retrieved Input tensor '%s':  %ix%ix%i\n", network->getInput(i)->getName(), dims.d[0], dims.d[1], dims.d[2]);
+		}
+
+		// default to random calibration
+		calibrator = new randInt8Calibrator(1, mCacheCalibrationPath, inputDimensions);
+		LogWarning(LOG_TRT "warning:  device %s using INT8 precision with RANDOM calibration\n", deviceTypeToStr(device));
+	}
+#endif
+
+	// configure the builder
+#if NV_TENSORRT_MAJOR >= 8
+	nvinfer1::IBuilderConfig* builderConfig = builder->createBuilderConfig();
+	
+	if( !ConfigureBuilder(builder, builderConfig, maxBatchSize, mWorkspaceSize, 
+					  precision, device, allowGPUFallback, calibrator) )
+	{
+		LogError(LOG_TRT "device %s, failed to configure builder\n", deviceTypeToStr(device));
+		return false;
+	}
+	
+	// attempt to load the timing cache
+	const nvinfer1::ITimingCache* timingCache = NULL;
+	
+	char timingCachePath[PATH_MAX];
+	sprintf(timingCachePath, "/usr/local/bin/networks/tensorrt.%i.timingcache", NV_TENSORRT_VERSION);
+
+	if( fileExists(timingCachePath) )
+	{
+		LogInfo(LOG_TRT "loading timing cache from %s\n", timingCachePath);
+		
+		void* timingCacheBuffer = NULL;
+		const size_t timingCacheSize = loadFile(timingCachePath, &timingCacheBuffer);
+		
+		if( timingCacheSize > 0 )
+		{
+			timingCache = builderConfig->createTimingCache(timingCacheBuffer, timingCacheSize);
+			free(timingCacheBuffer);
+		}
+	}
+	
+	if( !timingCache )
+	{
+		timingCache = builderConfig->createTimingCache(NULL, 0);  // create a new cache
+	
+		if( !timingCache )
+			LogWarning(LOG_TRT "couldn't create new timing cache\n");
+	}
+	
+	if( timingCache != NULL && !builderConfig->setTimingCache(*timingCache, false) )
+		LogWarning(LOG_TRT "failed to activate timing cache");
+#else
+	if( !ConfigureBuilder(builder, maxBatchSize, mWorkspaceSize, precision, device, allowGPUFallback, calibrator) )
+	{
+		LogError(LOG_TRT "device %s, failed to configure builder\n", deviceTypeToStr(device));
+		return false;
+	}
+#endif
+
+	// build CUDA engine
+	LogInfo(LOG_TRT "device %s, building CUDA engine (this may take a few minutes the first time a network is loaded)\n", deviceTypeToStr(device));
+
+	if( Log::GetLevel() < Log::VERBOSE )
+		LogInfo(LOG_TRT "info: to see status updates during engine building, enable verbose logging with --verbose\n");
+
+#if NV_TENSORRT_MAJOR >= 8
+	nvinfer1::ICudaEngine* engine = builder->buildEngineWithConfig(*network, *builderConfig);
+#else
+	nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);
+#endif
+
+	if( !engine )
+	{
+		LogError(LOG_TRT "device %s, failed to build CUDA engine\n", deviceTypeToStr(device));
+		return false;
+	}
+
+	LogSuccess(LOG_TRT "device %s, completed building CUDA engine\n", deviceTypeToStr(device));
+
+#if NV_TENSORRT_MAJOR >= 8
+	if( timingCache != NULL )
+	{
+		// save the updated timing cache
+		nvinfer1::IHostMemory* timingCacheMem = timingCache->serialize();
+		
+		if( timingCacheMem != NULL )
+		{
+			const char* timingCacheBuffer = (char*)timingCacheMem->data();
+			const size_t timingCacheSize = timingCacheMem->size();
+		
+			LogVerbose(LOG_TRT "saving timing cache to %s (%zu bytes)\n", timingCachePath, timingCacheSize);
+			
+			// write the cache file
+			FILE* timingCacheFile = NULL;
+			timingCacheFile = fopen(timingCachePath, "wb");
+
+			if( timingCacheFile != NULL )
+			{
+				if( fwrite(timingCacheBuffer,	1, timingCacheSize, timingCacheFile) != timingCacheSize )
+					LogWarning(LOG_TRT "failed to write %zu bytes to timing cache file %s\n", timingCacheSize, timingCachePath);
+			
+				fclose(timingCacheFile);
+			}
+			else
+			{
+				LogWarning(LOG_TRT "failed to open timing cache file for writing %s\n", timingCachePath);
+			}
+			
+			timingCacheMem->destroy();
+		}
+	
+		delete timingCache;
+	}
+	
+	builderConfig->destroy();
+#endif
+	
+	// we don't need the network definition any more, and we can destroy the parser
+	network->destroy();
+	//parser->destroy();
+	
+#if NV_TENSORRT_MAJOR >= 2
+	// serialize the engine
+	nvinfer1::IHostMemory* serMem = engine->serialize();
+
+	if( !serMem )
+	{
+		LogError(LOG_TRT "device %s, failed to serialize CUDA engine\n", deviceTypeToStr(device));
+		return false;
+	}
+
+	const char* serData = (char*)serMem->data();
+	const size_t serSize = serMem->size();
+
+	// allocate memory to store the bitstream
+	char* engineMemory = (char*)malloc(serSize);
+
+	if( !engineMemory )
+	{
+		LogError(LOG_TRT "failed to allocate %zu bytes to store CUDA engine\n", serSize);
+		return false;
+	}
+
+	memcpy(engineMemory, serData, serSize);
+	
+	*engineStream = engineMemory;
+	*engineSize = serSize;
+	
+	serMem->destroy();
+#else
+	engine->serialize(modelStream);
+#endif
+
+	// free builder resources
+	engine->destroy();
+	builder->destroy();
+	
+	return true;
+}
+
+
+// ConfigureBuilder
+#if NV_TENSORRT_MAJOR >= 8
+bool tensorNet::ConfigureBuilder( nvinfer1::IBuilder* builder, nvinfer1::IBuilderConfig* config,  
+				    		    uint32_t maxBatchSize, uint32_t workspaceSize, precisionType precision, 
+				    		    deviceType device, bool allowGPUFallback, 
+				    		    nvinfer1::IInt8Calibrator* calibrator )
+{
+	if( !builder )
+		return false;
+
+	LogVerbose(LOG_TRT "device %s, configuring network builder\n", deviceTypeToStr(device));
+		
+	builder->setMaxBatchSize(maxBatchSize);
+	config->setMaxWorkspaceSize(workspaceSize);
+
+	config->setMinTimingIterations(3); // allow time for GPU to spin up
+	config->setAvgTimingIterations(2);
+
+	if( mEnableDebug )
+		config->setFlag(nvinfer1::BuilderFlag::kDEBUG);
+	
+	// set up the builder for the desired precision
+	if( precision == TYPE_INT8 )
+	{
+		config->setFlag(nvinfer1::BuilderFlag::kINT8);
+		//config->setFlag(nvinfer1::BuilderFlag::kFP16); // TODO:  experiment for benefits of both INT8/FP16
+		
+		if( !calibrator )
+		{
+			LogError(LOG_TRT "device %s, INT8 requested but calibrator is NULL\n", deviceTypeToStr(device));
+			return false;
+		}
+
+		config->setInt8Calibrator(calibrator);
+	}
+	else if( precision == TYPE_FP16 )
+	{
+		config->setFlag(nvinfer1::BuilderFlag::kFP16);
+	}
+	
+	// set the default device type
+	config->setDefaultDeviceType(deviceTypeToTRT(device));
+
+	if( allowGPUFallback )
+		config->setFlag(nvinfer1::BuilderFlag::kGPU_FALLBACK);
+
+	LogInfo(LOG_TRT "device %s, building FP16:  %s\n", deviceTypeToStr(device), isFp16Enabled(config) ? "ON" : "OFF"); 
+	LogInfo(LOG_TRT "device %s, building INT8:  %s\n", deviceTypeToStr(device), isInt8Enabled(config) ? "ON" : "OFF"); 
+	LogInfo(LOG_TRT "device %s, workspace size: %u\n", deviceTypeToStr(device), workspaceSize);
+
+	return true;
+}
+
+#else  // NV_TENSORRT_MAJOR <= 7
+	
+bool tensorNet::ConfigureBuilder( nvinfer1::IBuilder* builder, uint32_t maxBatchSize, 
+				    		    uint32_t workspaceSize, precisionType precision, 
+				    		    deviceType device, bool allowGPUFallback, 
+				    		    nvinfer1::IInt8Calibrator* calibrator )
+{
+	if( !builder )
+		return false;
+
+	LogVerbose(LOG_TRT "device %s, configuring network builder\n", deviceTypeToStr(device));
+		
+	builder->setMaxBatchSize(maxBatchSize);
+	builder->setMaxWorkspaceSize(workspaceSize);
+
+	builder->setDebugSync(mEnableDebug);
+	builder->setMinFindIterations(3);	// allow time for GPU to spin up
+	builder->setAverageFindIterations(2);
+
+	// set up the builder for the desired precision
+	if( precision == TYPE_INT8 )
+	{
+	#if NV_TENSORRT_MAJOR >= 4
+		builder->setInt8Mode(true);
+		//builder->setFp16Mode(true);		// TODO:  experiment for benefits of both INT8/FP16
+		
+		if( !calibrator )
+		{
+			LogError(LOG_TRT "device %s, INT8 requested but calibrator is NULL\n", deviceTypeToStr(device));
+			return false;
+		}
+
+		builder->setInt8Calibrator(calibrator);
+	#else
+		LogError(LOG_TRT "INT8 precision requested, and TensorRT %u.%u doesn't meet minimum version for INT8\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
+		LogError(LOG_TRT "please use minumum version of TensorRT 4.0 or newer for INT8 support\n");
+
+		return false;
+	#endif
+	}
+	else if( precision == TYPE_FP16 )
+	{
+	#if NV_TENSORRT_MAJOR >= 4
+		builder->setFp16Mode(true);
+	#else
+		builder->setHalf2Mode(true);
+	#endif
+	}
+	
+	// set the default device type
+#if NV_TENSORRT_MAJOR >= 5
+	builder->setDefaultDeviceType(deviceTypeToTRT(device));
+
+	if( allowGPUFallback )
+		builder->allowGPUFallback(true);
+	
+#if !(NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0)
+	if( device == DEVICE_DLA_0 )
+		builder->setDLACore(0);
+	else if( device == DEVICE_DLA_1 )
+		builder->setDLACore(1);
+#endif
+#else
+	if( device != DEVICE_GPU )
+	{
+		LogError(LOG_TRT "device %s is not supported in TensorRT %u.%u\n", deviceTypeToStr(device), NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
+		return false;
+	}
+#endif
+
+	LogInfo(LOG_TRT "device %s, building FP16:  %s\n", deviceTypeToStr(device), isFp16Enabled(builder) ? "ON" : "OFF"); 
+	LogInfo(LOG_TRT "device %s, building INT8:  %s\n", deviceTypeToStr(device), isInt8Enabled(builder) ? "ON" : "OFF"); 
+	LogInfo(LOG_TRT "device %s, workspace size: %u\n", deviceTypeToStr(device), workspaceSize);
+
+	return true;
+}
+#endif
+
+
+// LoadNetwork
+bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
+					    const char* input_blob, const char* output_blob, uint32_t maxBatchSize,
+					    precisionType precision, deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
+{
+	std::vector<std::string> outputs;
+	outputs.push_back(output_blob);
+	
+	return LoadNetwork(prototxt_path, model_path, mean_path, input_blob, outputs, maxBatchSize, precision, device, allowGPUFallback );
+}
+
+
+// LoadNetwork
+bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
+					    const char* input_blob, const std::vector<std::string>& output_blobs, 
+					    uint32_t maxBatchSize, precisionType precision,
+				   	    deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
+{
+	return LoadNetwork(prototxt_path, model_path, mean_path,
+				    input_blob, Dims3(1,1,1), output_blobs,
+				    maxBatchSize, precision, device,
+				    allowGPUFallback, calibrator, stream);
+}
+
+
+// LoadNetwork
+bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
+					    const std::vector<std::string>& input_blobs, 
+					    const std::vector<std::string>& output_blobs, 
+					    uint32_t maxBatchSize, precisionType precision,
+				   	    deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
+{
+	std::vector<Dims3> input_dims;
+
+	for( size_t n=0; n < input_blobs.size(); n++ )
+		input_dims.push_back(Dims3(1,1,1));
+
+	return LoadNetwork(prototxt_path, model_path, mean_path,
+				    input_blobs, input_dims, output_blobs,
+				    maxBatchSize, precision, device,
+				    allowGPUFallback, calibrator, stream);
+}
+
+
+// LoadNetwork
+bool tensorNet::LoadNetwork( const char* prototxt_path, const char* model_path, const char* mean_path, 
+					    const char* input_blob, const Dims3& input_dim,
+					    const std::vector<std::string>& output_blobs, 
+					    uint32_t maxBatchSize, precisionType precision,
+				   	    deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
+{
+	std::vector<std::string> inputs;
+	std::vector<Dims3> input_dims;
+
+	inputs.push_back(input_blob);
+	input_dims.push_back(input_dim);
+
+	return LoadNetwork(prototxt_path, model_path, mean_path,
+				    inputs, input_dims, output_blobs,
+				    maxBatchSize, precision, device,
+				    allowGPUFallback, calibrator, stream);
+}
+
+		   
+// LoadNetwork
+bool tensorNet::LoadNetwork( const char* prototxt_path_, const char* model_path_, const char* mean_path, 
+					    const std::vector<std::string>& input_blobs, 
+					    const std::vector<Dims3>& input_dims,
+					    const std::vector<std::string>& output_blobs, 
+					    uint32_t maxBatchSize, precisionType precision,
+				   	    deviceType device, bool allowGPUFallback,
+					    nvinfer1::IInt8Calibrator* calibrator, cudaStream_t stream )
+{
+#if NV_TENSORRT_MAJOR >= 4
+	LogInfo(LOG_TRT "TensorRT version %u.%u.%u\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR, NV_TENSORRT_PATCH);
+#else
+	LogInfo(LOG_TRT "TensorRT version %u.%u\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
+#endif
+
+	/*
+	 * validate arguments
+	 */
+	if( !model_path_ )
+	{
+		LogError(LOG_TRT "model path was NULL - must have valid model path to LoadNetwork()\n");
+		return false;
+	}
+
+	if( input_blobs.size() == 0 || output_blobs.size() == 0 )
+	{
+		LogError(LOG_TRT "requested number of input layers or output layers was zero\n");
+		return false;
+	}
+
+	if( input_blobs.size() != input_dims.size() )
+	{
+		LogError(LOG_TRT "input mismatch - requested %zu input layers, but only %zu input dims\n", input_blobs.size(), input_dims.size());
+		return false;
+	}
+
+	
+	/*
+	 * load NV inference plugins
+	 */
+#if NV_TENSORRT_MAJOR > 4
+	static bool loadedPlugins = false;
+
+	if( !loadedPlugins )
+	{
+		LogVerbose(LOG_TRT "loading NVIDIA plugins...\n");
+
+		loadedPlugins = initLibNvInferPlugins(&gLogger, "");
+
+		if( !loadedPlugins )
+			LogError(LOG_TRT "failed to load NVIDIA plugins\n");
+		else
+			LogVerbose(LOG_TRT "completed loading NVIDIA plugins.\n");
+	}
+#endif
+
+	/*
+	 * verify the prototxt and model paths
+	 */
+	const std::string model_path    = locateFile(model_path_);
+	const std::string prototxt_path = locateFile(prototxt_path_ != NULL ? prototxt_path_ : "");
+	
+	const std::string model_ext = fileExtension(model_path_);
+	const modelType   model_fmt = modelTypeFromStr(model_ext.c_str());
+
+	if(model_fmt  == MODEL_ONNX) {
+		mONNXKind = ONNXKindFromStr(model_path.c_str());
+	}
+
+	LogVerbose(LOG_TRT "detected model format - %s  (extension '.%s')\n", modelTypeToStr(model_fmt), model_ext.c_str());
+
+	if( model_fmt == MODEL_CUSTOM )
+	{
+		LogError(LOG_TRT "model format '%s' not supported by jetson-inference\n", modelTypeToStr(model_fmt));
+		return false;
+	}
+#if NV_TENSORRT_MAJOR < 5
+	else if( model_fmt == MODEL_ONNX )
+	{
+		LogError(LOG_TRT "importing ONNX models is not supported in TensorRT %u.%u (version >= 5.0 required)\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
+		return false;
+	}
+	else if( model_fmt == MODEL_UFF )
+	{
+		LogError(LOG_TRT "importing UFF models is not supported in TensorRT %u.%u (version >= 5.0 required)\n", NV_TENSORRT_MAJOR, NV_TENSORRT_MINOR);
+		return false;
+	}
+#endif
+	else if( model_fmt == MODEL_CAFFE && !prototxt_path_ )
+	{
+		LogError(LOG_TRT "attempted to load caffe model without specifying prototxt file\n");
+		return false;
+	}
+	else if( model_fmt == MODEL_ENGINE )
+	{
+		if( !LoadEngine(model_path.c_str(), input_blobs, output_blobs, NULL, device, stream) )
+		{
+			LogError(LOG_TRT "failed to load %s\n", model_path.c_str());
+			return false;
+		}
+
+		mModelType = model_fmt;
+		mModelPath = model_path;
+		mModelFile = pathFilename(mModelPath);
+		
+		LogSuccess(LOG_TRT "device %s, initialized %s\n", deviceTypeToStr(device), mModelPath.c_str());	
+		return true;
+	}
+
+	mModelType = model_fmt;
+
+
+	/*
+	 * resolve the desired precision to a specific one that's available
+	 */
+	precision = SelectPrecision(precision, device, (calibrator != NULL));
+
+	if( precision == TYPE_DISABLED )
+		return false;
+
+	
+	/*
+	 * attempt to load network engine from cache before profiling with tensorRT
+	 */	
+	char* engineStream = NULL;
+	size_t engineSize = 0;
+
+	char cache_prefix[PATH_MAX];
+	char cache_path[PATH_MAX];
+
+	sprintf(cache_prefix, "%s.%u.%u.%i.%s.%s", model_path.c_str(), maxBatchSize, (uint32_t)allowGPUFallback, NV_TENSORRT_VERSION, deviceTypeToStr(device), precisionTypeToStr(precision));
+	sprintf(cache_path, "%s.calibration", cache_prefix);
+	mCacheCalibrationPath = cache_path;
+	
+	sprintf(cache_path, "%s.%s", model_path.c_str(), CHECKSUM_TYPE);
+	mChecksumPath = cache_path;
+	
+	sprintf(cache_path, "%s.engine", cache_prefix);
+	mCacheEnginePath = cache_path;	
+
+	// check for existence of cache
+	if( !ValidateEngine(model_path.c_str(), cache_path, mChecksumPath.c_str()) )
+	{
+		LogVerbose(LOG_TRT "cache file invalid, profiling network model on device %s\n", deviceTypeToStr(device));
+	
+		// check for existence of model
+		if( model_path.size() == 0 )
+		{
+			LogError("\nerror:  model file '%s' was not found.\n", model_path_);
+			LogInfo("%s\n", LOG_DOWNLOADER_TOOL);
+			return 0;
+		}
+
+		// parse the model and profile the engine
+		if( !ProfileModel(prototxt_path, model_path, input_blobs, input_dims,
+					   output_blobs, maxBatchSize, precision, device, 
+					   allowGPUFallback, calibrator, &engineStream, &engineSize) )
+		{
+			LogError(LOG_TRT "device %s, failed to load %s\n", deviceTypeToStr(device), model_path_);
+			return 0;
+		}
+	
+		LogVerbose(LOG_TRT "network profiling complete, saving engine cache to %s\n", cache_path);
+		
+		// write the cache file
+		FILE* cacheFile = NULL;
+		cacheFile = fopen(cache_path, "wb");
+
+		if( cacheFile != NULL )
+		{
+			if( fwrite(engineStream,	1, engineSize, cacheFile) != engineSize )
+				LogError(LOG_TRT "failed to write %zu bytes to engine cache file %s\n", engineSize, cache_path);
+		
+			fclose(cacheFile);
+		}
+		else
+		{
+			LogError(LOG_TRT "failed to open engine cache file for writing %s\n", cache_path);
+		}
+
+		LogSuccess(LOG_TRT "device %s, completed saving engine cache to %s\n", deviceTypeToStr(device), cache_path);
+		
+		// write the checksum file
+		LogVerbose(LOG_TRT "saving model checksum to %s\n", mChecksumPath.c_str());
+		
+		char cmd[PATH_MAX * 2 + 256];
+		snprintf(cmd, sizeof(cmd), "%s %s | awk '{print $1}' > %s", CHECKSUM_TYPE, model_path.c_str(), mChecksumPath.c_str());
+	
+		LogVerbose(LOG_TRT "%s\n", cmd);
+	
+		const int result = system(cmd);
+		
+		if( result != 0 )
+			LogError(LOG_TRT "failed to save model checksum to %s\n", mChecksumPath.c_str());
+	}
+	else
+	{
+		if( !LoadEngine(cache_path, &engineStream, &engineSize) )
+			return false;
+	}
+
+	LogSuccess(LOG_TRT "device %s, loaded %s\n", deviceTypeToStr(device), model_path.c_str());
+	
+
+	/*
+	 * create the runtime engine instance
+	 */
+	if( !LoadEngine(engineStream, engineSize, input_blobs, output_blobs, NULL, device, stream) )
+	{
+		LogError(LOG_TRT "failed to create TensorRT engine for %s, device %s\n", model_path.c_str(), deviceTypeToStr(device));
+		return false;
+	}
+
+	free(engineStream); // not used anymore
+
+	mPrototxtPath     = prototxt_path;
+	mModelPath        = model_path;
+	mModelFile        = pathFilename(mModelPath);
+	mPrecision        = precision;
+	mAllowGPUFallback = allowGPUFallback;
+	mMaxBatchSize 	   = maxBatchSize;
+
+	if( mean_path != NULL )
+		mMeanPath = mean_path;
+
+	LogInfo(LOG_TRT "\n");
+	LogSuccess(LOG_TRT "device %s, %s initialized.\n", deviceTypeToStr(device), mModelPath.c_str());	
+	
+	return true;
+}
+
+
+// LoadEngine
+bool tensorNet::LoadEngine( char* engine_stream, size_t engine_size,
+			  		   const std::vector<std::string>& input_blobs, 
+			  		   const std::vector<std::string>& output_blobs,
+			  		   nvinfer1::IPluginFactory* pluginFactory,
+					   deviceType device, cudaStream_t stream )
+{
+	/*
+	 * create runtime inference engine execution context
+	 */
+	nvinfer1::IRuntime* infer = CREATE_INFER_RUNTIME(gLogger);
+	
+	if( !infer )
+	{
+		LogError(LOG_TRT "device %s, failed to create TensorRT runtime\n", deviceTypeToStr(device));
+		return false;
+	}
+
+#if NV_TENSORRT_MAJOR >= 5 
+#if !(NV_TENSORRT_MAJOR == 5 && NV_TENSORRT_MINOR == 0 && NV_TENSORRT_PATCH == 0)
+	// if using DLA, set the desired core before deserialization occurs
+	if( device == DEVICE_DLA_0 )
+	{
+		LogVerbose(LOG_TRT "device %s, enabling DLA core 0\n", deviceTypeToStr(device));
+		infer->setDLACore(0);
+	}
+	else if( device == DEVICE_DLA_1 )
+	{
+		LogVerbose(LOG_TRT "device %s, enabling DLA core 1\n", deviceTypeToStr(device));
+		infer->setDLACore(1);
+	}
+#endif
+#endif
+
+#if NV_TENSORRT_MAJOR > 1
+	nvinfer1::ICudaEngine* engine = infer->deserializeCudaEngine(engine_stream, engine_size, pluginFactory);
+#else
+	nvinfer1::ICudaEngine* engine = infer->deserializeCudaEngine(engine_stream, engine_size); //infer->deserializeCudaEngine(modelStream);
+#endif
+
+	if( !engine )
+	{
+		LogError(LOG_TRT "device %s, failed to create CUDA engine\n", deviceTypeToStr(device));
+		return false;
+	}
+
+	if( !LoadEngine(engine, input_blobs, output_blobs, device, stream) )
+	{
+		LogError(LOG_TRT "device %s, failed to create resources for CUDA engine\n", deviceTypeToStr(device));
+		return false;
+	}	
+
+	mInfer = infer;
+	return true;
+}
+
+
+// LoadEngine
+bool tensorNet::LoadEngine( nvinfer1::ICudaEngine* engine,
+ 			  		   const std::vector<std::string>& input_blobs, 
+			  		   const std::vector<std::string>& output_blobs,
+			  		   deviceType device, cudaStream_t stream)
+{
+	if( !engine )
+		return NULL;
+
+	nvinfer1::IExecutionContext* context = engine->createExecutionContext();
+	
+	if( !context )
+	{
+		LogError(LOG_TRT "device %s, failed to create execution context\n", deviceTypeToStr(device));
+		return 0;
+	}
+
+	if( mEnableDebug )
+	{
+		LogVerbose(LOG_TRT "device %s, enabling context debug sync.\n", deviceTypeToStr(device));
+		context->setDebugSync(true);
+	}
+
+	if( mEnableProfiler )
+		context->setProfiler(&gProfiler);
+
+	mMaxBatchSize = engine->getMaxBatchSize();
+
+	LogInfo(LOG_TRT "\n");
+	LogInfo(LOG_TRT "CUDA engine context initialized on device %s:\n", deviceTypeToStr(device));
+	LogInfo(LOG_TRT "   -- layers       %i\n", engine->getNbLayers());
+	LogInfo(LOG_TRT "   -- maxBatchSize %u\n", mMaxBatchSize);
+	
+#if NV_TENSORRT_MAJOR <= 7
+	LogInfo(LOG_TRT "   -- workspace    %zu\n", engine->getWorkspaceSize());
+#endif
+
+#if NV_TENSORRT_MAJOR >= 4
+	LogInfo(LOG_TRT "   -- deviceMemory %zu\n", engine->getDeviceMemorySize());
+	LogInfo(LOG_TRT "   -- bindings     %i\n", engine->getNbBindings());
+
+	/*
+	 * print out binding info
+	 */
+	const int numBindings = engine->getNbBindings();
+	
+	for( int n=0; n < numBindings; n++ )
+	{
+		LogInfo(LOG_TRT "   binding %i\n", n);
+
+		const char* bind_name = engine->getBindingName(n);
+
+		LogInfo("                -- index   %i\n", n);
+		LogInfo("                -- name    '%s'\n", bind_name);
+		LogInfo("                -- type    %s\n", dataTypeToStr(engine->getBindingDataType(n)));
+		LogInfo("                -- in/out  %s\n", engine->bindingIsInput(n) ? "INPUT" : "OUTPUT");
+
+		const nvinfer1::Dims bind_dims = engine->getBindingDimensions(n);
+
+		LogInfo("                -- # dims  %i\n", bind_dims.nbDims);
+		
+		for( int i=0; i < bind_dims.nbDims; i++ )
+		#if NV_TENSORRT_MAJOR >= 8
+			LogInfo("                -- dim #%i  %i\n", i, bind_dims.d[i]);	
+		#else
+			LogInfo("                -- dim #%i  %i (%s)\n", i, bind_dims.d[i], dimensionTypeToStr(bind_dims.type[i]));	
+		#endif
+	}
+
+	LogInfo(LOG_TRT "\n");
+#endif
+
+	/*
+	 * setup network input buffers
+	 */
+	const int numInputs = input_blobs.size();
+	
+	for( int n=0; n < numInputs; n++ )
+	{
+		const int inputIndex = engine->getBindingIndex(input_blobs[n].c_str());	
+		
+		if( inputIndex < 0 )
+		{
+			LogError(LOG_TRT "failed to find requested input layer %s in network\n", input_blobs[n].c_str());
+			return false;
+		}
+
+		LogVerbose(LOG_TRT "binding to input %i %s  binding index:  %i\n", n, input_blobs[n].c_str(), inputIndex);
+
+	#if NV_TENSORRT_MAJOR > 1
+		nvinfer1::Dims inputDims = validateDims(engine->getBindingDimensions(inputIndex));
+
+	#if NV_TENSORRT_MAJOR >= 7
+	    if( mModelType == MODEL_ONNX )
+		   inputDims = shiftDims(inputDims);   // change NCHW to CHW if EXPLICIT_BATCH set
+	#endif
+	#else
+		Dims3 inputDims = engine->getBindingDimensions(inputIndex);
+	#endif
+
+		const size_t inputSize = mMaxBatchSize * sizeDims(inputDims) * sizeof(float);
+		LogVerbose(LOG_TRT "binding to input %i %s  dims (b=%u c=%u h=%u w=%u) size=%zu\n", n, input_blobs[n].c_str(), mMaxBatchSize, DIMS_C(inputDims), DIMS_H(inputDims), DIMS_W(inputDims), inputSize);
+
+		// allocate memory to hold the input buffer
+		void* inputCPU  = NULL;
+		void* inputCUDA = NULL;
+
+	#ifdef USE_INPUT_TENSOR_CUDA_DEVICE_MEMORY
+		if( CUDA_FAILED(cudaMalloc((void**)&inputCUDA, inputSize)) )
+		{
+			LogError(LOG_TRT "failed to alloc CUDA device memory for tensor input, %zu bytes\n", inputSize);
+			return false;
+		}
+		
+		CUDA(cudaMemset(inputCUDA, 0, inputSize));
+	#else
+		if( !cudaAllocMapped((void**)&inputCPU, (void**)&inputCUDA, inputSize) )
+		{
+			LogError(LOG_TRT "failed to alloc CUDA mapped memory for tensor input, %zu bytes\n", inputSize);
+			return false;
+		}
+	#endif
+	
+		layerInfo l;
+		
+		l.CPU  = (float*)inputCPU;
+		l.CUDA = (float*)inputCUDA;
+		l.size = inputSize;
+		l.name = input_blobs[n];
+		l.binding = inputIndex;
+		
+		copyDims(&l.dims, &inputDims);
+		mInputs.push_back(l);
+	}
+
+	/*
+	 * setup network output buffers
+	 */
+	const int numOutputs = output_blobs.size();
+	
+	for( int n=0; n < numOutputs; n++ )
+	{
+		const int outputIndex = engine->getBindingIndex(output_blobs[n].c_str());
+
+		if( outputIndex < 0 )
+		{
+			LogError(LOG_TRT "failed to find requested output layer %s in network\n", output_blobs[n].c_str());
+			return false;
+		}
+
+		LogVerbose(LOG_TRT "binding to output %i %s  binding index:  %i\n", n, output_blobs[n].c_str(), outputIndex);
+
+	#if NV_TENSORRT_MAJOR > 1
+		nvinfer1::Dims outputDims = validateDims(engine->getBindingDimensions(outputIndex));
+
+	#if NV_TENSORRT_MAJOR >= 7
+		if( mModelType == MODEL_ONNX )
+			outputDims = shiftDims(outputDims);  // change NCHW to CHW if EXPLICIT_BATCH set
+	#endif
+	#else
+		Dims3 outputDims = engine->getBindingDimensions(outputIndex);
+	#endif
+
+		const size_t outputSize = mMaxBatchSize * sizeDims(outputDims) * sizeof(float);
+		LogVerbose(LOG_TRT "binding to output %i %s  dims (b=%u c=%u h=%u w=%u) size=%zu\n", n, output_blobs[n].c_str(), mMaxBatchSize, DIMS_C(outputDims), DIMS_H(outputDims), DIMS_W(outputDims), outputSize);
+	
+		// allocate output memory 
+		void* outputCPU  = NULL;
+		void* outputCUDA = NULL;
+		
+		//if( CUDA_FAILED(cudaMalloc((void**)&outputCUDA, outputSize)) )
+		if( !cudaAllocMapped((void**)&outputCPU, (void**)&outputCUDA, outputSize) )
+		{
+			LogError(LOG_TRT "failed to alloc CUDA mapped memory for tensor output, %zu bytes\n", outputSize);
+			return false;
+		}
+	
+		layerInfo l;
+		
+		l.CPU  = (float*)outputCPU;
+		l.CUDA = (float*)outputCUDA;
+		l.size = outputSize;
+		l.name = output_blobs[n];
+		l.binding = outputIndex;
+		
+		copyDims(&l.dims, &outputDims);
+		mOutputs.push_back(l);
+	}
+	
+	/*
+	 * create list of binding buffers
+	 */
+	const int bindingSize = numBindings * sizeof(void*);
+
+	mBindings = (void**)malloc(bindingSize);
+
+	if( !mBindings )
+	{
+		LogError(LOG_TRT "failed to allocate %u bytes for bindings list\n", bindingSize);
+		return false;
+	}
+
+	memset(mBindings, 0, bindingSize);
+
+	for( uint32_t n=0; n < GetInputLayers(); n++ )
+		mBindings[mInputs[n].binding] = mInputs[n].CUDA;
+
+	for( uint32_t n=0; n < GetOutputLayers(); n++ )
+		mBindings[mOutputs[n].binding] = mOutputs[n].CUDA;
+	
+	// find unassigned bindings and allocate them
+	for( uint32_t n=0; n < numBindings; n++ )
+	{
+		if( mBindings[n] != NULL )
+			continue;
+		
+		const size_t bindingSize = sizeDims(validateDims(engine->getBindingDimensions(n))) * mMaxBatchSize * sizeof(float);
+		
+		if( CUDA_FAILED(cudaMalloc(&mBindings[n], bindingSize)) )
+		{
+			LogError(LOG_TRT "failed to allocate %zu bytes for unused binding %u\n", bindingSize, n);
+			return false;
+		}
+		
+		LogVerbose(LOG_TRT "allocated %zu bytes for unused binding %u\n", bindingSize, n);
+	}
+	
+
+	/*
+	 * create events for timing
+	 */
+	for( int n=0; n < PROFILER_TOTAL * 2; n++ )
+		CUDA(cudaEventCreate(&mEventsGPU[n]));
+	
+	mEngine  = engine;
+	mDevice  = device;
+	mContext = context;
+	
+	SetStream(stream);	// set default device stream
+
+	return true;
+}
+
+
+// LoadEngine
+bool tensorNet::LoadEngine( const char* engine_filename,
+			  		   const std::vector<std::string>& input_blobs, 
+			  		   const std::vector<std::string>& output_blobs,
+			  		   nvinfer1::IPluginFactory* pluginFactory,
+					   deviceType device, cudaStream_t stream )
+{
+	char* engineStream = NULL;
+	size_t engineSize = 0;
+
+	// load the engine file contents
+	if( !LoadEngine(engine_filename, &engineStream, &engineSize) )
+		return false;
+
+	// load engine resources from stream
+	if( !LoadEngine(engineStream, engineSize, input_blobs, output_blobs,
+				 pluginFactory, device, stream) )
+	{
+		free(engineStream);
+		return false;
+	}
+
+	free(engineStream);
+	return true;
+}
+
+
+// LoadEngine
+bool tensorNet::LoadEngine( const char* filename, char** stream, size_t* size )
+{
+	if( !filename || !stream || !size )
+		return false;
+
+	LogInfo(LOG_TRT "loading network plan from engine cache... %s\n", filename);
+		
+	void* engineStream = NULL;
+	const size_t engineSize = loadFile(filename, &engineStream);
+	
+	if( engineSize == 0 )
+	{
+		LogError(LOG_TRT "failed to load engine cache from %s\n", filename);
+		return false;
+	}
+	
+	*stream = (char*)engineStream;
+	*size = engineSize;
+	
+	return true;
+}
+
+
+// ValidateEngine
+bool tensorNet::ValidateEngine( const char* model_path, const char* cache_path, const char* checksum_path )
+{
+	// check for existence of cache
+	if( !fileExists(cache_path) )
+	{
+		LogVerbose(LOG_TRT "could not find engine cache %s\n", cache_path);
+		return false;
+	}
+	
+	LogVerbose(LOG_TRT "found engine cache file %s\n", cache_path);
+	
+	// check for existence of checksum
+	if( !fileExists(checksum_path) )
+	{
+		LogVerbose(LOG_TRT "could not find model checksum %s\n", checksum_path);
+		return false;
+	}
+	
+	LogVerbose(LOG_TRT "found model checksum %s\n", checksum_path);
+	
+	// validate that the checksum matches the original model
+	char cmd[PATH_MAX * 2 + 256];
+	snprintf(cmd, sizeof(cmd), "echo \"$(cat %s) %s\" | %s --check --status", checksum_path, model_path, CHECKSUM_TYPE);  // https://superuser.com/a/1468626
+	
+	LogVerbose(LOG_TRT "%s\n", cmd);
+	
+	const int result = system(cmd);
+	
+	if( result != 0 )
+	{
+		LogVerbose(LOG_TRT "model did not match checksum %s (return code %i)\n", checksum_path, result);
+		return false;
+	}
+	
+	LogVerbose(LOG_TRT "model matched checksum %s\n", checksum_path);
+	return true;
+}
+
+
+// CreateStream
+cudaStream_t tensorNet::CreateStream( bool nonBlocking )
+{
+	uint32_t flags = cudaStreamDefault;
+
+	if( nonBlocking )
+		flags = cudaStreamNonBlocking;
+
+	cudaStream_t stream = NULL;
+
+	if( CUDA_FAILED(cudaStreamCreateWithFlags(&stream, flags)) )
+		return NULL;
+
+	SetStream(stream);
+	return stream;
+}
+
+
+// SetStream
+void tensorNet::SetStream( cudaStream_t stream )
+{
+	mStream = stream;
+
+	if( !mStream )
+		return;
+}	
+
+
+// ProcessNetwork
+bool tensorNet::ProcessNetwork( bool sync )
+{
+	if( TENSORRT_VERSION_CHECK(8,4,1) && mModelType == MODEL_ONNX )
+	{
+		// on TensorRT 8.4.1 (JetPack 5.0.2 / L4T R35.1.0) and newer, this warning appears:
+		// the execute() method has been deprecated when used with engines built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. Please use executeV2() instead.
+		// also, the batchSize argument passed into this function has no effect on changing the input shapes. Please use setBindingDimensions() function to change input shapes instead.
+		if( sync )
+		{
+			if( !mContext->executeV2(mBindings) )
+			{
+				LogError(LOG_TRT "failed to execute TensorRT context on device %s\n", deviceTypeToStr(mDevice));
+				return false;
+			}
+		}
+		else
+		{
+			if( !mContext->enqueueV2(mBindings, mStream, NULL) )
+			{
+				LogError(LOG_TRT "failed to enqueue TensorRT context on device %s\n", deviceTypeToStr(mDevice));
+				return false;
+			}
+		}
+	}
+	else
+	{
+		if( sync )
+		{
+			if( !mContext->execute(1, mBindings) )
+			{
+				LogError(LOG_TRT "failed to execute TensorRT context on device %s\n", deviceTypeToStr(mDevice));
+				return false;
+			}
+		}
+		else
+		{
+			if( !mContext->enqueue(1, mBindings, mStream, NULL) )
+			{
+				LogError(LOG_TRT "failed to enqueue TensorRT context on device %s\n", deviceTypeToStr(mDevice));
+				return false;
+			}
+		}
+	}
+	
+	return true;
+}
+
+
+// validateClassLabels
+static bool validateClassLabels( std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses )
+{
+	const int numLoaded = descriptions.size();
+	LogVerbose(LOG_TRT "loaded %i class labels\n", numLoaded);
+	
+	if( expectedClasses > 0 )
+	{
+		if( numLoaded != expectedClasses )
+			LogError(LOG_TRT "didn't load expected number of class descriptions  (%i of %i)\n", numLoaded, expectedClasses);
+
+		if( numLoaded < expectedClasses )
+		{
+			LogWarning(LOG_TRT "filling in remaining %i class descriptions with default labels\n", (expectedClasses - numLoaded));
+	
+			for( int n=numLoaded; n < expectedClasses; n++ )
+			{
+				char synset[10];
+				sprintf(synset, "n%08i", n);
+
+				char desc[64];
+				sprintf(desc, "Class #%i", n);
+
+				synsets.push_back(synset);
+				descriptions.push_back(desc);
+			}
+		}
+	}
+	else if( numLoaded == 0 )
+	{
+		return false;
+	}
+	
+	/*for( uint32_t n=0; n < descriptions.size(); n++ )
+		LogVerbose(LOG_TRT "detectNet -- class label #%u:  '%s'\n", n, descriptions[n].c_str());*/
+	
+	return true;
+}
+
+	
+// LoadClassLabels
+bool tensorNet::LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses )
+{
+	if( !filename )
+		return validateClassLabels(descriptions, synsets, expectedClasses);
+	
+	// locate the file
+	const std::string path = locateFile(filename);
+
+	if( path.length() == 0 )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassLabels() failed to find %s\n", filename);
+		return validateClassLabels(descriptions, synsets, expectedClasses);
+	}
+
+	// open the file
+	FILE* f = fopen(path.c_str(), "r");
+	
+	if( !f )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassLabels() failed to open %s\n", path.c_str());
+		return validateClassLabels(descriptions, synsets, expectedClasses);
+	}
+	
+	descriptions.clear();
+	synsets.clear();
+
+	// read class descriptions
+	char str[512];
+	uint32_t customClasses = 0;
+
+	while( fgets(str, 512, f) != NULL )
+	{
+		const int syn = 9;  // length of synset prefix (in characters)
+		const int len = strlen(str);
+		
+		if( len > syn && str[0] == 'n' && str[syn] == ' ' )
+		{
+			str[syn]   = 0;
+			str[len-1] = 0;
+	
+			const std::string a = str;
+			const std::string b = (str + syn + 1);
+	
+			//printf("a=%s b=%s\n", a.c_str(), b.c_str());
+
+			synsets.push_back(a);
+			descriptions.push_back(b);
+		}
+		else if( len > 0 )	// no 9-character synset prefix (i.e. from DIGITS snapshot)
+		{
+			char a[10];
+			sprintf(a, "n%08u", customClasses);
+
+			//printf("a=%s b=%s (custom non-synset)\n", a, str);
+			customClasses++;
+
+			if( str[len-1] == '\n' )
+				str[len-1] = 0;
+
+			synsets.push_back(a);
+			descriptions.push_back(str);
+		}
+	}
+	
+	fclose(f);
+	return validateClassLabels(descriptions, synsets, expectedClasses);
+}
+
+
+// LoadClassLabels
+bool tensorNet::LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, int expectedClasses )
+{
+	std::vector<std::string> synsets;
+	return LoadClassLabels(filename, descriptions, synsets, expectedClasses);
+}
+
+
+// validateClassColors
+static bool validateClassColors( float4* colors, int numLoaded, int expectedClasses, float defaultAlpha )
+{
+	LogVerbose(LOG_TRT "loaded %i class colors\n", numLoaded);
+	
+	if( expectedClasses > 0 )
+	{
+		if( numLoaded != expectedClasses )
+			LogWarning(LOG_TRT "didn't load expected number of class colors  (%i of %i)\n", numLoaded, expectedClasses);
+
+		if( numLoaded < expectedClasses )
+		{
+			LogWarning(LOG_TRT "filling in remaining %i class colors with default colors\n", (expectedClasses - numLoaded));
+	
+			for( int n=numLoaded; n < expectedClasses; n++ )
+			{
+				colors[n] = tensorNet::GenerateColor(n, defaultAlpha);
+				//LogVerbose(LOG_TRT "class color %i  (%f %f %f %f\n", n, colors[n].x, colors[n].y, colors[n].z, colors[n].w);
+			}
+		}
+	}
+	else if( numLoaded == 0 )
+	{
+		return false;
+	}
+
+	return true;
+}
+
+
+// LoadClassColors
+bool tensorNet::LoadClassColors( const char* filename, float4* colors, int expectedClasses, float defaultAlpha )
+{
+	// validate parameters
+	if( !colors || expectedClasses <= 0 )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassColors() had invalid/NULL parameters\n");
+		return false;
+	}
+	
+	if( !filename )
+		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
+	
+	// locate the file
+	const std::string path = locateFile(filename);
+
+	if( path.length() == 0 )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassColors() failed to find %s\n", filename);
+		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
+	}
+	
+	// open the file
+	FILE* f = fopen(path.c_str(), "r");
+	
+	if( !f )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassColors() failed to open %s\n", path.c_str());
+		return validateClassColors(colors, 0, expectedClasses, defaultAlpha);
+	}
+	
+	// read class colors
+	char str[512];
+	int numLoaded = 0;
+
+	while( fgets(str, 512, f) != NULL && numLoaded < expectedClasses )
+	{
+		const int len = strlen(str);
+		
+		if( len <= 0 )
+			continue;
+		
+		if( str[len-1] == '\n' )
+			str[len-1] = 0;
+
+		float r = 255;
+		float g = 255;
+		float b = 255;
+		float a = defaultAlpha;
+
+		sscanf(str, "%f %f %f %f", &r, &g, &b, &a);
+		LogVerbose(LOG_TRT "class %02i  color %f %f %f %f\n", numLoaded, r, g, b, a);
+		colors[numLoaded] = make_float4(r, g, b, a);
+		numLoaded++; 
+	}
+	
+	fclose(f);
+	return validateClassColors(colors, numLoaded, expectedClasses, defaultAlpha);
+}
+
+
+// LoadClassColors
+bool tensorNet::LoadClassColors( const char* filename, float4** colors, int expectedClasses, float defaultAlpha )
+{
+	// validate parameters
+	if( !colors || expectedClasses <= 0 )
+	{
+		LogError(LOG_TRT "tensorNet::LoadClassColors() had invalid/NULL parameters\n");
+		return false;
+	}
+	
+	// allocate memory
+	if( !cudaAllocMapped((void**)colors, expectedClasses * sizeof(float4)) )
+		return false;
+	
+	// load colors
+	return LoadClassColors(filename, colors[0], expectedClasses, defaultAlpha);
+}
+
+
+// GenerateColor
+float4 tensorNet::GenerateColor( uint32_t classID, float alpha )
+{
+	// the first color is black, skip that one
+	classID += 1;
+
+	// https://github.com/dusty-nv/pytorch-segmentation/blob/16882772bc767511d892d134918722011d1ea771/datasets/sun_remap.py#L90
+	#define bitget(byteval, idx)	((byteval & (1 << idx)) != 0)
+	
+	int r = 0;
+	int g = 0;
+	int b = 0;
+	int c = classID;
+
+	for( int j=0; j < 8; j++ )
+	{
+		r = r | (bitget(c, 0) << 7 - j);
+		g = g | (bitget(c, 1) << 7 - j);
+		b = b | (bitget(c, 2) << 7 - j);
+		c = c >> 3;
+	}
+
+	return make_float4(r, g, b, alpha);
+}
diff --git a/c/tensorNet.h b/c/tensorNet.h
index ad71e64a..dd2af8de 100644
--- a/c/tensorNet.h
+++ b/c/tensorNet.h
@@ -1,827 +1,839 @@
-/*
- * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
- * DEALINGS IN THE SOFTWARE.
- */
- 
-#ifndef __TENSOR_NET_H__
-#define __TENSOR_NET_H__
-
-// forward declaration of IInt8Calibrator
-namespace nvinfer1 { class IInt8Calibrator; }
-
-// includes
-#include <NvInfer.h>
-
-#include <jetson-utils/cudaUtility.h>
-#include <jetson-utils/commandLine.h>
-#include <jetson-utils/imageFormat.h>
-#include <jetson-utils/timespec.h>
-#include <jetson-utils/logging.h>
-
-#include <vector>
-#include <sstream>
-#include <math.h>
-
-
-#if NV_TENSORRT_MAJOR >= 6
-typedef nvinfer1::Dims3 Dims3;
-
-#define DIMS_C(x) x.d[0]
-#define DIMS_H(x) x.d[1]
-#define DIMS_W(x) x.d[2]
-
-#elif NV_TENSORRT_MAJOR >= 2
-typedef nvinfer1::DimsCHW Dims3;
-
-#define DIMS_C(x) x.d[0]
-#define DIMS_H(x) x.d[1]
-#define DIMS_W(x) x.d[2]
-
-#else
-typedef nvinfer1::Dims3 Dims3; 
-
-#define DIMS_C(x) x.c
-#define DIMS_H(x) x.h
-#define DIMS_W(x) x.w
-
-#ifndef NV_TENSORRT_MAJOR
-#define NV_TENSORRT_MAJOR 1
-#define NV_TENSORRT_MINOR 0
-#endif
-#endif
-
-#if NV_TENSORRT_MAJOR >= 8
-#define NOEXCEPT noexcept
-#else
-#define NOEXCEPT
-#endif
-
-
-/**
- * Macro for checking the minimum version of TensorRT that is installed.
- * This evaluates to true if TensorRT is newer or equal to the provided version.
- * @ingroup tensorNet
- */
-#define TENSORRT_VERSION_CHECK(major, minor, patch)  (NV_TENSORRT_MAJOR > major || (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR > minor) || (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR == minor && NV_TENSORRT_PATCH >= patch))
-
-/**
- * Default maximum batch size
- * @ingroup tensorNet
- */
-#define DEFAULT_MAX_BATCH_SIZE  1
-
-/**
- * Prefix used for tagging printed log output from TensorRT.
- * @ingroup tensorNet
- */
-#define LOG_TRT "[TRT]    "
-
-
-/**
- * Enumeration for indicating the desired precision that
- * the network should run in, if available in hardware.
- * @ingroup tensorNet
- */
-enum precisionType
-{
-	TYPE_DISABLED = 0,	/**< Unknown, unspecified, or disabled type */
-	TYPE_FASTEST,		/**< The fastest detected precision should be use (i.e. try INT8, then FP16, then FP32) */
-	TYPE_FP32,		/**< 32-bit floating-point precision (FP32) */
-	TYPE_FP16,		/**< 16-bit floating-point half precision (FP16) */
-	TYPE_INT8,		/**< 8-bit integer precision (INT8) */
-	NUM_PRECISIONS		/**< Number of precision types defined */
-};
-
-/**
- * Stringize function that returns precisionType in text.
- * @ingroup tensorNet
- */
-const char* precisionTypeToStr( precisionType type );
-
-/**
- * Parse the precision type from a string.
- * @ingroup tensorNet
- */
-precisionType precisionTypeFromStr( const char* str );
-
-/**
- * Enumeration for indicating the desired device that 
- * the network should run on, if available in hardware.
- * @ingroup tensorNet
- */
-enum deviceType
-{
-	DEVICE_GPU = 0,			/**< GPU (if multiple GPUs are present, a specific GPU can be selected with cudaSetDevice() */
-	DEVICE_DLA,				/**< Deep Learning Accelerator (DLA) Core 0 (only on Jetson Xavier) */
-	DEVICE_DLA_0 = DEVICE_DLA,	/**< Deep Learning Accelerator (DLA) Core 0 (only on Jetson Xavier) */
-	DEVICE_DLA_1,				/**< Deep Learning Accelerator (DLA) Core 1 (only on Jetson Xavier) */
-	NUM_DEVICES				/**< Number of device types defined */
-};
-
-/**
- * Stringize function that returns deviceType in text.
- * @ingroup tensorNet
- */
-const char* deviceTypeToStr( deviceType type );
-
-/**
- * Parse the device type from a string.
- * @ingroup tensorNet
- */
-deviceType deviceTypeFromStr( const char* str );
-
-/**
- * Enumeration indicating the format of the model that's
- * imported in TensorRT (either caffe, ONNX, or UFF).
- * @ingroup tensorNet
- */
-enum modelType
-{
-	MODEL_CUSTOM = 0,	/**< Created directly with TensorRT API */
-	MODEL_CAFFE,		/**< caffemodel */
-	MODEL_ONNX,		/**< ONNX */
-	MODEL_UFF,		/**< UFF */
-	MODEL_ENGINE		/**< TensorRT engine/plan */
-};
-
-/**
- * Stringize function that returns modelType in text.
- * @ingroup tensorNet
- */
-const char* modelTypeToStr( modelType type );
-
-/**
- * Parse the model format from a string.
- * @ingroup tensorNet
- */
-modelType modelTypeFromStr( const char* str );
-
-/**
- * Parse the model format from a file path.
- * @ingroup tensorNet
- */
-modelType modelTypeFromPath( const char* path );
-
-/**
- * Profiling queries
- * @see tensorNet::GetProfilerTime()
- * @ingroup tensorNet
- */
-enum profilerQuery
-{
-	PROFILER_PREPROCESS = 0,
-	PROFILER_NETWORK,
-	PROFILER_POSTPROCESS,
-	PROFILER_VISUALIZE,
-	PROFILER_TOTAL,
-};
-
-/**
- * Stringize function that returns profilerQuery in text.
- * @ingroup tensorNet
- */
-const char* profilerQueryToStr( profilerQuery query );
-
-/**
- * Profiler device
- * @ingroup tensorNet
- */
-enum profilerDevice
-{
-	PROFILER_CPU = 0,	/**< CPU walltime */
-	PROFILER_CUDA,		/**< CUDA kernel time */ 
-};
-
-
-/**
- * Abstract class for loading a tensor network with TensorRT.
- * For example implementations, @see imageNet and @see detectNet
- * @ingroup tensorNet
- */
-class tensorNet
-{
-public:
-	/**
-	 * Destory
-	 */
-	virtual ~tensorNet();
-	
-	/**
-	 * Load a new network instance
-	 * @param prototxt File path to the deployable network prototxt
-	 * @param model File path to the caffemodel 
-	 * @param mean File path to the mean value binary proto (NULL if none)
-	 * @param input_blob The name of the input blob data to the network.
-	 * @param output_blob The name of the output blob data from the network.
-	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
-	 */
-	bool LoadNetwork( const char* prototxt, const char* model, const char* mean=NULL,
-				   const char* input_blob="data", const char* output_blob="prob",
-				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST,
-				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
-				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
-
-	/**
-	 * Load a new network instance with multiple output layers
-	 * @param prototxt File path to the deployable network prototxt
-	 * @param model File path to the caffemodel 
-	 * @param mean File path to the mean value binary proto (NULL if none)
-	 * @param input_blob The name of the input blob data to the network.
-	 * @param output_blobs List of names of the output blobs from the network.
-	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
-	 */
-	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
-				   const char* input_blob, const std::vector<std::string>& output_blobs,
-				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST,
-				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
-				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
-
-	/**
-	 * Load a new network instance with multiple input layers.
-	 * @param prototxt File path to the deployable network prototxt
-	 * @param model File path to the caffemodel 
-	 * @param mean File path to the mean value binary proto (NULL if none)
-	 * @param input_blobs List of names of the inputs blob data to the network.
-	 * @param output_blobs List of names of the output blobs from the network.
-	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
-	 */
-	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
-				   const std::vector<std::string>& input_blobs, 
-				   const std::vector<std::string>& output_blobs,
-				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-				   precisionType precision=TYPE_FASTEST,
-				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
-				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
-
-	/**
-	 * Load a new network instance (this variant is used for UFF models)
-	 * @param prototxt File path to the deployable network prototxt
-	 * @param model File path to the caffemodel 
-	 * @param mean File path to the mean value binary proto (NULL if none)
-	 * @param input_blob The name of the input blob data to the network.
-	 * @param input_dims The dimensions of the input blob (used for UFF).
-	 * @param output_blobs List of names of the output blobs from the network.
-	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
-	 */
-	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
-				   const char* input_blob, const Dims3& input_dims, 
-				   const std::vector<std::string>& output_blobs,
-				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-				   precisionType precision=TYPE_FASTEST,
-				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
-				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
-
-	/**
-	 * Load a new network instance with multiple input layers (used for UFF models)
-	 * @param prototxt File path to the deployable network prototxt
-	 * @param model File path to the caffemodel 
-	 * @param mean File path to the mean value binary proto (NULL if none)
-	 * @param input_blobs List of names of the inputs blob data to the network.
-	 * @param input_dims List of the dimensions of the input blobs (used for UFF).
-	 * @param output_blobs List of names of the output blobs from the network.
-	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
-	 */
-	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
-				   const std::vector<std::string>& input_blobs, 
-				   const std::vector<Dims3>& input_dims, 
-				   const std::vector<std::string>& output_blobs,
-				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
-				   precisionType precision=TYPE_FASTEST,
-				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
-				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
-
-	/**
-	 * Load a network instance from a serialized engine plan file.
-	 * @param engine_filename path to the serialized engine plan file.
-	 * @param input_blobs List of names of the inputs blob data to the network.
-	 * @param output_blobs List of names of the output blobs from the network.
-	 */
-	bool LoadEngine( const char* engine_filename,
-				  const std::vector<std::string>& input_blobs, 
-				  const std::vector<std::string>& output_blobs,
-				  nvinfer1::IPluginFactory* pluginFactory=NULL,
-				  deviceType device=DEVICE_GPU,
-				  cudaStream_t stream=NULL );
-
-	/**
-	 * Load a network instance from a serialized engine plan file.
-	 * @param engine_stream Memory containing the serialized engine plan file.
-	 * @param engine_size Size of the serialized engine stream (in bytes).
-	 * @param input_blobs List of names of the inputs blob data to the network.
-	 * @param output_blobs List of names of the output blobs from the network.
-	 */
-	bool LoadEngine( char* engine_stream, size_t engine_size,
-				  const std::vector<std::string>& input_blobs, 
-				  const std::vector<std::string>& output_blobs,
-				  nvinfer1::IPluginFactory* pluginFactory=NULL,
-				  deviceType device=DEVICE_GPU,
-				  cudaStream_t stream=NULL );
-
-	/**
-	 * Load network resources from an existing TensorRT engine instance.
-	 * @param engine_stream Memory containing the serialized engine plan file.
-	 * @param engine_size Size of the serialized engine stream (in bytes).
-	 * @param input_blobs List of names of the inputs blob data to the network.
-	 * @param output_blobs List of names of the output blobs from the network.
-	 */
-	bool LoadEngine( nvinfer1::ICudaEngine* engine,
-				  const std::vector<std::string>& input_blobs, 
-				  const std::vector<std::string>& output_blobs,
-				  deviceType device=DEVICE_GPU,
-				  cudaStream_t stream=NULL );
-
-	/**
-	 * Load a serialized engine plan file into memory.
-	 */
-	bool LoadEngine( const char* filename, char** stream, size_t* size );
-
-	/**
-	 * Load a binary file into memory.
-	 */
-	bool LoadBinary( const char* filename, char** stream, size_t* size );
-	
-	/**
-	 * Load class descriptions from a label file.  
-	 * Each line of the text file should include one class label (and optionally a synset).
-	 * If the number of expected labels aren't parsed, they will be automatically generated.
-	 */
-	static bool LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, int expectedClasses=-1 );
-
-	/**
-	 * Load class descriptions and synset strings from a label file.
-	 * Each line of the text file should include one class label (and optionally a synset).
-	 * If the number of expected labels aren't parsed, they will be automatically generated.
-	 */
-	static bool LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses=-1 );
-
-	/**
-	 * Load class colors from a text file.  If the number of expected colors aren't parsed, they will be generated.
-	 * The float4 color array should be `expectedClasses` long, and would typically be in shared CPU/GPU memory.
-	 * If a line in the text file only has RGB, then the defaultAlpha value will be used for the alpha channel.
-	 */
-	static bool LoadClassColors( const char* filename, float4* colors, int expectedClasses, float defaultAlpha=255.0f );
-
-	/**
-	 * Load class colors from a text file.  If the number of expected colors aren't parsed, they will be generated.
-	 * The float4 color array will automatically be allocated in shared CPU/GPU memory by `cudaAllocMapped()`.
-	 * If a line in the text file only has RGB, then the defaultAlpha value will be used for the alpha channel.
-	 */
-	static bool LoadClassColors( const char* filename, float4** colors, int expectedClasses, float defaultAlpha=255.0f );
-
-	/**
-	 * Procedurally generate a color for a given class index with the specified alpha value.
-	 * This function can be used to generate a range of colors when a colors.txt file isn't available.
-	 */
-	static float4 GenerateColor( uint32_t classID, float alpha=255.0f ); 
-	
-	/**
-	 * Manually enable layer profiling times.	
-	 */
-	void EnableLayerProfiler();
-
-	/**
-	 * Manually enable debug messages and synchronization.
-	 */
-	void EnableDebug();
-
-	/**
- 	 * Return true if GPU fallback is enabled.
-	 */
-	inline bool AllowGPUFallback() const					{ return mAllowGPUFallback; }
-
-	/**
- 	 * Retrieve the device being used for execution.
-	 */
-	inline deviceType GetDevice() const					{ return mDevice; }
-
-	/**
-	 * Retrieve the type of precision being used.
-	 */
-	inline precisionType GetPrecision() const				{ return mPrecision; }
-
-	/**
-	 * Check if a particular precision is being used.
-	 */
-	inline bool IsPrecision( precisionType type ) const		{ return (mPrecision == type); }
-
-	/**
-	 * Resolve a desired precision to a specific one that's available.
-	 */
-	static precisionType SelectPrecision( precisionType precision, deviceType device=DEVICE_GPU, bool allowInt8=true );
-
-	/**
-	 * Determine the fastest native precision on a device.
-	 */
-	static precisionType FindFastestPrecision( deviceType device=DEVICE_GPU, bool allowInt8=true );
-
-	/**
-	 * Detect the precisions supported natively on a device.
-	 */
-	static std::vector<precisionType> DetectNativePrecisions( deviceType device=DEVICE_GPU );
-	
-	/**
-	 * Detect if a particular precision is supported natively.
-	 */
-	static bool DetectNativePrecision( const std::vector<precisionType>& nativeTypes, precisionType type );
-
-	/**
-	 * Detect if a particular precision is supported natively.
-	 */
-	static bool DetectNativePrecision( precisionType precision, deviceType device=DEVICE_GPU );
-
-	/**
-	 * Retrieve the stream that the device is operating on.
-	 */
-	inline cudaStream_t GetStream() const					{ return mStream; }
-
-	/**
-	 * Create and use a new stream for execution.
-	 */
-	cudaStream_t CreateStream( bool nonBlocking=true );
-
-	/**
-	 * Set the stream that the device is operating on.
-	 */
-	void SetStream( cudaStream_t stream );
-
-	/**
-	 * Retrieve the path to the network prototxt file.
-	 */
-	inline const char* GetPrototxtPath() const				{ return mPrototxtPath.c_str(); }
-
-	/**
-	 * Retrieve the full path to model file, including the filename.
-	 */
-	inline const char* GetModelPath() const					{ return mModelPath.c_str(); }
-
-	/**
-	 * Retrieve the filename of the file, excluding the directory.
-	 */
-	inline const char* GetModelFilename() const				{ return mModelFile.c_str(); }
-	
-	/**
-	 * Retrieve the format of the network model.
-	 */
-	inline modelType GetModelType() const					{ return mModelType; }
-
-	/**
-	 * Return true if the model is of the specified format.
-	 */
-	inline bool IsModelType( modelType type ) const			{ return (mModelType == type); }
-
-	/**
-	 * Retrieve the number of input layers to the network.
-	 */
-	inline uint32_t GetInputLayers() const					{ return mInputs.size(); }
-
-	/**
-	 * Retrieve the number of output layers to the network.
-	 */
-	inline uint32_t GetOutputLayers() const					{ return mOutputs.size(); }
-
-	/**
-	 * Retrieve the dimensions of network input layer.
-	 */
-	inline Dims3 GetInputDims( uint32_t layer=0 ) const		{ return mInputs[layer].dims; }
-
-	/**
-	 * Retrieve the width of network input layer.
-	 */
-	inline uint32_t GetInputWidth( uint32_t layer=0 ) const	{ return DIMS_W(mInputs[layer].dims); }
-
-	/**
-	 * Retrieve the height of network input layer.
-	 */
-	inline uint32_t GetInputHeight( uint32_t layer=0 ) const	{ return DIMS_H(mInputs[layer].dims); }
-
-	/**
-	 * Retrieve the size (in bytes) of network input layer.
-	 */
-	inline uint32_t GetInputSize( uint32_t layer=0 ) const		{ return mInputs[layer].size; }
-
-	/**
-	 * Get the CUDA pointer to the input layer's memory.
-	 */
-	inline float* GetInputPtr( uint32_t layer=0 ) const		{ return mInputs[layer].CUDA; }
-	
-	/**
-	 * Retrieve the dimensions of network output layer.
-	 */
-	inline Dims3 GetOutputDims( uint32_t layer=0 ) const		{ return mOutputs[layer].dims; }
-
-	/**
-	 * Retrieve the width of network output layer.
-	 */
-	inline uint32_t GetOutputWidth( uint32_t layer=0 ) const	{ return DIMS_W(mOutputs[layer].dims); }
-
-	/**
-	 * Retrieve the height of network output layer.
-	 */
-	inline uint32_t GetOutputHeight( uint32_t layer=0 ) const	{ return DIMS_H(mOutputs[layer].dims); }
-
-	/**
-	 * Retrieve the size (in bytes) of network output layer.
-	 */
-	inline uint32_t GetOutputSize( uint32_t layer=0 ) const	{ return mOutputs[layer].size; }
-
-	/**
-	 * Get the CUDA pointer to the output memory.
-	 */
-	inline float* GetOutputPtr( uint32_t layer=0 ) const		{ return mOutputs[layer].CUDA; }
-	
-	/**
-	 * Retrieve the network frames per second (FPS).
-	 */
-	inline float GetNetworkFPS()							{ return 1000.0f / GetNetworkTime(); }
-
-	/**
-	 * Retrieve the network runtime (in milliseconds).
-	 */
-	inline float GetNetworkTime()							{ return GetProfilerTime(PROFILER_NETWORK, PROFILER_CUDA); }
-	
-	/**
-	 * Retrieve the network name (it's filename).
-	 */
-	inline const char* GetNetworkName() const				{ return mModelFile.c_str(); }
-	
-	/**
-	 * Retrieve the profiler runtime (in milliseconds).
-	 */
-	inline float2 GetProfilerTime( profilerQuery query )		{ PROFILER_QUERY(query); return mProfilerTimes[query]; }
-	
-	/**
-	 * Retrieve the profiler runtime (in milliseconds).
-	 */
-	inline float GetProfilerTime( profilerQuery query, profilerDevice device ) { PROFILER_QUERY(query); return (device == PROFILER_CPU) ? mProfilerTimes[query].x : mProfilerTimes[query].y; }
-	
-	/**
-	 * Print the profiler times (in millseconds).
-	 */
-	inline void PrintProfilerTimes()
-	{
-		LogInfo("\n");
-		LogInfo(LOG_TRT "------------------------------------------------\n");
-		LogInfo(LOG_TRT "Timing Report %s\n", GetModelPath());
-		LogInfo(LOG_TRT "------------------------------------------------\n");
-
-		for( uint32_t n=0; n <= PROFILER_TOTAL; n++ )
-		{
-			const profilerQuery query = (profilerQuery)n;
-
-			if( PROFILER_QUERY(query) )
-				LogInfo(LOG_TRT "%-12s  CPU %9.5fms  CUDA %9.5fms\n", profilerQueryToStr(query), mProfilerTimes[n].x, mProfilerTimes[n].y);
-		}
-
-		LogInfo(LOG_TRT "------------------------------------------------\n\n");
-
-		static bool first_run=true;
-
-		if( first_run )
-		{
-			LogWarning(LOG_TRT "note -- when processing a single image, run 'sudo jetson_clocks' before\n"
-				      "                to disable DVFS for more accurate profiling/timing measurements\n\n");
-			
-			first_run = false;
-		}
-	}
-	
-protected:
-
-	/**
-	 * Constructor.
-	 */
-	tensorNet();
-		
-	/**
-	 * Execute processing of the network.
-	 * @param sync if true (default), the device will be synchronized after processing
-	 *             and the thread/function will block until processing is complete. 
-	 *             if false, the function will return immediately after the processing
-	 *             has been enqueued to the CUDA stream indicated by GetStream().
-	 */
-	bool ProcessNetwork( bool sync=true );
-	  
-	/**
-	 * Create and output an optimized network model
-	 * @note this function is automatically used by LoadNetwork, but also can 
-	 *       be used individually to perform the network operations offline.
-	 * @param deployFile name for network prototxt
-	 * @param modelFile name for model
-	 * @param outputs network outputs
-	 * @param maxBatchSize maximum batch size 
-	 * @param modelStream output model stream
-	 */
-	bool ProfileModel( const std::string& deployFile, const std::string& modelFile,
-				    const std::vector<std::string>& inputs, const std::vector<Dims3>& inputDims,
-				    const std::vector<std::string>& outputs, uint32_t maxBatchSize, 
-				    precisionType precision, deviceType device, bool allowGPUFallback,
-				    nvinfer1::IInt8Calibrator* calibrator, char** engineStream, size_t* engineSize );
-
-	/**
-	 * Configure builder options
-	 */
-#if NV_TENSORRT_MAJOR >= 8
-	bool ConfigureBuilder( nvinfer1::IBuilder* builder, nvinfer1::IBuilderConfig* config,  
-					   uint32_t maxBatchSize, uint32_t workspaceSize, precisionType precision, 
-					   deviceType device, bool allowGPUFallback, 
-					   nvinfer1::IInt8Calibrator* calibrator );
-#else	 
-	bool ConfigureBuilder( nvinfer1::IBuilder* builder, uint32_t maxBatchSize, 
-					   uint32_t workspaceSize, precisionType precision, 
-					   deviceType device, bool allowGPUFallback, 
-					   nvinfer1::IInt8Calibrator* calibrator );
-#endif
-
-	/**
-	 * Validate that the model already has a built TensorRT engine that exists and doesn't need updating.
-	 */
-	bool ValidateEngine( const char* model_path, const char* cache_path, const char* checksum_path );
-
-	/**
-	 * Logger class for GIE info/warning/errors
-	 */
-	class Logger : public nvinfer1::ILogger			
-	{
-	public:
-		void log( Severity severity, const char* msg ) NOEXCEPT override
-		{
-			if( severity == Severity::kWARNING )
-			{
-				LogWarning(LOG_TRT "%s\n", msg);
-			}
-			else if( severity == Severity::kINFO )
-			{
-				LogInfo(LOG_TRT "%s\n", msg);
-			}
-		#if NV_TENSORRT_MAJOR >= 6
-			else if( severity == Severity::kVERBOSE )
-			{
-				LogVerbose(LOG_TRT "%s\n", msg);
-			}
-		#endif
-			else
-			{
-				LogError(LOG_TRT "%s\n", msg);
-			}
-		}
-	} static gLogger;
-
-	/**
-	 * Profiler interface for measuring layer timings
-	 */
-	class Profiler : public nvinfer1::IProfiler
-	{
-	public:
-		Profiler() : timingAccumulator(0.0f)	{ }
-		
-		virtual void reportLayerTime(const char* layerName, float ms) NOEXCEPT
-		{
-			LogVerbose(LOG_TRT "layer %s - %f ms\n", layerName, ms);
-			timingAccumulator += ms;
-		}
-		
-		float timingAccumulator;
-	} gProfiler;
-
-	/**
-	 * Begin a profiling query, before network is run
-	 */
-	inline void PROFILER_BEGIN( profilerQuery query )		
-	{ 
-		const uint32_t evt = query*2; 
-		const uint32_t flag = (1 << query);
-
-		CUDA(cudaEventRecord(mEventsGPU[evt], mStream)); 
-		timestamp(&mEventsCPU[evt]); 
-
-		mProfilerQueriesUsed |= flag;
-		mProfilerQueriesDone &= ~flag;
-	}
-
-	/**
-	 * End a profiling query, after the network is run
-	 */
-	inline void PROFILER_END( profilerQuery query )		
-	{ 
-		const uint32_t evt = query*2+1; 
-
-		CUDA(cudaEventRecord(mEventsGPU[evt])); 
-		timestamp(&mEventsCPU[evt]); 
-		timespec cpuTime; 
-		timeDiff(mEventsCPU[evt-1], mEventsCPU[evt], &cpuTime);
-		mProfilerTimes[query].x = timeFloat(cpuTime);
-
-		if( mEnableProfiler && query == PROFILER_NETWORK ) 
-		{ 
-			LogVerbose(LOG_TRT "layer network time - %f ms\n", gProfiler.timingAccumulator); 
-			gProfiler.timingAccumulator = 0.0f; 
-			LogWarning(LOG_TRT "note -- when processing a single image, run 'sudo jetson_clocks' before\n"
-				      "                to disable DVFS for more accurate profiling/timing measurements\n"); 
-		}
-	}
-	
-	/**
-	 * Query the CUDA part of a profiler query.
-	 */
-	inline bool PROFILER_QUERY( profilerQuery query )
-	{
-		const uint32_t flag = (1 << query);
-
-		if( query == PROFILER_TOTAL )
-		{
-			mProfilerTimes[PROFILER_TOTAL].x = 0.0f;
-			mProfilerTimes[PROFILER_TOTAL].y = 0.0f;
-
-			for( uint32_t n=0; n < PROFILER_TOTAL; n++ )
-			{
-				if( PROFILER_QUERY((profilerQuery)n) )
-				{
-					mProfilerTimes[PROFILER_TOTAL].x += mProfilerTimes[n].x;
-					mProfilerTimes[PROFILER_TOTAL].y += mProfilerTimes[n].y;
-				}
-			}
-
-			return true;
-		}
-		else if( mProfilerQueriesUsed & flag )
-		{
-			if( !(mProfilerQueriesDone & flag) )
-			{
-				const uint32_t evt = query*2;
-				float cuda_time = 0.0f;
-				CUDA(cudaEventElapsedTime(&cuda_time, mEventsGPU[evt], mEventsGPU[evt+1]));
-				mProfilerTimes[query].y = cuda_time;
-				mProfilerQueriesDone |= flag;
-				//mProfilerQueriesUsed &= ~flag;
-			}
-
-			return true;
-		}
-
-		return false;
-	}
-
-protected:
-
-	/* Member Variables */
-	std::string mPrototxtPath;
-	std::string mModelPath;
-	std::string mModelFile;
-	std::string mMeanPath;
-	std::string mCacheEnginePath;
-	std::string mCacheCalibrationPath;
-	std::string mChecksumPath;
-	
-	deviceType    mDevice;
-	precisionType mPrecision;
-	modelType     mModelType;
-	cudaStream_t  mStream;
-	cudaEvent_t   mEventsGPU[PROFILER_TOTAL * 2];
-	timespec      mEventsCPU[PROFILER_TOTAL * 2];
-
-	nvinfer1::IRuntime* mInfer;
-	nvinfer1::ICudaEngine* mEngine;
-	nvinfer1::IExecutionContext* mContext;
-	
-	float2   mProfilerTimes[PROFILER_TOTAL + 1];
-	uint32_t mProfilerQueriesUsed;
-	uint32_t mProfilerQueriesDone;
-	uint32_t mWorkspaceSize;
-	uint32_t mMaxBatchSize;
-	bool	    mEnableProfiler;
-	bool     mEnableDebug;
-	bool	    mAllowGPUFallback;
-	void**   mBindings;
-
-	struct layerInfo
-	{
-		std::string name;
-		Dims3 dims;
-		uint32_t size;
-		uint32_t binding;
-		float* CPU;
-		float* CUDA;
-	};
-	
-	std::vector<layerInfo> mInputs;
-	std::vector<layerInfo> mOutputs;
-};
-
-#endif
+/*
+ * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+ 
+#ifndef __TENSOR_NET_H__
+#define __TENSOR_NET_H__
+
+// forward declaration of IInt8Calibrator
+namespace nvinfer1 { class IInt8Calibrator; }
+
+// includes
+#include <NvInfer.h>
+
+#include <jetson-utils/cudaUtility.h>
+#include <jetson-utils/commandLine.h>
+#include <jetson-utils/imageFormat.h>
+#include <jetson-utils/timespec.h>
+#include <jetson-utils/logging.h>
+
+#include <vector>
+#include <sstream>
+#include <math.h>
+
+
+#if NV_TENSORRT_MAJOR >= 6
+typedef nvinfer1::Dims3 Dims3;
+
+#define DIMS_C(x) x.d[0]
+#define DIMS_H(x) x.d[1]
+#define DIMS_W(x) x.d[2]
+
+#elif NV_TENSORRT_MAJOR >= 2
+typedef nvinfer1::DimsCHW Dims3;
+
+#define DIMS_C(x) x.d[0]
+#define DIMS_H(x) x.d[1]
+#define DIMS_W(x) x.d[2]
+
+#else
+typedef nvinfer1::Dims3 Dims3; 
+
+#define DIMS_C(x) x.c
+#define DIMS_H(x) x.h
+#define DIMS_W(x) x.w
+
+#ifndef NV_TENSORRT_MAJOR
+#define NV_TENSORRT_MAJOR 8 // sebi: was 1, changed to 8 to easier code study (not dim the code, as Jetson nano uses TRT8.2)
+#define NV_TENSORRT_MINOR 0
+#endif
+#endif
+
+#if NV_TENSORRT_MAJOR >= 8
+#define NOEXCEPT noexcept
+#else
+#define NOEXCEPT
+#endif
+
+
+/**
+ * Macro for checking the minimum version of TensorRT that is installed.
+ * This evaluates to true if TensorRT is newer or equal to the provided version.
+ * @ingroup tensorNet
+ */
+#define TENSORRT_VERSION_CHECK(major, minor, patch)  (NV_TENSORRT_MAJOR > major || (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR > minor) || (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR == minor && NV_TENSORRT_PATCH >= patch))
+
+/**
+ * Default maximum batch size
+ * @ingroup tensorNet
+ */
+#define DEFAULT_MAX_BATCH_SIZE  1
+
+/**
+ * Prefix used for tagging printed log output from TensorRT.
+ * @ingroup tensorNet
+ */
+#define LOG_TRT "[TRT]    "
+
+
+/**
+ * Enumeration for indicating the desired precision that
+ * the network should run in, if available in hardware.
+ * @ingroup tensorNet
+ */
+enum precisionType
+{
+	TYPE_DISABLED = 0,	/**< Unknown, unspecified, or disabled type */
+	TYPE_FASTEST,		/**< The fastest detected precision should be use (i.e. try INT8, then FP16, then FP32) */
+	TYPE_FP32,		/**< 32-bit floating-point precision (FP32) */
+	TYPE_FP16,		/**< 16-bit floating-point half precision (FP16) */
+	TYPE_INT8,		/**< 8-bit integer precision (INT8) */
+	NUM_PRECISIONS		/**< Number of precision types defined */
+};
+
+/**
+ * Stringize function that returns precisionType in text.
+ * @ingroup tensorNet
+ */
+const char* precisionTypeToStr( precisionType type );
+
+/**
+ * Parse the precision type from a string.
+ * @ingroup tensorNet
+ */
+precisionType precisionTypeFromStr( const char* str );
+
+/**
+ * Enumeration for indicating the desired device that 
+ * the network should run on, if available in hardware.
+ * @ingroup tensorNet
+ */
+enum deviceType
+{
+	DEVICE_GPU = 0,			/**< GPU (if multiple GPUs are present, a specific GPU can be selected with cudaSetDevice() */
+	DEVICE_DLA,				/**< Deep Learning Accelerator (DLA) Core 0 (only on Jetson Xavier) */
+	DEVICE_DLA_0 = DEVICE_DLA,	/**< Deep Learning Accelerator (DLA) Core 0 (only on Jetson Xavier) */
+	DEVICE_DLA_1,				/**< Deep Learning Accelerator (DLA) Core 1 (only on Jetson Xavier) */
+	NUM_DEVICES				/**< Number of device types defined */
+};
+
+/**
+ * Stringize function that returns deviceType in text.
+ * @ingroup tensorNet
+ */
+const char* deviceTypeToStr( deviceType type );
+
+/**
+ * Parse the device type from a string.
+ * @ingroup tensorNet
+ */
+deviceType deviceTypeFromStr( const char* str );
+
+/**
+ * Enumeration indicating the format of the model that's
+ * imported in TensorRT (either caffe, ONNX, or UFF).
+ * @ingroup tensorNet
+ */
+enum modelType
+{
+	MODEL_CUSTOM = 0,	/**< Created directly with TensorRT API */
+	MODEL_CAFFE,		/**< caffemodel */
+	MODEL_ONNX,		/**< ONNX */
+	MODEL_UFF,		/**< UFF */
+	MODEL_ENGINE		/**< TensorRT engine/plan */
+};
+
+//sebi:
+enum ONNXKind
+{
+	ONNX_SSD = 0,
+	ONNX_YOLO
+};
+//
+
+/**
+ * Stringize function that returns modelType in text.
+ * @ingroup tensorNet
+ */
+const char* modelTypeToStr( modelType type );
+
+/**
+ * Parse the model format from a string.
+ * @ingroup tensorNet
+ */
+modelType modelTypeFromStr( const char* str );
+
+/**
+ * Parse the model format from a file path.
+ * @ingroup tensorNet
+ */
+modelType modelTypeFromPath( const char* path );
+
+//sebi
+ONNXKind ONNXKindFromStr(const char* str);
+
+/**
+ * Profiling queries
+ * @see tensorNet::GetProfilerTime()
+ * @ingroup tensorNet
+ */
+enum profilerQuery
+{
+	PROFILER_PREPROCESS = 0,
+	PROFILER_NETWORK,
+	PROFILER_POSTPROCESS,
+	PROFILER_VISUALIZE,
+	PROFILER_TOTAL,
+};
+
+/**
+ * Stringize function that returns profilerQuery in text.
+ * @ingroup tensorNet
+ */
+const char* profilerQueryToStr( profilerQuery query );
+
+/**
+ * Profiler device
+ * @ingroup tensorNet
+ */
+enum profilerDevice
+{
+	PROFILER_CPU = 0,	/**< CPU walltime */
+	PROFILER_CUDA,		/**< CUDA kernel time */ 
+};
+
+
+/**
+ * Abstract class for loading a tensor network with TensorRT.
+ * For example implementations, @see imageNet and @see detectNet
+ * @ingroup tensorNet
+ */
+class tensorNet
+{
+public:
+	/**
+	 * Destory
+	 */
+	virtual ~tensorNet();
+	
+	/**
+	 * Load a new network instance
+	 * @param prototxt File path to the deployable network prototxt
+	 * @param model File path to the caffemodel 
+	 * @param mean File path to the mean value binary proto (NULL if none)
+	 * @param input_blob The name of the input blob data to the network.
+	 * @param output_blob The name of the output blob data from the network.
+	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
+	 */
+	bool LoadNetwork( const char* prototxt, const char* model, const char* mean=NULL,
+				   const char* input_blob="data", const char* output_blob="prob",
+				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST,
+				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
+				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
+
+	/**
+	 * Load a new network instance with multiple output layers
+	 * @param prototxt File path to the deployable network prototxt
+	 * @param model File path to the caffemodel 
+	 * @param mean File path to the mean value binary proto (NULL if none)
+	 * @param input_blob The name of the input blob data to the network.
+	 * @param output_blobs List of names of the output blobs from the network.
+	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
+	 */
+	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
+				   const char* input_blob, const std::vector<std::string>& output_blobs,
+				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, precisionType precision=TYPE_FASTEST,
+				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
+				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
+
+	/**
+	 * Load a new network instance with multiple input layers.
+	 * @param prototxt File path to the deployable network prototxt
+	 * @param model File path to the caffemodel 
+	 * @param mean File path to the mean value binary proto (NULL if none)
+	 * @param input_blobs List of names of the inputs blob data to the network.
+	 * @param output_blobs List of names of the output blobs from the network.
+	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
+	 */
+	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
+				   const std::vector<std::string>& input_blobs, 
+				   const std::vector<std::string>& output_blobs,
+				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+				   precisionType precision=TYPE_FASTEST,
+				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
+				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
+
+	/**
+	 * Load a new network instance (this variant is used for UFF models)
+	 * @param prototxt File path to the deployable network prototxt
+	 * @param model File path to the caffemodel 
+	 * @param mean File path to the mean value binary proto (NULL if none)
+	 * @param input_blob The name of the input blob data to the network.
+	 * @param input_dims The dimensions of the input blob (used for UFF).
+	 * @param output_blobs List of names of the output blobs from the network.
+	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
+	 */
+	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
+				   const char* input_blob, const Dims3& input_dims, 
+				   const std::vector<std::string>& output_blobs,
+				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+				   precisionType precision=TYPE_FASTEST,
+				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
+				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
+
+	/**
+	 * Load a new network instance with multiple input layers (used for UFF models)
+	 * @param prototxt File path to the deployable network prototxt
+	 * @param model File path to the caffemodel 
+	 * @param mean File path to the mean value binary proto (NULL if none)
+	 * @param input_blobs List of names of the inputs blob data to the network.
+	 * @param input_dims List of the dimensions of the input blobs (used for UFF).
+	 * @param output_blobs List of names of the output blobs from the network.
+	 * @param maxBatchSize The maximum batch size that the network will be optimized for.
+	 */
+	bool LoadNetwork( const char* prototxt, const char* model, const char* mean,
+				   const std::vector<std::string>& input_blobs, 
+				   const std::vector<Dims3>& input_dims, 
+				   const std::vector<std::string>& output_blobs,
+				   uint32_t maxBatchSize=DEFAULT_MAX_BATCH_SIZE, 
+				   precisionType precision=TYPE_FASTEST,
+				   deviceType device=DEVICE_GPU, bool allowGPUFallback=true,
+				   nvinfer1::IInt8Calibrator* calibrator=NULL, cudaStream_t stream=NULL );
+
+	/**
+	 * Load a network instance from a serialized engine plan file.
+	 * @param engine_filename path to the serialized engine plan file.
+	 * @param input_blobs List of names of the inputs blob data to the network.
+	 * @param output_blobs List of names of the output blobs from the network.
+	 */
+	bool LoadEngine( const char* engine_filename,
+				  const std::vector<std::string>& input_blobs, 
+				  const std::vector<std::string>& output_blobs,
+				  nvinfer1::IPluginFactory* pluginFactory=NULL,
+				  deviceType device=DEVICE_GPU,
+				  cudaStream_t stream=NULL );
+
+	/**
+	 * Load a network instance from a serialized engine plan file.
+	 * @param engine_stream Memory containing the serialized engine plan file.
+	 * @param engine_size Size of the serialized engine stream (in bytes).
+	 * @param input_blobs List of names of the inputs blob data to the network.
+	 * @param output_blobs List of names of the output blobs from the network.
+	 */
+	bool LoadEngine( char* engine_stream, size_t engine_size,
+				  const std::vector<std::string>& input_blobs, 
+				  const std::vector<std::string>& output_blobs,
+				  nvinfer1::IPluginFactory* pluginFactory=NULL,
+				  deviceType device=DEVICE_GPU,
+				  cudaStream_t stream=NULL );
+
+	/**
+	 * Load network resources from an existing TensorRT engine instance.
+	 * @param engine_stream Memory containing the serialized engine plan file.
+	 * @param engine_size Size of the serialized engine stream (in bytes).
+	 * @param input_blobs List of names of the inputs blob data to the network.
+	 * @param output_blobs List of names of the output blobs from the network.
+	 */
+	bool LoadEngine( nvinfer1::ICudaEngine* engine,
+				  const std::vector<std::string>& input_blobs, 
+				  const std::vector<std::string>& output_blobs,
+				  deviceType device=DEVICE_GPU,
+				  cudaStream_t stream=NULL );
+
+	/**
+	 * Load a serialized engine plan file into memory.
+	 */
+	bool LoadEngine( const char* filename, char** stream, size_t* size );
+
+	/**
+	 * Load a binary file into memory.
+	 */
+	bool LoadBinary( const char* filename, char** stream, size_t* size );
+	
+	/**
+	 * Load class descriptions from a label file.  
+	 * Each line of the text file should include one class label (and optionally a synset).
+	 * If the number of expected labels aren't parsed, they will be automatically generated.
+	 */
+	static bool LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, int expectedClasses=-1 );
+
+	/**
+	 * Load class descriptions and synset strings from a label file.
+	 * Each line of the text file should include one class label (and optionally a synset).
+	 * If the number of expected labels aren't parsed, they will be automatically generated.
+	 */
+	static bool LoadClassLabels( const char* filename, std::vector<std::string>& descriptions, std::vector<std::string>& synsets, int expectedClasses=-1 );
+
+	/**
+	 * Load class colors from a text file.  If the number of expected colors aren't parsed, they will be generated.
+	 * The float4 color array should be `expectedClasses` long, and would typically be in shared CPU/GPU memory.
+	 * If a line in the text file only has RGB, then the defaultAlpha value will be used for the alpha channel.
+	 */
+	static bool LoadClassColors( const char* filename, float4* colors, int expectedClasses, float defaultAlpha=255.0f );
+
+	/**
+	 * Load class colors from a text file.  If the number of expected colors aren't parsed, they will be generated.
+	 * The float4 color array will automatically be allocated in shared CPU/GPU memory by `cudaAllocMapped()`.
+	 * If a line in the text file only has RGB, then the defaultAlpha value will be used for the alpha channel.
+	 */
+	static bool LoadClassColors( const char* filename, float4** colors, int expectedClasses, float defaultAlpha=255.0f );
+
+	/**
+	 * Procedurally generate a color for a given class index with the specified alpha value.
+	 * This function can be used to generate a range of colors when a colors.txt file isn't available.
+	 */
+	static float4 GenerateColor( uint32_t classID, float alpha=255.0f ); 
+	
+	/**
+	 * Manually enable layer profiling times.	
+	 */
+	void EnableLayerProfiler();
+
+	/**
+	 * Manually enable debug messages and synchronization.
+	 */
+	void EnableDebug();
+
+	/**
+ 	 * Return true if GPU fallback is enabled.
+	 */
+	inline bool AllowGPUFallback() const					{ return mAllowGPUFallback; }
+
+	/**
+ 	 * Retrieve the device being used for execution.
+	 */
+	inline deviceType GetDevice() const					{ return mDevice; }
+
+	/**
+	 * Retrieve the type of precision being used.
+	 */
+	inline precisionType GetPrecision() const				{ return mPrecision; }
+
+	/**
+	 * Check if a particular precision is being used.
+	 */
+	inline bool IsPrecision( precisionType type ) const		{ return (mPrecision == type); }
+
+	/**
+	 * Resolve a desired precision to a specific one that's available.
+	 */
+	static precisionType SelectPrecision( precisionType precision, deviceType device=DEVICE_GPU, bool allowInt8=true );
+
+	/**
+	 * Determine the fastest native precision on a device.
+	 */
+	static precisionType FindFastestPrecision( deviceType device=DEVICE_GPU, bool allowInt8=true );
+
+	/**
+	 * Detect the precisions supported natively on a device.
+	 */
+	static std::vector<precisionType> DetectNativePrecisions( deviceType device=DEVICE_GPU );
+	
+	/**
+	 * Detect if a particular precision is supported natively.
+	 */
+	static bool DetectNativePrecision( const std::vector<precisionType>& nativeTypes, precisionType type );
+
+	/**
+	 * Detect if a particular precision is supported natively.
+	 */
+	static bool DetectNativePrecision( precisionType precision, deviceType device=DEVICE_GPU );
+
+	/**
+	 * Retrieve the stream that the device is operating on.
+	 */
+	inline cudaStream_t GetStream() const					{ return mStream; }
+
+	/**
+	 * Create and use a new stream for execution.
+	 */
+	cudaStream_t CreateStream( bool nonBlocking=true );
+
+	/**
+	 * Set the stream that the device is operating on.
+	 */
+	void SetStream( cudaStream_t stream );
+
+	/**
+	 * Retrieve the path to the network prototxt file.
+	 */
+	inline const char* GetPrototxtPath() const				{ return mPrototxtPath.c_str(); }
+
+	/**
+	 * Retrieve the full path to model file, including the filename.
+	 */
+	inline const char* GetModelPath() const					{ return mModelPath.c_str(); }
+
+	/**
+	 * Retrieve the filename of the file, excluding the directory.
+	 */
+	inline const char* GetModelFilename() const				{ return mModelFile.c_str(); }
+	
+	/**
+	 * Retrieve the format of the network model.
+	 */
+	inline modelType GetModelType() const					{ return mModelType; }
+
+	/**
+	 * Return true if the model is of the specified format.
+	 */
+	inline bool IsModelType( modelType type ) const			{ return (mModelType == type); }
+
+	/**
+	 * Retrieve the number of input layers to the network.
+	 */
+	inline uint32_t GetInputLayers() const					{ return mInputs.size(); }
+
+	/**
+	 * Retrieve the number of output layers to the network.
+	 */
+	inline uint32_t GetOutputLayers() const					{ return mOutputs.size(); }
+
+	/**
+	 * Retrieve the dimensions of network input layer.
+	 */
+	inline Dims3 GetInputDims( uint32_t layer=0 ) const		{ return mInputs[layer].dims; }
+
+	/**
+	 * Retrieve the width of network input layer.
+	 */
+	inline uint32_t GetInputWidth( uint32_t layer=0 ) const	{ return DIMS_W(mInputs[layer].dims); }
+
+	/**
+	 * Retrieve the height of network input layer.
+	 */
+	inline uint32_t GetInputHeight( uint32_t layer=0 ) const	{ return DIMS_H(mInputs[layer].dims); }
+
+	/**
+	 * Retrieve the size (in bytes) of network input layer.
+	 */
+	inline uint32_t GetInputSize( uint32_t layer=0 ) const		{ return mInputs[layer].size; }
+
+	/**
+	 * Get the CUDA pointer to the input layer's memory.
+	 */
+	inline float* GetInputPtr( uint32_t layer=0 ) const		{ return mInputs[layer].CUDA; }
+	
+	/**
+	 * Retrieve the dimensions of network output layer.
+	 */
+	inline Dims3 GetOutputDims( uint32_t layer=0 ) const		{ return mOutputs[layer].dims; }
+
+	/**
+	 * Retrieve the width of network output layer.
+	 */
+	inline uint32_t GetOutputWidth( uint32_t layer=0 ) const	{ return DIMS_W(mOutputs[layer].dims); }
+
+	/**
+	 * Retrieve the height of network output layer.
+	 */
+	inline uint32_t GetOutputHeight( uint32_t layer=0 ) const	{ return DIMS_H(mOutputs[layer].dims); }
+
+	/**
+	 * Retrieve the size (in bytes) of network output layer.
+	 */
+	inline uint32_t GetOutputSize( uint32_t layer=0 ) const	{ return mOutputs[layer].size; }
+
+	/**
+	 * Get the CUDA pointer to the output memory.
+	 */
+	inline float* GetOutputPtr( uint32_t layer=0 ) const		{ return mOutputs[layer].CUDA; }
+	
+	/**
+	 * Retrieve the network frames per second (FPS).
+	 */
+	inline float GetNetworkFPS()							{ return 1000.0f / GetNetworkTime(); }
+
+	/**
+	 * Retrieve the network runtime (in milliseconds).
+	 */
+	inline float GetNetworkTime()							{ return GetProfilerTime(PROFILER_NETWORK, PROFILER_CUDA); }
+	
+	/**
+	 * Retrieve the network name (it's filename).
+	 */
+	inline const char* GetNetworkName() const				{ return mModelFile.c_str(); }
+	
+	/**
+	 * Retrieve the profiler runtime (in milliseconds).
+	 */
+	inline float2 GetProfilerTime( profilerQuery query )		{ PROFILER_QUERY(query); return mProfilerTimes[query]; }
+	
+	/**
+	 * Retrieve the profiler runtime (in milliseconds).
+	 */
+	inline float GetProfilerTime( profilerQuery query, profilerDevice device ) { PROFILER_QUERY(query); return (device == PROFILER_CPU) ? mProfilerTimes[query].x : mProfilerTimes[query].y; }
+	
+	/**
+	 * Print the profiler times (in millseconds).
+	 */
+	inline void PrintProfilerTimes()
+	{
+		LogInfo("\n");
+		LogInfo(LOG_TRT "------------------------------------------------\n");
+		LogInfo(LOG_TRT "Timing Report %s\n", GetModelPath());
+		LogInfo(LOG_TRT "------------------------------------------------\n");
+
+		for( uint32_t n=0; n <= PROFILER_TOTAL; n++ )
+		{
+			const profilerQuery query = (profilerQuery)n;
+
+			if( PROFILER_QUERY(query) )
+				LogInfo(LOG_TRT "%-12s  CPU %9.5fms  CUDA %9.5fms\n", profilerQueryToStr(query), mProfilerTimes[n].x, mProfilerTimes[n].y);
+		}
+
+		LogInfo(LOG_TRT "------------------------------------------------\n\n");
+
+		static bool first_run=true;
+
+		if( first_run )
+		{
+			LogWarning(LOG_TRT "note -- when processing a single image, run 'sudo jetson_clocks' before\n"
+				      "                to disable DVFS for more accurate profiling/timing measurements\n\n");
+			
+			first_run = false;
+		}
+	}
+	
+protected:
+
+	/**
+	 * Constructor.
+	 */
+	tensorNet();
+		
+	/**
+	 * Execute processing of the network.
+	 * @param sync if true (default), the device will be synchronized after processing
+	 *             and the thread/function will block until processing is complete. 
+	 *             if false, the function will return immediately after the processing
+	 *             has been enqueued to the CUDA stream indicated by GetStream().
+	 */
+	bool ProcessNetwork( bool sync=true );
+	  
+	/**
+	 * Create and output an optimized network model
+	 * @note this function is automatically used by LoadNetwork, but also can 
+	 *       be used individually to perform the network operations offline.
+	 * @param deployFile name for network prototxt
+	 * @param modelFile name for model
+	 * @param outputs network outputs
+	 * @param maxBatchSize maximum batch size 
+	 * @param modelStream output model stream
+	 */
+	bool ProfileModel( const std::string& deployFile, const std::string& modelFile,
+				    const std::vector<std::string>& inputs, const std::vector<Dims3>& inputDims,
+				    const std::vector<std::string>& outputs, uint32_t maxBatchSize, 
+				    precisionType precision, deviceType device, bool allowGPUFallback,
+				    nvinfer1::IInt8Calibrator* calibrator, char** engineStream, size_t* engineSize );
+
+	/**
+	 * Configure builder options
+	 */
+#if NV_TENSORRT_MAJOR >= 8
+	bool ConfigureBuilder( nvinfer1::IBuilder* builder, nvinfer1::IBuilderConfig* config,  
+					   uint32_t maxBatchSize, uint32_t workspaceSize, precisionType precision, 
+					   deviceType device, bool allowGPUFallback, 
+					   nvinfer1::IInt8Calibrator* calibrator );
+#else	 
+	bool ConfigureBuilder( nvinfer1::IBuilder* builder, uint32_t maxBatchSize, 
+					   uint32_t workspaceSize, precisionType precision, 
+					   deviceType device, bool allowGPUFallback, 
+					   nvinfer1::IInt8Calibrator* calibrator );
+#endif
+
+	/**
+	 * Validate that the model already has a built TensorRT engine that exists and doesn't need updating.
+	 */
+	bool ValidateEngine( const char* model_path, const char* cache_path, const char* checksum_path );
+
+	/**
+	 * Logger class for GIE info/warning/errors
+	 */
+	class Logger : public nvinfer1::ILogger			
+	{
+	public:
+		void log( Severity severity, const char* msg ) NOEXCEPT override
+		{
+			if( severity == Severity::kWARNING )
+			{
+				LogWarning(LOG_TRT "%s\n", msg);
+			}
+			else if( severity == Severity::kINFO )
+			{
+				LogInfo(LOG_TRT "%s\n", msg);
+			}
+		#if NV_TENSORRT_MAJOR >= 6
+			else if( severity == Severity::kVERBOSE )
+			{
+				LogVerbose(LOG_TRT "%s\n", msg);
+			}
+		#endif
+			else
+			{
+				LogError(LOG_TRT "%s\n", msg);
+			}
+		}
+	} static gLogger;
+
+	/**
+	 * Profiler interface for measuring layer timings
+	 */
+	class Profiler : public nvinfer1::IProfiler
+	{
+	public:
+		Profiler() : timingAccumulator(0.0f)	{ }
+		
+		virtual void reportLayerTime(const char* layerName, float ms) NOEXCEPT
+		{
+			LogVerbose(LOG_TRT "layer %s - %f ms\n", layerName, ms);
+			timingAccumulator += ms;
+		}
+		
+		float timingAccumulator;
+	} gProfiler;
+
+	/**
+	 * Begin a profiling query, before network is run
+	 */
+	inline void PROFILER_BEGIN( profilerQuery query )		
+	{ 
+		const uint32_t evt = query*2; 
+		const uint32_t flag = (1 << query);
+
+		CUDA(cudaEventRecord(mEventsGPU[evt], mStream)); 
+		timestamp(&mEventsCPU[evt]); 
+
+		mProfilerQueriesUsed |= flag;
+		mProfilerQueriesDone &= ~flag;
+	}
+
+	/**
+	 * End a profiling query, after the network is run
+	 */
+	inline void PROFILER_END( profilerQuery query )		
+	{ 
+		const uint32_t evt = query*2+1; 
+
+		CUDA(cudaEventRecord(mEventsGPU[evt])); 
+		timestamp(&mEventsCPU[evt]); 
+		timespec cpuTime; 
+		timeDiff(mEventsCPU[evt-1], mEventsCPU[evt], &cpuTime);
+		mProfilerTimes[query].x = timeFloat(cpuTime);
+
+		if( mEnableProfiler && query == PROFILER_NETWORK ) 
+		{ 
+			LogVerbose(LOG_TRT "layer network time - %f ms\n", gProfiler.timingAccumulator); 
+			gProfiler.timingAccumulator = 0.0f; 
+			LogWarning(LOG_TRT "note -- when processing a single image, run 'sudo jetson_clocks' before\n"
+				      "                to disable DVFS for more accurate profiling/timing measurements\n"); 
+		}
+	}
+	
+	/**
+	 * Query the CUDA part of a profiler query.
+	 */
+	inline bool PROFILER_QUERY( profilerQuery query )
+	{
+		const uint32_t flag = (1 << query);
+
+		if( query == PROFILER_TOTAL )
+		{
+			mProfilerTimes[PROFILER_TOTAL].x = 0.0f;
+			mProfilerTimes[PROFILER_TOTAL].y = 0.0f;
+
+			for( uint32_t n=0; n < PROFILER_TOTAL; n++ )
+			{
+				if( PROFILER_QUERY((profilerQuery)n) )
+				{
+					mProfilerTimes[PROFILER_TOTAL].x += mProfilerTimes[n].x;
+					mProfilerTimes[PROFILER_TOTAL].y += mProfilerTimes[n].y;
+				}
+			}
+
+			return true;
+		}
+		else if( mProfilerQueriesUsed & flag )
+		{
+			if( !(mProfilerQueriesDone & flag) )
+			{
+				const uint32_t evt = query*2;
+				float cuda_time = 0.0f;
+				CUDA(cudaEventElapsedTime(&cuda_time, mEventsGPU[evt], mEventsGPU[evt+1]));
+				mProfilerTimes[query].y = cuda_time;
+				mProfilerQueriesDone |= flag;
+				//mProfilerQueriesUsed &= ~flag;
+			}
+
+			return true;
+		}
+
+		return false;
+	}
+
+protected:
+
+	/* Member Variables */
+	std::string mPrototxtPath;
+	std::string mModelPath;
+	std::string mModelFile;
+	std::string mMeanPath;
+	std::string mCacheEnginePath;
+	std::string mCacheCalibrationPath;
+	std::string mChecksumPath;
+	
+	deviceType    mDevice;
+	precisionType mPrecision;
+	modelType     mModelType;
+	ONNXKind	  mONNXKind;
+	cudaStream_t  mStream;
+	cudaEvent_t   mEventsGPU[PROFILER_TOTAL * 2];
+	timespec      mEventsCPU[PROFILER_TOTAL * 2];
+
+	nvinfer1::IRuntime* mInfer;
+	nvinfer1::ICudaEngine* mEngine;
+	nvinfer1::IExecutionContext* mContext;
+	
+	float2   mProfilerTimes[PROFILER_TOTAL + 1];
+	uint32_t mProfilerQueriesUsed;
+	uint32_t mProfilerQueriesDone;
+	uint32_t mWorkspaceSize;
+	uint32_t mMaxBatchSize;
+	bool	    mEnableProfiler;
+	bool     mEnableDebug;
+	bool	    mAllowGPUFallback;
+	void**   mBindings;
+
+	struct layerInfo
+	{
+		std::string name;
+		Dims3 dims;
+		uint32_t size;
+		uint32_t binding;
+		float* CPU;
+		float* CUDA;
+	};
+	
+	std::vector<layerInfo> mInputs;
+	std::vector<layerInfo> mOutputs;
+};
+
+#endif
